commit 02befde445c400f6db2e9943684c4bb405024743
Author: Hamad Al Marri <hamad@cachyos.org>
Date:   Mon Jul 12 20:50:42 2021 +0300

    yield_rework

diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 5a66fc5826fc..d17e2d60a6ab 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -36,6 +36,7 @@ extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int interactivity_factor;
 extern unsigned int interactivity_threshold;
 extern unsigned int cacule_max_lifetime;
+extern int sched_yield_type;
 #endif
 
 enum sched_tunable_scaling {
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 88131d66856f..b14b172a4f13 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -82,6 +82,10 @@ const_debug unsigned int sysctl_sched_nr_migrate = 32;
  */
 unsigned int sysctl_sched_rt_period = 1000000;
 
+#ifdef CONFIG_CACULE_SCHED
+int __read_mostly sched_yield_type = 1;
+#endif
+
 __read_mostly int scheduler_running;
 
 /*
@@ -6968,6 +6972,15 @@ static void do_sched_yield(void)
 	struct rq_flags rf;
 	struct rq *rq;
 
+#ifdef CONFIG_CACULE_SCHED
+	struct task_struct *curr = current;
+	struct cacule_node *cn = &curr->se.cacule_node;
+
+	if (sched_yield_type) {
+		cn->vruntime |= YIELD_MARK;
+		return;
+	}
+#endif
 	rq = this_rq_lock_irq(&rf);
 
 	schedstat_inc(rq->yld_count);
@@ -7136,6 +7149,12 @@ int __sched yield_to(struct task_struct *p, bool preempt)
 	unsigned long flags;
 	int yielded = 0;
 
+// not sure about yield_to
+//#ifdef CONFIG_CACULE_SCHED
+	//if (sched_yield_type)
+		//return 0;
+//#endif
+
 	local_irq_save(flags);
 	rq = this_rq();
 
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 162395e3fda2..56585f578ad1 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1072,7 +1072,7 @@ static void update_tg_load_avg(struct cfs_rq *cfs_rq)
 static void normalize_lifetime(u64 now, struct sched_entity *se)
 {
 	struct cacule_node *cn = &se->cacule_node;
-	u64 max_life_ns, life_time;
+	u64 max_life_ns, life_time, old_hrrn_x;
 	s64 diff;
 
 	/*
@@ -1085,8 +1085,12 @@ static void normalize_lifetime(u64 now, struct sched_entity *se)
 	diff		= life_time - max_life_ns;
 
 	if (diff > 0) {
+		// unmark YIELD. No need to check or remark since
+		// this normalize action doesn't happen very often
+		cn->vruntime &= YIELD_UNMARK;
+
 		// multiply life_time by 1024 for more precision
-		u64 old_hrrn_x	= (life_time << 7) / ((cn->vruntime >> 3) | 1);
+		old_hrrn_x = (life_time << 7) / ((cn->vruntime >> 3) | 1);
 
 		// reset life to half max_life (i.e ~15s)
 		cn->cacule_start_time = now - (max_life_ns >> 1);
@@ -4919,6 +4923,9 @@ static void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev)
 		/* in !on_rq case, update occurred at dequeue */
 		update_load_avg(cfs_rq, prev, 0);
 	}
+
+	prev->cacule_node.vruntime &= YIELD_UNMARK;
+
 	cfs_rq->curr = NULL;
 }
 
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0affe3be7c21..ff9ebf5da738 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -159,6 +159,11 @@ extern void call_trace_sched_update_nr_running(struct rq *rq, int count);
  */
 #define RUNTIME_INF		((u64)~0ULL)
 
+#ifdef CONFIG_CACULE_SCHED
+#define YIELD_MARK	0x8000000000000000ULL
+#define YIELD_UNMARK	0x7FFFFFFFFFFFFFFFULL
+#endif
+
 static inline int idle_policy(int policy)
 {
 	return policy == SCHED_IDLE;
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index e8cdedf74fed..e1146d89ef9e 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -1758,6 +1758,15 @@ static struct ctl_table kern_table[] = {
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec,
 	},
+	{
+		.procname	= "yield_type",
+		.data		= &sched_yield_type,
+		.maxlen		= sizeof (int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= &one_ul,
+	},
 #endif
 #ifdef CONFIG_SCHEDSTATS
 	{
