From 969bebb846fa83236860cd63ad92d8a5ed94aee0 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 25 Dec 2020 09:30:03 +0800
Subject: [PATCH 2/4] sched/alt: Introduce sched_cpu_topo_masks.

Introduce sched_cpu_topo_masks and rework best_mask_cpu(), which help to
prefered cpu implementation later.
---
 kernel/sched/alt_core.c  | 26 ++++++++++++++++------
 kernel/sched/alt_sched.h | 48 +++++++++++++++++++++++++++++++++++-----
 2 files changed, 62 insertions(+), 12 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 982562808cc7..4c008d3cd0db 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -92,6 +92,8 @@ static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
 
 DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_affinity_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_end_mask);
+
+DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_topo_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
 #ifdef CONFIG_SCHED_SMT
@@ -5874,42 +5876,52 @@ static void sched_init_topology_cpumask_early(void)
 	cpumask_t *tmp;
 
 	for_each_possible_cpu(cpu) {
+		/* init affinity masks */
 		tmp = per_cpu(sched_cpu_affinity_masks, cpu);
 
 		cpumask_copy(tmp, cpumask_of(cpu));
 		tmp++;
 		cpumask_copy(tmp, cpu_possible_mask);
 		cpumask_clear_cpu(cpu, tmp);
-		per_cpu(sched_cpu_llc_mask, cpu) = tmp;
 		per_cpu(sched_cpu_affinity_end_mask, cpu) = ++tmp;
+		/* init topo masks */
+		tmp = per_cpu(sched_cpu_topo_masks, cpu);
+
+		cpumask_copy(tmp, cpumask_of(cpu));
+		tmp++;
+		cpumask_copy(tmp, cpu_possible_mask);
+		per_cpu(sched_cpu_llc_mask, cpu) = tmp;
 		/*per_cpu(sd_llc_id, cpu) = cpu;*/
 	}
 }
 
 #define TOPOLOGY_CPUMASK(name, mask, last) \
-	if (cpumask_and(chk, chk, mask))					\
-		printk(KERN_INFO "sched: cpu#%02d affinity mask: 0x%08lx - "#name,\
-		       cpu, (chk++)->bits[0]);					\
+	if (cpumask_and(chk, chk, mask)) {					\
+		cpumask_copy(topo, mask);					\
+		printk(KERN_INFO "sched: cpu#%02d affinity: 0x%08lx topo: 0x%08lx - "#name,\
+		       cpu, (chk++)->bits[0], (topo++)->bits[0]);		\
+	}									\
 	if (!last)								\
 		cpumask_complement(chk, mask)
 
 static void sched_init_topology_cpumask(void)
 {
 	int cpu;
-	cpumask_t *chk;
+	cpumask_t *chk, *topo;
 
 	for_each_online_cpu(cpu) {
 		/* take chance to reset time slice for idle tasks */
 		cpu_rq(cpu)->idle->time_slice = sched_timeslice_ns;
 
 		chk = per_cpu(sched_cpu_affinity_masks, cpu) + 1;
+		topo = per_cpu(sched_cpu_topo_masks, cpu) + 1;
 
 		cpumask_complement(chk, cpumask_of(cpu));
 #ifdef CONFIG_SCHED_SMT
 		TOPOLOGY_CPUMASK(smt, topology_sibling_cpumask(cpu), false);
 #endif
 		per_cpu(sd_llc_id, cpu) = cpumask_first(cpu_coregroup_mask(cpu));
-		per_cpu(sched_cpu_llc_mask, cpu) = chk;
+		per_cpu(sched_cpu_llc_mask, cpu) = topo;
 		TOPOLOGY_CPUMASK(coregroup, cpu_coregroup_mask(cpu), false);
 
 		TOPOLOGY_CPUMASK(core, topology_core_cpumask(cpu), false);
@@ -5920,7 +5932,7 @@ static void sched_init_topology_cpumask(void)
 		printk(KERN_INFO "sched: cpu#%02d llc_id = %d, llc_mask idx = %d\n",
 		       cpu, per_cpu(sd_llc_id, cpu),
 		       (int) (per_cpu(sched_cpu_llc_mask, cpu) -
-			      per_cpu(sched_cpu_affinity_masks, cpu)));
+			      per_cpu(sched_cpu_topo_masks, cpu)));
 	}
 }
 #endif
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index fd75b7895469..5d6ee22875b9 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -223,7 +223,8 @@ enum {
 	NR_CPU_AFFINITY_LEVELS
 };
 
-DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_affinity_masks);
+DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_topo_masks);
+DECLARE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
 static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask,
 				  const cpumask_t *mask)
@@ -242,13 +243,50 @@ static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask,
 #endif
 }
 
-static inline int best_mask_cpu(int cpu, const cpumask_t *cpumask)
+static inline int best_mask_cpu(int cpu, cpumask_t *mask)
 {
 #if NR_CPUS <= 64
-	return __best_mask_cpu(cpu, cpumask, per_cpu(sched_cpu_affinity_masks, cpu));
+	unsigned long llc_match;
+	cpumask_t *chk = per_cpu(sched_cpu_llc_mask, cpu);
+
+	if ((llc_match = mask->bits[0] & chk->bits[0])) {
+		unsigned long match;
+
+		chk = per_cpu(sched_cpu_topo_masks, cpu);
+		if (mask->bits[0] & chk->bits[0])
+			return cpu;
+
+#ifdef CONFIG_SCHED_SMT
+		chk++;
+		if ((match = mask->bits[0] & chk->bits[0]))
+			return __ffs(match);
+#endif
+
+		return __ffs(llc_match);
+	}
+
+	return __best_mask_cpu(cpu, mask, chk + 1);
 #else
-	return cpumask_test_cpu(cpu, cpumask) ? cpu:
-		__best_mask_cpu(cpu, cpumask, per_cpu(sched_cpu_affinity_masks, cpu) + 1);
+	cpumask_t llc_match;
+	cpumask_t *chk = per_cpu(sched_cpu_llc_mask, cpu);
+
+	if (cpumask_and(&llc_match, mask, chk)) {
+		cpumask_t tmp;
+
+		chk = per_cpu(sched_cpu_topo_masks, cpu);
+		if (cpumask_test_cpu(cpu, mask))
+			return cpu;
+
+#ifdef CONFIG_SCHED_SMT
+		chk++;
+		if (cpumask_and(&tmp, mask, chk))
+			return cpumask_any(&tmp);
+#endif
+
+		return cpumask_any(&llc_match);
+	}
+
+	return __best_mask_cpu(cpu, mask, chk + 1);
 #endif
 }
 
-- 
2.30.0.rc0

