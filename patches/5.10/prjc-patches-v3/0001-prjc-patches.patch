From 8d6fcf9c940495d0de9f176126ebb125a4854c82 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 22 Dec 2020 11:08:31 +0800
Subject: [PATCH 1/4] sched/alt: rcu_read_xxx() put_task_xxxx() sync up.

---
 kernel/sched/alt_core.c | 19 +++++++++++--------
 1 file changed, 11 insertions(+), 8 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 1a857d7e230b..982562808cc7 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -4786,10 +4786,15 @@ SYSCALL_DEFINE3(sched_setattr, pid_t, pid, struct sched_attr __user *, uattr,
 	rcu_read_lock();
 	retval = -ESRCH;
 	p = find_process_by_pid(pid);
-	if (p != NULL)
-		retval = sched_setattr(p, &attr);
+	if (likely(p))
+		get_task_struct(p);
 	rcu_read_unlock();
 
+	if (likely(p)) {
+		retval = sched_setattr(p, &attr);
+		put_task_struct(p);
+	}
+
 	return retval;
 }
 
@@ -4961,13 +4966,11 @@ long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)
 	struct task_struct *p;
 	int retval;
 
-	get_online_cpus();
 	rcu_read_lock();
 
 	p = find_process_by_pid(pid);
 	if (!p) {
 		rcu_read_unlock();
-		put_online_cpus();
 		return -ESRCH;
 	}
 
@@ -4992,17 +4995,18 @@ long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)
 		rcu_read_lock();
 		if (!ns_capable(__task_cred(p)->user_ns, CAP_SYS_NICE)) {
 			rcu_read_unlock();
-			goto out_unlock;
+			goto out_free_new_mask;
 		}
 		rcu_read_unlock();
 	}
 
 	retval = security_task_setscheduler(p);
 	if (retval)
-		goto out_unlock;
+		goto out_free_new_mask;
 
 	cpuset_cpus_allowed(p, cpus_allowed);
 	cpumask_and(new_mask, in_mask, cpus_allowed);
+
 again:
 	retval = __set_cpus_allowed_ptr(p, new_mask, true);
 
@@ -5018,13 +5022,12 @@ long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)
 			goto again;
 		}
 	}
-out_unlock:
+out_free_new_mask:
 	free_cpumask_var(new_mask);
 out_free_cpus_allowed:
 	free_cpumask_var(cpus_allowed);
 out_put_task:
 	put_task_struct(p);
-	put_online_cpus();
 	return retval;
 }
 
-- 
2.30.0.rc0


From 969bebb846fa83236860cd63ad92d8a5ed94aee0 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 25 Dec 2020 09:30:03 +0800
Subject: [PATCH 2/4] sched/alt: Introduce sched_cpu_topo_masks.

Introduce sched_cpu_topo_masks and rework best_mask_cpu(), which help to
prefered cpu implementation later.
---
 kernel/sched/alt_core.c  | 26 ++++++++++++++++------
 kernel/sched/alt_sched.h | 48 +++++++++++++++++++++++++++++++++++-----
 2 files changed, 62 insertions(+), 12 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 982562808cc7..4c008d3cd0db 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -92,6 +92,8 @@ static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
 
 DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_affinity_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_end_mask);
+
+DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_topo_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
 #ifdef CONFIG_SCHED_SMT
@@ -5874,42 +5876,52 @@ static void sched_init_topology_cpumask_early(void)
 	cpumask_t *tmp;
 
 	for_each_possible_cpu(cpu) {
+		/* init affinity masks */
 		tmp = per_cpu(sched_cpu_affinity_masks, cpu);
 
 		cpumask_copy(tmp, cpumask_of(cpu));
 		tmp++;
 		cpumask_copy(tmp, cpu_possible_mask);
 		cpumask_clear_cpu(cpu, tmp);
-		per_cpu(sched_cpu_llc_mask, cpu) = tmp;
 		per_cpu(sched_cpu_affinity_end_mask, cpu) = ++tmp;
+		/* init topo masks */
+		tmp = per_cpu(sched_cpu_topo_masks, cpu);
+
+		cpumask_copy(tmp, cpumask_of(cpu));
+		tmp++;
+		cpumask_copy(tmp, cpu_possible_mask);
+		per_cpu(sched_cpu_llc_mask, cpu) = tmp;
 		/*per_cpu(sd_llc_id, cpu) = cpu;*/
 	}
 }
 
 #define TOPOLOGY_CPUMASK(name, mask, last) \
-	if (cpumask_and(chk, chk, mask))					\
-		printk(KERN_INFO "sched: cpu#%02d affinity mask: 0x%08lx - "#name,\
-		       cpu, (chk++)->bits[0]);					\
+	if (cpumask_and(chk, chk, mask)) {					\
+		cpumask_copy(topo, mask);					\
+		printk(KERN_INFO "sched: cpu#%02d affinity: 0x%08lx topo: 0x%08lx - "#name,\
+		       cpu, (chk++)->bits[0], (topo++)->bits[0]);		\
+	}									\
 	if (!last)								\
 		cpumask_complement(chk, mask)
 
 static void sched_init_topology_cpumask(void)
 {
 	int cpu;
-	cpumask_t *chk;
+	cpumask_t *chk, *topo;
 
 	for_each_online_cpu(cpu) {
 		/* take chance to reset time slice for idle tasks */
 		cpu_rq(cpu)->idle->time_slice = sched_timeslice_ns;
 
 		chk = per_cpu(sched_cpu_affinity_masks, cpu) + 1;
+		topo = per_cpu(sched_cpu_topo_masks, cpu) + 1;
 
 		cpumask_complement(chk, cpumask_of(cpu));
 #ifdef CONFIG_SCHED_SMT
 		TOPOLOGY_CPUMASK(smt, topology_sibling_cpumask(cpu), false);
 #endif
 		per_cpu(sd_llc_id, cpu) = cpumask_first(cpu_coregroup_mask(cpu));
-		per_cpu(sched_cpu_llc_mask, cpu) = chk;
+		per_cpu(sched_cpu_llc_mask, cpu) = topo;
 		TOPOLOGY_CPUMASK(coregroup, cpu_coregroup_mask(cpu), false);
 
 		TOPOLOGY_CPUMASK(core, topology_core_cpumask(cpu), false);
@@ -5920,7 +5932,7 @@ static void sched_init_topology_cpumask(void)
 		printk(KERN_INFO "sched: cpu#%02d llc_id = %d, llc_mask idx = %d\n",
 		       cpu, per_cpu(sd_llc_id, cpu),
 		       (int) (per_cpu(sched_cpu_llc_mask, cpu) -
-			      per_cpu(sched_cpu_affinity_masks, cpu)));
+			      per_cpu(sched_cpu_topo_masks, cpu)));
 	}
 }
 #endif
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index fd75b7895469..5d6ee22875b9 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -223,7 +223,8 @@ enum {
 	NR_CPU_AFFINITY_LEVELS
 };
 
-DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_affinity_masks);
+DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_topo_masks);
+DECLARE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
 static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask,
 				  const cpumask_t *mask)
@@ -242,13 +243,50 @@ static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask,
 #endif
 }
 
-static inline int best_mask_cpu(int cpu, const cpumask_t *cpumask)
+static inline int best_mask_cpu(int cpu, cpumask_t *mask)
 {
 #if NR_CPUS <= 64
-	return __best_mask_cpu(cpu, cpumask, per_cpu(sched_cpu_affinity_masks, cpu));
+	unsigned long llc_match;
+	cpumask_t *chk = per_cpu(sched_cpu_llc_mask, cpu);
+
+	if ((llc_match = mask->bits[0] & chk->bits[0])) {
+		unsigned long match;
+
+		chk = per_cpu(sched_cpu_topo_masks, cpu);
+		if (mask->bits[0] & chk->bits[0])
+			return cpu;
+
+#ifdef CONFIG_SCHED_SMT
+		chk++;
+		if ((match = mask->bits[0] & chk->bits[0]))
+			return __ffs(match);
+#endif
+
+		return __ffs(llc_match);
+	}
+
+	return __best_mask_cpu(cpu, mask, chk + 1);
 #else
-	return cpumask_test_cpu(cpu, cpumask) ? cpu:
-		__best_mask_cpu(cpu, cpumask, per_cpu(sched_cpu_affinity_masks, cpu) + 1);
+	cpumask_t llc_match;
+	cpumask_t *chk = per_cpu(sched_cpu_llc_mask, cpu);
+
+	if (cpumask_and(&llc_match, mask, chk)) {
+		cpumask_t tmp;
+
+		chk = per_cpu(sched_cpu_topo_masks, cpu);
+		if (cpumask_test_cpu(cpu, mask))
+			return cpu;
+
+#ifdef CONFIG_SCHED_SMT
+		chk++;
+		if (cpumask_and(&tmp, mask, chk))
+			return cpumask_any(&tmp);
+#endif
+
+		return cpumask_any(&llc_match);
+	}
+
+	return __best_mask_cpu(cpu, mask, chk + 1);
 #endif
 }
 
-- 
2.30.0.rc0


From d52228599109025fa9204496f5983ae1a96c4094 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 25 Dec 2020 09:33:25 +0800
Subject: [PATCH 3/4] Project-C v5.10-r1

---
 kernel/sched/alt_core.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 4c008d3cd0db..9880d9b50f7e 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -52,7 +52,7 @@
  */
 EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_irq_tp);
 
-#define ALT_SCHED_VERSION "v5.10-r0"
+#define ALT_SCHED_VERSION "v5.10-r1"
 
 /* rt_prio(prio) defined in include/linux/sched/rt.h */
 #define rt_task(p)		rt_prio((p)->prio)
-- 
2.30.0.rc0


From bc8ff377855876b7709fb5cc74ac70f96ef72fef Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 25 Dec 2020 11:33:48 +0800
Subject: [PATCH 4/4] sched/alt: Fix UP compilation warning.

---
 kernel/sched/bmq_imp.h | 2 ++
 kernel/sched/pds_imp.h | 2 ++
 2 files changed, 4 insertions(+)

diff --git a/kernel/sched/bmq_imp.h b/kernel/sched/bmq_imp.h
index 3faba5f9bb69..13eda4b26b6a 100644
--- a/kernel/sched/bmq_imp.h
+++ b/kernel/sched/bmq_imp.h
@@ -185,11 +185,13 @@ static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
 	p->boost_prio = MAX_PRIORITY_ADJ;
 }
 
+#ifdef CONFIG_SMP
 static void sched_task_ttwu(struct task_struct *p)
 {
 	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
 		boost_task(p);
 }
+#endif
 
 static void sched_task_deactivate(struct task_struct *p, struct rq *rq)
 {
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
index 6b2140f0a69e..b1ad3d0b0430 100644
--- a/kernel/sched/pds_imp.h
+++ b/kernel/sched/pds_imp.h
@@ -268,5 +268,7 @@ static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
 	time_slice_expired(p, rq);
 }
 
+#ifdef CONFIG_SMP
 static void sched_task_ttwu(struct task_struct *p) {}
+#endif
 static void sched_task_deactivate(struct task_struct *p, struct rq *rq) {}
-- 
2.30.0.rc0

