From 60a368158c97b4014575ab80259f26558234855e Mon Sep 17 00:00:00 2001
From: Paolo Valente <paolo.valente@linaro.org>
Date: Thu, 26 Mar 2020 10:05:42 +0100
Subject: [PATCH 07/25] block, bfq, DEBUG: move check bfqd->queued to
 lock-protected sections

Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
---
 block/bfq-iosched.c | 23 +++++++++--------------
 1 file changed, 9 insertions(+), 14 deletions(-)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index db3083269..98921f6dd 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -5201,26 +5201,12 @@ static struct request *bfq_dispatch_rq_from_bfqq(struct bfq_data *bfqd,
 
 static bool bfq_has_work(struct blk_mq_hw_ctx *hctx)
 {
-#ifndef CONFIG_BFQ_MQ_NOLOG_BUG_ON
-	unsigned long flags;
-	bool condition, acquired = false;
-#endif
 	struct bfq_data *bfqd = hctx->queue->elevator->elevator_data;
 
 	bfq_log(bfqd, "dispatch_non_empty %d busy_queues %d",
 		!list_empty_careful(&bfqd->dispatch),
 		bfq_tot_busy_queues(bfqd) > 0);
 
-#ifndef CONFIG_BFQ_MQ_NOLOG_BUG_ON
-	if (!lock_is_held(&(bfqd->lock.dep_map))) {
-		spin_lock_irqsave(&bfqd->lock, flags);
-		acquired = true;
-	}
-	condition = bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0;
-	if (acquired)
-		spin_unlock_irqrestore(&bfqd->lock, flags);
-	BFQ_BUG_ON(condition);
-#endif
 	/*
 	 * Avoiding lock: a race on bfqd->busy_queues should cause at
 	 * most a call to dispatch for nothing
@@ -5421,6 +5407,8 @@ static struct request *bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)
 	idle_timer_disabled =
 		waiting_rq && !bfq_bfqq_wait_request(in_serv_queue);
 
+	BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0);
+
 	spin_unlock_irq(&bfqd->lock);
 
 	bfq_update_dispatch_stats(hctx->queue, rq, in_serv_queue,
@@ -5582,6 +5570,7 @@ static void bfq_exit_icq_bfqq(struct bfq_io_cq *bic, bool is_sync)
 		bfqq->bic = NULL;
 		bfq_exit_bfqq(bfqd, bfqq);
 		bic_set_bfqq(bic, NULL, is_sync);
+		BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0);
 		spin_unlock_irqrestore(&bfqd->lock, flags);
 	}
 }
@@ -6167,6 +6156,8 @@ static void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 		bfqg_stats_update_legacy_io(q, rq);
 #endif
 	spin_lock_irq(&bfqd->lock);
+	BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0);
+
 	if (blk_mq_sched_try_insert_merge(q, rq)) {
 		spin_unlock_irq(&bfqd->lock);
 		return;
@@ -6223,6 +6214,8 @@ static void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	 */
 	cmd_flags = rq->cmd_flags;
 
+	BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0);
+
 	spin_unlock_irq(&bfqd->lock);
 
 	bfq_update_insert_stats(q, bfqq, idle_timer_disabled,
@@ -6689,6 +6682,8 @@ static void bfq_finish_requeue_request(struct request *rq)
 		bfq_completed_request(bfqq, bfqd);
 		bfq_finish_requeue_request_body(bfqq);
 
+		BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0);
+
 		spin_unlock_irqrestore(&bfqd->lock, flags);
 	} else {
 		/*
-- 
2.30.0.rc0

