From 12d8a0961fa22f9d58a7c2977c157bb899be524b Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Fri, 25 Dec 2020 22:17:27 +0100
Subject: [PATCH 11/12] ntfs3-5.10: update to v16

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 fs/ntfs3/Kconfig                 |  10 +
 fs/ntfs3/Makefile                |   1 -
 fs/ntfs3/attrib.c                | 400 +++++++++++++++++-
 fs/ntfs3/file.c                  | 336 ++++++++-------
 fs/ntfs3/frecord.c               |  98 ++---
 fs/ntfs3/fsntfs.c                |   3 +-
 fs/ntfs3/index.c                 |   4 +-
 fs/ntfs3/inode.c                 |  13 +-
 fs/ntfs3/lib/common_defs.h       | 196 ---------
 fs/ntfs3/lib/decompress_common.c | 472 +++++++++++----------
 fs/ntfs3/lib/decompress_common.h | 546 ++++++++----------------
 fs/ntfs3/lib/lib.h               |  10 +-
 fs/ntfs3/lib/lzx_common.c        | 204 ---------
 fs/ntfs3/lib/lzx_common.h        |  31 --
 fs/ntfs3/lib/lzx_constants.h     | 113 -----
 fs/ntfs3/lib/lzx_decompress.c    | 683 ++++++++++++++++++-------------
 fs/ntfs3/lib/xpress_constants.h  |  23 --
 fs/ntfs3/lib/xpress_decompress.c | 155 ++++---
 fs/ntfs3/ntfs_fs.h               |  39 +-
 fs/ntfs3/run.c                   |  62 +++
 fs/ntfs3/super.c                 |  12 +-
 fs/ntfs3/xattr.c                 | 145 ++++---
 22 files changed, 1723 insertions(+), 1833 deletions(-)
 delete mode 100644 fs/ntfs3/lib/common_defs.h
 delete mode 100644 fs/ntfs3/lib/lzx_common.c
 delete mode 100644 fs/ntfs3/lib/lzx_common.h
 delete mode 100644 fs/ntfs3/lib/lzx_constants.h
 delete mode 100644 fs/ntfs3/lib/xpress_constants.h

diff --git a/fs/ntfs3/Kconfig b/fs/ntfs3/Kconfig
index b44665104..f9b732f4a 100644
--- a/fs/ntfs3/Kconfig
+++ b/fs/ntfs3/Kconfig
@@ -29,3 +29,13 @@ config NTFS3_LZX_XPRESS
 	  In Windows 10 one can use command "compact" to compress any files.
 	  4 possible variants of compression are: xpress4k, xpress8k, xpress16 and lzx.
 	  To read such "compacted" files say Y here.
+
+config NTFS3_POSIX_ACL
+	bool "NTFS POSIX Access Control Lists"
+	depends on NTFS3_FS
+	select FS_POSIX_ACL
+	help
+	  POSIX Access Control Lists (ACLs) support additional access rights
+	  for users and groups beyond the standard owner/group/world scheme,
+	  and this option selects support for ACLs specifically for ntfs
+	  filesystems.
diff --git a/fs/ntfs3/Makefile b/fs/ntfs3/Makefile
index 60151bea5..b9aacc061 100644
--- a/fs/ntfs3/Makefile
+++ b/fs/ntfs3/Makefile
@@ -26,7 +26,6 @@ ntfs3-y :=	attrib.o \
 
 ntfs3-$(CONFIG_NTFS3_LZX_XPRESS) += $(addprefix lib/,\
 		decompress_common.o \
-		lzx_common.o \
 		lzx_decompress.o \
 		xpress_decompress.o \
 		)
\ No newline at end of file
diff --git a/fs/ntfs3/attrib.c b/fs/ntfs3/attrib.c
index b6340181d..3f4c847c4 100644
--- a/fs/ntfs3/attrib.c
+++ b/fs/ntfs3/attrib.c
@@ -168,7 +168,7 @@ static int run_deallocate_ex(struct ntfs_sb_info *sbi, struct runs_tree *run,
 
 out:
 	if (done)
-		*done = dn;
+		*done += dn;
 
 	return err;
 }
@@ -686,6 +686,7 @@ int attr_set_size(struct ntfs_inode *ni, enum ATTR_TYPE type,
 		vcn = max(svcn, new_alen);
 		new_alloc_tmp = (u64)vcn << cluster_bits;
 
+		alen = 0;
 		err = run_deallocate_ex(sbi, run, vcn, evcn - vcn + 1, &alen,
 					true);
 		if (err)
@@ -1680,3 +1681,400 @@ int attr_allocate_frame(struct ntfs_inode *ni, CLST frame, size_t compr_size,
 
 	return err;
 }
+
+/* Collapse range in file */
+int attr_collapse_range(struct ntfs_inode *ni, u64 vbo, u64 bytes)
+{
+	int err = 0;
+	struct runs_tree *run = &ni->file.run;
+	struct ntfs_sb_info *sbi = ni->mi.sbi;
+	struct ATTRIB *attr, *attr_b;
+	struct ATTR_LIST_ENTRY *le, *le_b;
+	struct mft_inode *mi, *mi_b;
+	CLST svcn, evcn1, len, dealloc, alen;
+	CLST vcn, end;
+	u64 valid_size, data_size, alloc_size, total_size;
+	u32 mask;
+	__le16 a_flags;
+
+	if (!bytes)
+		return 0;
+
+	le_b = NULL;
+	attr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL, &mi_b);
+	if (!attr_b)
+		return -ENOENT;
+
+	if (!attr_b->non_res) {
+		/* Attribute is resident. Nothing to do? */
+		return 0;
+	}
+
+	data_size = le64_to_cpu(attr_b->nres.data_size);
+	valid_size = le64_to_cpu(attr_b->nres.valid_size);
+	alloc_size = le64_to_cpu(attr_b->nres.alloc_size);
+	a_flags = attr_b->flags;
+
+	if (is_attr_ext(attr_b)) {
+		total_size = le64_to_cpu(attr_b->nres.total_size);
+		mask = (1u << (attr_b->nres.c_unit + sbi->cluster_bits)) - 1;
+	} else {
+		total_size = alloc_size;
+		mask = sbi->cluster_mask;
+	}
+
+	if (vbo & mask)
+		return -EINVAL;
+
+	if (bytes & mask)
+		return -EINVAL;
+
+	if (vbo > data_size)
+		return -EINVAL;
+
+	down_write(&ni->file.run_lock);
+
+	if (vbo + bytes >= data_size) {
+		u64 new_valid = min(ni->i_valid, vbo);
+
+		/* Simple truncate file at 'vbo' */
+		truncate_setsize(&ni->vfs_inode, vbo);
+		err = attr_set_size(ni, ATTR_DATA, NULL, 0, &ni->file.run, vbo,
+				    &new_valid, true, NULL);
+
+		if (!err && new_valid < ni->i_valid)
+			ni->i_valid = new_valid;
+
+		goto out;
+	}
+
+	/*
+	 * Enumerate all attribute segments and collapse
+	 */
+	alen = alloc_size >> sbi->cluster_bits;
+	vcn = vbo >> sbi->cluster_bits;
+	len = bytes >> sbi->cluster_bits;
+	end = vcn + len;
+	dealloc = 0;
+
+	svcn = le64_to_cpu(attr_b->nres.svcn);
+	evcn1 = le64_to_cpu(attr_b->nres.evcn) + 1;
+
+	if (svcn <= vcn && vcn < evcn1) {
+		attr = attr_b;
+		le = le_b;
+		mi = mi_b;
+	} else if (!le_b) {
+		err = -EINVAL;
+		goto out;
+	} else {
+		le = le_b;
+		attr = ni_find_attr(ni, attr_b, &le, ATTR_DATA, NULL, 0, &vcn,
+				    &mi);
+		if (!attr) {
+			err = -EINVAL;
+			goto out;
+		}
+
+		svcn = le64_to_cpu(attr->nres.svcn);
+		evcn1 = le64_to_cpu(attr->nres.evcn) + 1;
+	}
+
+	for (;;) {
+		if (svcn >= end) {
+			/* shift vcn */
+			attr->nres.svcn = cpu_to_le64(svcn - len);
+			attr->nres.evcn = cpu_to_le64(evcn1 - 1 - len);
+			if (le) {
+				le->vcn = attr->nres.svcn;
+				ni->attr_list.dirty = true;
+			}
+			mi->dirty = true;
+		} else if (svcn < vcn || end < evcn1) {
+			CLST vcn1, eat, next_svcn;
+
+			/* collapse a part of this attribute segment */
+			err = attr_load_runs(attr, ni, run, &svcn);
+			if (err)
+				goto out;
+			vcn1 = max(vcn, svcn);
+			eat = min(end, evcn1) - vcn1;
+
+			err = run_deallocate_ex(sbi, run, vcn1, eat, &dealloc,
+						true);
+			if (err)
+				goto out;
+
+			if (!run_collapse_range(run, vcn1, eat)) {
+				err = -ENOMEM;
+				goto out;
+			}
+
+			if (svcn >= vcn) {
+				/* shift vcn */
+				attr->nres.svcn = cpu_to_le64(vcn);
+				if (le) {
+					le->vcn = attr->nres.svcn;
+					ni->attr_list.dirty = true;
+				}
+			}
+
+			err = mi_pack_runs(mi, attr, run, evcn1 - svcn - eat);
+			if (err)
+				goto out;
+
+			next_svcn = le64_to_cpu(attr->nres.evcn) + 1;
+			if (next_svcn + eat < evcn1) {
+				err = ni_insert_nonresident(
+					ni, ATTR_DATA, NULL, 0, run, next_svcn,
+					evcn1 - eat - next_svcn, a_flags, &attr,
+					&mi);
+				if (err)
+					goto out;
+
+				/* layout of records maybe changed */
+				attr_b = NULL;
+				le = al_find_ex(ni, NULL, ATTR_DATA, NULL, 0,
+						&next_svcn);
+				if (!le) {
+					err = -EINVAL;
+					goto out;
+				}
+			}
+
+			/* free all allocated memory */
+			run_truncate(run, 0);
+		} else {
+			u16 le_sz;
+			u16 roff = le16_to_cpu(attr->nres.run_off);
+
+			/*run==1 means unpack and deallocate*/
+			run_unpack_ex(RUN_DEALLOCATE, sbi, ni->mi.rno, svcn,
+				      evcn1 - 1, svcn, Add2Ptr(attr, roff),
+				      le32_to_cpu(attr->size) - roff);
+
+			/* delete this attribute segment */
+			mi_remove_attr(mi, attr);
+			if (!le)
+				break;
+
+			le_sz = le16_to_cpu(le->size);
+			if (!al_remove_le(ni, le)) {
+				err = -EINVAL;
+				goto out;
+			}
+
+			if (evcn1 >= alen)
+				break;
+
+			if (!svcn) {
+				/* Load next record that contains this attribute */
+				if (ni_load_mi(ni, le, &mi)) {
+					err = -EINVAL;
+					goto out;
+				}
+
+				/* Look for required attribute */
+				attr = mi_find_attr(mi, NULL, ATTR_DATA, NULL,
+						    0, &le->id);
+				if (!attr) {
+					err = -EINVAL;
+					goto out;
+				}
+				goto next_attr;
+			}
+			le = (struct ATTR_LIST_ENTRY *)((u8 *)le - le_sz);
+		}
+
+		if (evcn1 >= alen)
+			break;
+
+		attr = ni_enum_attr_ex(ni, attr, &le, &mi);
+		if (!attr) {
+			err = -EINVAL;
+			goto out;
+		}
+
+next_attr:
+		svcn = le64_to_cpu(attr->nres.svcn);
+		evcn1 = le64_to_cpu(attr->nres.evcn) + 1;
+	}
+
+	if (vbo + bytes <= valid_size)
+		valid_size -= bytes;
+	else if (vbo < valid_size)
+		valid_size = vbo;
+
+	if (!attr_b) {
+		le_b = NULL;
+		attr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL,
+				      &mi_b);
+		if (!attr_b) {
+			err = -ENOENT;
+			goto out;
+		}
+	}
+
+	attr_b->nres.alloc_size = cpu_to_le64(alloc_size - bytes);
+	attr_b->nres.data_size = cpu_to_le64(data_size - bytes);
+	attr_b->nres.valid_size = cpu_to_le64(valid_size);
+	total_size -= (u64)dealloc << sbi->cluster_bits;
+	if (is_attr_ext(attr_b))
+		attr_b->nres.total_size = cpu_to_le64(total_size);
+	mi_b->dirty = true;
+
+	/*update inode size*/
+	ni->i_valid = valid_size;
+	ni->vfs_inode.i_size = data_size - bytes;
+	inode_set_bytes(&ni->vfs_inode, total_size);
+	ni->ni_flags |= NI_FLAG_UPDATE_PARENT;
+	mark_inode_dirty(&ni->vfs_inode);
+
+out:
+	up_write(&ni->file.run_lock);
+	if (err)
+		make_bad_inode(&ni->vfs_inode);
+
+	return err;
+}
+
+/* not for normal files */
+int attr_punch_hole(struct ntfs_inode *ni, u64 vbo, u64 bytes)
+{
+	int err = 0;
+	struct runs_tree *run = &ni->file.run;
+	struct ntfs_sb_info *sbi = ni->mi.sbi;
+	struct ATTRIB *attr, *attr_b;
+	struct ATTR_LIST_ENTRY *le, *le_b;
+	struct mft_inode *mi, *mi_b;
+	CLST svcn, evcn1, vcn, len, end, alen, dealloc;
+	u64 total_size, alloc_size;
+
+	if (!bytes)
+		return 0;
+
+	le_b = NULL;
+	attr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL, &mi_b);
+	if (!attr_b)
+		return -ENOENT;
+
+	if (!attr_b->non_res) {
+		u32 data_size = le32_to_cpu(attr->res.data_size);
+		u32 from, to;
+
+		if (vbo > data_size)
+			return 0;
+
+		from = vbo;
+		to = (vbo + bytes) < data_size ? (vbo + bytes) : data_size;
+		memset(Add2Ptr(resident_data(attr_b), from), 0, to - from);
+		return 0;
+	}
+
+	/* TODO: add support for normal files too */
+	if (!is_attr_ext(attr_b))
+		return -EOPNOTSUPP;
+
+	alloc_size = le64_to_cpu(attr_b->nres.alloc_size);
+	total_size = le64_to_cpu(attr_b->nres.total_size);
+
+	if (vbo >= alloc_size) {
+		// NOTE: it is allowed
+		return 0;
+	}
+
+	if (vbo + bytes > alloc_size)
+		bytes = alloc_size - vbo;
+
+	down_write(&ni->file.run_lock);
+	/*
+	 * Enumerate all attribute segments and punch hole where necessary
+	 */
+	alen = alloc_size >> sbi->cluster_bits;
+	vcn = vbo >> sbi->cluster_bits;
+	len = bytes >> sbi->cluster_bits;
+	end = vcn + len;
+	dealloc = 0;
+
+	svcn = le64_to_cpu(attr_b->nres.svcn);
+	evcn1 = le64_to_cpu(attr_b->nres.evcn) + 1;
+
+	if (svcn <= vcn && vcn < evcn1) {
+		attr = attr_b;
+		le = le_b;
+		mi = mi_b;
+	} else if (!le_b) {
+		err = -EINVAL;
+		goto out;
+	} else {
+		le = le_b;
+		attr = ni_find_attr(ni, attr_b, &le, ATTR_DATA, NULL, 0, &vcn,
+				    &mi);
+		if (!attr) {
+			err = -EINVAL;
+			goto out;
+		}
+
+		svcn = le64_to_cpu(attr->nres.svcn);
+		evcn1 = le64_to_cpu(attr->nres.evcn) + 1;
+	}
+
+	while (svcn < end) {
+		CLST vcn1, zero, dealloc2;
+
+		err = attr_load_runs(attr, ni, run, &svcn);
+		if (err)
+			goto out;
+		vcn1 = max(vcn, svcn);
+		zero = min(end, evcn1) - vcn1;
+
+		dealloc2 = dealloc;
+		err = run_deallocate_ex(sbi, run, vcn1, zero, &dealloc, true);
+		if (err)
+			goto out;
+
+		if (dealloc2 == dealloc) {
+			/* looks like  the required range is already sparsed */
+		} else {
+			if (!run_add_entry(run, vcn1, SPARSE_LCN, zero,
+					   false)) {
+				err = -ENOMEM;
+				goto out;
+			}
+
+			err = mi_pack_runs(mi, attr, run, evcn1 - svcn);
+			if (err)
+				goto out;
+		}
+		/* free all allocated memory */
+		run_truncate(run, 0);
+
+		if (evcn1 >= alen)
+			break;
+
+		attr = ni_enum_attr_ex(ni, attr, &le, &mi);
+		if (!attr) {
+			err = -EINVAL;
+			goto out;
+		}
+
+		svcn = le64_to_cpu(attr->nres.svcn);
+		evcn1 = le64_to_cpu(attr->nres.evcn) + 1;
+	}
+
+	total_size -= (u64)dealloc << sbi->cluster_bits;
+	attr_b->nres.total_size = cpu_to_le64(total_size);
+	mi_b->dirty = true;
+
+	/*update inode size*/
+	inode_set_bytes(&ni->vfs_inode, total_size);
+	ni->ni_flags |= NI_FLAG_UPDATE_PARENT;
+	mark_inode_dirty(&ni->vfs_inode);
+
+out:
+	up_write(&ni->file.run_lock);
+	if (err)
+		make_bad_inode(&ni->vfs_inode);
+
+	return err;
+}
diff --git a/fs/ntfs3/file.c b/fs/ntfs3/file.c
index 0193ac39f..a65ca1673 100644
--- a/fs/ntfs3/file.c
+++ b/fs/ntfs3/file.c
@@ -350,8 +350,8 @@ int ntfs_file_fsync(struct file *filp, loff_t start, loff_t end, int datasync)
 	return generic_file_fsync(filp, start, end, datasync);
 }
 
-static int ntfs_extend_ex(struct inode *inode, loff_t pos, size_t count,
-			  struct file *file)
+static int ntfs_extend(struct inode *inode, loff_t pos, size_t count,
+		       struct file *file)
 {
 	struct ntfs_inode *ni = ntfs_i(inode);
 	struct address_space *mapping = inode->i_mapping;
@@ -401,6 +401,63 @@ static int ntfs_extend_ex(struct inode *inode, loff_t pos, size_t count,
 	return err;
 }
 
+static int ntfs_truncate(struct inode *inode, loff_t new_size)
+{
+	struct super_block *sb = inode->i_sb;
+	struct ntfs_sb_info *sbi = sb->s_fs_info;
+	struct ntfs_inode *ni = ntfs_i(inode);
+	int err, dirty = 0;
+	u32 vcn;
+	u64 new_valid;
+
+	if (!S_ISREG(inode->i_mode))
+		return 0;
+
+	if (is_compressed(ni)) {
+		if (ni->i_valid > new_size)
+			ni->i_valid = new_size;
+	} else {
+		err = block_truncate_page(inode->i_mapping, new_size,
+					  ntfs_get_block);
+		if (err)
+			return err;
+	}
+
+	vcn = bytes_to_cluster(sbi, new_size);
+	new_valid = ntfs_up_block(sb, min_t(u64, ni->i_valid, new_size));
+
+	ni_lock(ni);
+
+	truncate_setsize(inode, new_size);
+
+	down_write(&ni->file.run_lock);
+	err = attr_set_size(ni, ATTR_DATA, NULL, 0, &ni->file.run, new_size,
+			    &new_valid, true, NULL);
+	up_write(&ni->file.run_lock);
+
+	if (new_valid < ni->i_valid)
+		ni->i_valid = new_valid;
+
+	ni_unlock(ni);
+
+	ni->std_fa |= FILE_ATTRIBUTE_ARCHIVE;
+	inode->i_ctime = inode->i_mtime = current_time(inode);
+	if (!IS_DIRSYNC(inode)) {
+		dirty = 1;
+	} else {
+		err = ntfs_sync_inode(inode);
+		if (err)
+			return err;
+	}
+
+	if (dirty)
+		mark_inode_dirty(inode);
+
+	/*ntfs_flush_inodes(inode->i_sb, inode, NULL);*/
+
+	return 0;
+}
+
 /*
  * Preallocate space for a file. This implements ntfs's fallocate file
  * operation, which gets called from sys_fallocate system call. User
@@ -414,8 +471,9 @@ static long ntfs_fallocate(struct file *file, int mode, loff_t vbo, loff_t len)
 	struct super_block *sb = inode->i_sb;
 	struct ntfs_sb_info *sbi = sb->s_fs_info;
 	struct ntfs_inode *ni = ntfs_i(inode);
+	loff_t end = vbo + len;
+	loff_t vbo_down = round_down(vbo, PAGE_SIZE);
 	loff_t i_size;
-	loff_t end;
 	int err;
 
 	/* No support for dir */
@@ -427,89 +485,135 @@ static long ntfs_fallocate(struct file *file, int mode, loff_t vbo, loff_t len)
 		     FALLOC_FL_COLLAPSE_RANGE))
 		return -EOPNOTSUPP;
 
+	ntfs_set_state(sbi, NTFS_DIRTY_DIRTY);
+
 	inode_lock(inode);
 	i_size = inode->i_size;
 
+	if (WARN_ON(ni->ni_flags & NI_FLAG_COMPRESSED_MASK)) {
+		/* should never be here, see ntfs_file_open*/
+		err = -EOPNOTSUPP;
+		goto out;
+	}
+
 	if (mode & FALLOC_FL_PUNCH_HOLE) {
 		if (!(mode & FALLOC_FL_KEEP_SIZE)) {
 			err = -EINVAL;
 			goto out;
 		}
-		/*TODO: add support*/
-		err = -EOPNOTSUPP;
-		goto out;
-	}
 
-	if (mode & FALLOC_FL_COLLAPSE_RANGE) {
-		if (mode & ~FALLOC_FL_COLLAPSE_RANGE) {
-			err = -EINVAL;
+		if (!is_sparsed(ni) && !is_compressed(ni)) {
+			ntfs_inode_warn(
+				inode,
+				"punch_hole only for sparsed/compressed files");
+			err = -EOPNOTSUPP;
 			goto out;
 		}
 
-		/*TODO: add support*/
-		err = -EOPNOTSUPP;
-		goto out;
-	}
+		err = filemap_write_and_wait_range(inode->i_mapping, vbo,
+						   end - 1);
+		if (err)
+			goto out;
 
-	end = vbo + len;
+		err = filemap_write_and_wait_range(inode->i_mapping, end,
+						   LLONG_MAX);
+		if (err)
+			goto out;
 
-	ntfs_set_state(sbi, NTFS_DIRTY_DIRTY);
+		truncate_pagecache(inode, vbo_down);
 
-	/*
-	 * normal file: allocate clusters, do not change 'valid' size
-	 */
-	err = ntfs_set_size(inode, max(end, i_size));
-	if (err)
-		goto out;
+		ni_lock(ni);
+		err = attr_punch_hole(ni, vbo, len);
+		ni_unlock(ni);
+	} else if (mode & FALLOC_FL_COLLAPSE_RANGE) {
+		if (mode & ~FALLOC_FL_COLLAPSE_RANGE) {
+			err = -EINVAL;
+			goto out;
+		}
 
-	if (is_sparsed(ni) || is_compressed(ni)) {
-		CLST vcn_v = ni->i_valid >> sbi->cluster_bits;
-		CLST vcn = vbo >> sbi->cluster_bits;
-		CLST cend = bytes_to_cluster(sbi, end);
-		CLST lcn, clen;
-		bool new;
+		/*
+		 * Write tail of the last page before removed range since
+		 * it will get removed from the page cache below.
+		 */
+		err = filemap_write_and_wait_range(inode->i_mapping, vbo_down,
+						   vbo);
+		if (err)
+			goto out;
 
 		/*
-		 * allocate but not zero new clusters (see below comments)
-		 * this breaks security (one can read unused on-disk areas)
-		 * zeroing these clusters may be too long
-		 * may be we should check here for root rights?
+		 * Write data that will be shifted to preserve them
+		 * when discarding page cache below
 		 */
-		for (; vcn < cend; vcn += clen) {
-			err = attr_data_get_block(ni, vcn, cend - vcn, &lcn,
-						  &clen, &new);
-			if (err)
-				goto out;
-			if (!new || vcn >= vcn_v)
-				continue;
+		err = filemap_write_and_wait_range(inode->i_mapping, end,
+						   LLONG_MAX);
+		if (err)
+			goto out;
 
-			/*
-			 * This variant zeroes new allocated clusters inside valid size
-			 * Dangerous in case:
-			 * 1G of sparsed clusters + 1 cluster of data =>
-			 * valid_size == 1G + 1 cluster
-			 * fallocate(1G) will zero 1G and this can be very long
-			 * xfstest 086 will fail if below function is not called
-			 */
-			/*ntfs_sparse_cluster(inode, NULL, vcn,
-			 *		    min(vcn_v - vcn, clen));
-			 */
-		}
-	}
+		truncate_pagecache(inode, vbo_down);
 
-	if (mode & FALLOC_FL_KEEP_SIZE) {
 		ni_lock(ni);
-		/*true - keep preallocated*/
-		err = attr_set_size(ni, ATTR_DATA, NULL, 0, &ni->file.run,
-				    i_size, &ni->i_valid, true, NULL);
+		err = attr_collapse_range(ni, vbo, len);
 		ni_unlock(ni);
+	} else {
+		/*
+		 * normal file: allocate clusters, do not change 'valid' size
+		 */
+		err = ntfs_set_size(inode, max(end, i_size));
 		if (err)
 			goto out;
-	}
 
-	inode->i_ctime = inode->i_mtime = current_time(inode);
-	mark_inode_dirty(inode);
+		if (is_sparsed(ni) || is_compressed(ni)) {
+			CLST vcn_v = ni->i_valid >> sbi->cluster_bits;
+			CLST vcn = vbo >> sbi->cluster_bits;
+			CLST cend = bytes_to_cluster(sbi, end);
+			CLST lcn, clen;
+			bool new;
 
+			/*
+			 * allocate but not zero new clusters (see below comments)
+			 * this breaks security (one can read unused on-disk areas)
+			 * zeroing these clusters may be too long
+			 * may be we should check here for root rights?
+			 */
+			for (; vcn < cend; vcn += clen) {
+				err = attr_data_get_block(ni, vcn, cend - vcn,
+							  &lcn, &clen, &new);
+				if (err)
+					goto out;
+				if (!new || vcn >= vcn_v)
+					continue;
+
+				/*
+				 * Unwritten area
+				 * NTFS is not able to store several unwritten areas
+				 * Activate 'ntfs_sparse_cluster' to zero new allocated clusters
+				 *
+				 * Dangerous in case:
+				 * 1G of sparsed clusters + 1 cluster of data =>
+				 * valid_size == 1G + 1 cluster
+				 * fallocate(1G) will zero 1G and this can be very long
+				 * xfstest 016/086 will fail whithout 'ntfs_sparse_cluster'
+				 */
+				/*ntfs_sparse_cluster(inode, NULL, vcn,
+				 *		    min(vcn_v - vcn, clen));
+				 */
+			}
+		}
+
+		if (mode & FALLOC_FL_KEEP_SIZE) {
+			ni_lock(ni);
+			/*true - keep preallocated*/
+			err = attr_set_size(ni, ATTR_DATA, NULL, 0,
+					    &ni->file.run, i_size, &ni->i_valid,
+					    true, NULL);
+			ni_unlock(ni);
+		}
+	}
+
+	if (!err) {
+		inode->i_ctime = inode->i_mtime = current_time(inode);
+		mark_inode_dirty(inode);
+	}
 out:
 	if (err == -EFBIG)
 		err = -ENOSPC;
@@ -518,51 +622,6 @@ static long ntfs_fallocate(struct file *file, int mode, loff_t vbo, loff_t len)
 	return err;
 }
 
-void ntfs_truncate_blocks(struct inode *inode, loff_t new_size)
-{
-	struct super_block *sb = inode->i_sb;
-	struct ntfs_sb_info *sbi = sb->s_fs_info;
-	struct ntfs_inode *ni = ntfs_i(inode);
-	int err, dirty = 0;
-	u32 vcn;
-	u64 new_valid;
-
-	if (!S_ISREG(inode->i_mode))
-		return;
-
-	vcn = bytes_to_cluster(sbi, new_size);
-	new_valid = ntfs_up_block(sb, min(ni->i_valid, new_size));
-
-	ni_lock(ni);
-
-	truncate_setsize(inode, new_size);
-
-	down_write(&ni->file.run_lock);
-	err = attr_set_size(ni, ATTR_DATA, NULL, 0, &ni->file.run, new_size,
-			    &new_valid, true, NULL);
-	up_write(&ni->file.run_lock);
-
-	if (new_valid < ni->i_valid)
-		ni->i_valid = new_valid;
-
-	ni_unlock(ni);
-
-	ni->std_fa |= FILE_ATTRIBUTE_ARCHIVE;
-	inode->i_ctime = inode->i_mtime = current_time(inode);
-	if (!IS_DIRSYNC(inode)) {
-		dirty = 1;
-	} else {
-		err = ntfs_sync_inode(inode);
-		if (err)
-			return;
-	}
-
-	if (dirty)
-		mark_inode_dirty(inode);
-
-	/*ntfs_flush_inodes(inode->i_sb, inode, NULL);*/
-}
-
 /*
  * inode_operations::setattr
  */
@@ -591,38 +650,20 @@ int ntfs3_setattr(struct dentry *dentry, struct iattr *attr)
 	if (ia_valid & ATTR_SIZE) {
 		loff_t oldsize = inode->i_size;
 
-		if (ni->ni_flags & NI_FLAG_COMPRESSED_MASK) {
-#ifdef CONFIG_NTFS3_LZX_XPRESS
-			err = ni_decompress_file(ni);
-			if (err)
-				goto out;
-#else
-			ntfs_inode_warn(
-				inode,
-				"activate CONFIG_NTFS3_LZX_XPRESS to truncate external compressed files");
+		if (WARN_ON(ni->ni_flags & NI_FLAG_COMPRESSED_MASK)) {
+			/* should never be here, see ntfs_file_open*/
 			err = -EOPNOTSUPP;
 			goto out;
-#endif
 		}
 		inode_dio_wait(inode);
 
-		if (attr->ia_size < oldsize) {
-			if (is_compressed(ni)) {
-				if (ni->i_valid > attr->ia_size)
-					ni->i_valid = attr->ia_size;
-			} else {
-				err = block_truncate_page(inode->i_mapping,
-							  attr->ia_size,
-							  ntfs_get_block);
-				if (err)
-					goto out;
-			}
-			ntfs_truncate_blocks(inode, attr->ia_size);
-		} else if (attr->ia_size > oldsize) {
-			err = ntfs_extend_ex(inode, attr->ia_size, 0, NULL);
-			if (err)
-				goto out;
-		}
+		if (attr->ia_size < oldsize)
+			err = ntfs_truncate(inode, attr->ia_size);
+		else if (attr->ia_size > oldsize)
+			err = ntfs_extend(inode, attr->ia_size, 0, NULL);
+
+		if (err)
+			goto out;
 
 		ni->ni_flags |= NI_FLAG_UPDATE_PARENT;
 	}
@@ -975,24 +1016,13 @@ static ssize_t ntfs_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 	if (ret <= 0)
 		goto out;
 
-	if (ni->ni_flags & NI_FLAG_COMPRESSED_MASK) {
-#ifdef CONFIG_NTFS3_LZX_XPRESS
-		int err = ni_decompress_file(ni);
-
-		if (err) {
-			ret = err;
-			goto out;
-		}
-#else
-		ntfs_inode_warn(
-			inode,
-			"activate CONFIG_NTFS3_LZX_XPRESS to read external compressed files");
+	if (WARN_ON(ni->ni_flags & NI_FLAG_COMPRESSED_MASK)) {
+		/* should never be here, see ntfs_file_open*/
 		ret = -EOPNOTSUPP;
 		goto out;
-#endif
 	}
 
-	ret = ntfs_extend_ex(inode, iocb->ki_pos, ret, file);
+	ret = ntfs_extend(inode, iocb->ki_pos, ret, file);
 	if (ret)
 		goto out;
 
@@ -1020,6 +1050,22 @@ int ntfs_file_open(struct inode *inode, struct file *file)
 		return -EOPNOTSUPP;
 	}
 
+	/* Decompress "external compressed" file if opened for rw */
+	if ((ni->ni_flags & NI_FLAG_COMPRESSED_MASK) &&
+	    (file->f_flags & (O_WRONLY | O_RDWR | O_TRUNC))) {
+#ifdef CONFIG_NTFS3_LZX_XPRESS
+		int err = ni_decompress_file(ni);
+
+		if (err)
+			return err;
+#else
+		ntfs_inode_warn(
+			inode,
+			"activate CONFIG_NTFS3_LZX_XPRESS to write external compressed files");
+		return -EOPNOTSUPP;
+#endif
+	}
+
 	return generic_file_open(inode, file);
 }
 
diff --git a/fs/ntfs3/frecord.c b/fs/ntfs3/frecord.c
index 2d10cb43e..0f6cb42ce 100644
--- a/fs/ntfs3/frecord.c
+++ b/fs/ntfs3/frecord.c
@@ -274,14 +274,17 @@ struct ATTRIB *ni_find_attr(struct ntfs_inode *ni, struct ATTRIB *attr,
  * enumerates attributes in ntfs_inode
  */
 struct ATTRIB *ni_enum_attr_ex(struct ntfs_inode *ni, struct ATTRIB *attr,
-			       struct ATTR_LIST_ENTRY **le)
+			       struct ATTR_LIST_ENTRY **le,
+			       struct mft_inode **mi)
 {
-	struct mft_inode *mi;
+	struct mft_inode *mi2;
 	struct ATTR_LIST_ENTRY *le2;
 
 	/* Do we have an attribute list? */
 	if (!ni->attr_list.size) {
 		*le = NULL;
+		if (mi)
+			*mi = &ni->mi;
 		/* Enum attributes in primary record */
 		return mi_enum_attr(&ni->mi, attr);
 	}
@@ -292,12 +295,14 @@ struct ATTRIB *ni_enum_attr_ex(struct ntfs_inode *ni, struct ATTRIB *attr,
 		return NULL;
 
 	/* Load record that contains the required attribute */
-	if (ni_load_mi(ni, le2, &mi))
+	if (ni_load_mi(ni, le2, &mi2))
 		return NULL;
 
+	if (mi)
+		*mi = mi2;
+
 	/* Find attribute in loaded record */
-	attr = rec_find_attr_le(mi, le2);
-	return attr;
+	return rec_find_attr_le(mi2, le2);
 }
 
 /*
@@ -561,15 +566,10 @@ static int ni_repack(struct ntfs_inode *ni)
 
 	run_init(&run);
 
-	while ((attr = ni_enum_attr_ex(ni, attr, &le))) {
+	while ((attr = ni_enum_attr_ex(ni, attr, &le, &mi))) {
 		if (!attr->non_res)
 			continue;
 
-		if (ni_load_mi(ni, le, &mi)) {
-			err = -EINVAL;
-			break;
-		}
-
 		svcn = le64_to_cpu(attr->nres.svcn);
 		if (svcn != le64_to_cpu(le->vcn)) {
 			err = -EINVAL;
@@ -1541,7 +1541,7 @@ int ni_delete_all(struct ntfs_inode *ni)
 	bool nt3 = is_ntfs3(sbi);
 	struct MFT_REF ref;
 
-	while ((attr = ni_enum_attr_ex(ni, attr, &le))) {
+	while ((attr = ni_enum_attr_ex(ni, attr, &le, NULL))) {
 		if (!nt3 || attr->name_len) {
 			;
 		} else if (attr->type == ATTR_REPARSE) {
@@ -2224,7 +2224,7 @@ int ni_decompress_file(struct ntfs_inode *ni)
 	 */
 	attr = NULL;
 	le = NULL;
-	while ((attr = ni_enum_attr_ex(ni, attr, &le))) {
+	while ((attr = ni_enum_attr_ex(ni, attr, &le, NULL))) {
 		CLST svcn, evcn;
 		u32 asize, roff;
 
@@ -2336,57 +2336,49 @@ static int decompress_lzx_xpress(struct ntfs_sb_info *sbi, const char *cmpr,
 	}
 
 	err = 0;
-	ctx = NULL;
-	spin_lock(&sbi->compress.lock);
 	if (frame_size == 0x8000) {
+		mutex_lock(&sbi->compress.mtx_lzx);
 		/* LZX: frame compressed */
-		if (!sbi->compress.lzx) {
+		ctx = sbi->compress.lzx;
+		if (!ctx) {
 			/* Lazy initialize lzx decompress context */
-			spin_unlock(&sbi->compress.lock);
-			ctx = lzx_allocate_decompressor(0x8000);
-			if (!ctx)
-				return -ENOMEM;
-			if (IS_ERR(ctx)) {
-				/* should never failed */
-				err = PTR_ERR(ctx);
-				goto out;
+			ctx = lzx_allocate_decompressor();
+			if (!ctx) {
+				err = -ENOMEM;
+				goto out1;
 			}
 
-			spin_lock(&sbi->compress.lock);
-			if (!sbi->compress.lzx) {
-				sbi->compress.lzx = ctx;
-				ctx = NULL;
-			}
+			sbi->compress.lzx = ctx;
 		}
 
-		if (lzx_decompress(sbi->compress.lzx, cmpr, cmpr_size, unc,
-				   unc_size)) {
+		if (lzx_decompress(ctx, cmpr, cmpr_size, unc, unc_size)) {
+			/* treat all errors as "invalid argument" */
 			err = -EINVAL;
 		}
+out1:
+		mutex_unlock(&sbi->compress.mtx_lzx);
 	} else {
 		/* XPRESS: frame compressed */
-		if (!sbi->compress.xpress) {
+		mutex_lock(&sbi->compress.mtx_xpress);
+		ctx = sbi->compress.xpress;
+		if (!ctx) {
 			/* Lazy initialize xpress decompress context */
-			spin_unlock(&sbi->compress.lock);
 			ctx = xpress_allocate_decompressor();
-			if (!ctx)
-				return -ENOMEM;
-
-			spin_lock(&sbi->compress.lock);
-			if (!sbi->compress.xpress) {
-				sbi->compress.xpress = ctx;
-				ctx = NULL;
+			if (!ctx) {
+				err = -ENOMEM;
+				goto out2;
 			}
+
+			sbi->compress.xpress = ctx;
 		}
 
-		if (xpress_decompress(sbi->compress.xpress, cmpr, cmpr_size,
-				      unc, unc_size)) {
+		if (xpress_decompress(ctx, cmpr, cmpr_size, unc, unc_size)) {
+			/* treat all errors as "invalid argument" */
 			err = -EINVAL;
 		}
+out2:
+		mutex_unlock(&sbi->compress.mtx_xpress);
 	}
-	spin_unlock(&sbi->compress.lock);
-out:
-	ntfs_free(ctx);
 	return err;
 }
 #endif
@@ -2705,8 +2697,7 @@ int ni_write_frame(struct ntfs_inode *ni, struct page **pages,
 		goto out;
 	}
 
-	if (!is_attr_compressed(attr)) {
-		WARN_ON(1);
+	if (WARN_ON(!is_attr_compressed(attr))) {
 		err = -EINVAL;
 		goto out;
 	}
@@ -2767,10 +2758,9 @@ int ni_write_frame(struct ntfs_inode *ni, struct page **pages,
 		goto out2;
 	}
 
-	spin_lock(&sbi->compress.lock);
+	mutex_lock(&sbi->compress.mtx_lznt);
 	lznt = NULL;
 	if (!sbi->compress.lznt) {
-		spin_unlock(&sbi->compress.lock);
 		/*
 		 * lznt implements two levels of compression:
 		 * 0 - standard compression
@@ -2779,21 +2769,19 @@ int ni_write_frame(struct ntfs_inode *ni, struct page **pages,
 		 */
 		lznt = get_lznt_ctx(0);
 		if (!lznt) {
+			mutex_unlock(&sbi->compress.mtx_lznt);
 			err = -ENOMEM;
 			goto out3;
 		}
 
-		spin_lock(&sbi->compress.lock);
-		if (!sbi->compress.lznt) {
-			sbi->compress.lznt = lznt;
-			lznt = NULL;
-		}
+		sbi->compress.lznt = lznt;
+		lznt = NULL;
 	}
 
 	/* compress: frame_mem -> frame_ondisk */
 	compr_size = compress_lznt(frame_mem, frame_size, frame_ondisk,
 				   frame_size, sbi->compress.lznt);
-	spin_unlock(&sbi->compress.lock);
+	mutex_unlock(&sbi->compress.mtx_lznt);
 	ntfs_free(lznt);
 
 	if (compr_size + sbi->cluster_size > frame_size) {
diff --git a/fs/ntfs3/fsntfs.c b/fs/ntfs3/fsntfs.c
index d7614749d..eb972616c 100644
--- a/fs/ntfs3/fsntfs.c
+++ b/fs/ntfs3/fsntfs.c
@@ -1537,8 +1537,7 @@ int ntfs_bio_pages(struct ntfs_sb_info *sbi, const struct runs_tree *run,
 
 			if (add + off == PAGE_SIZE) {
 				page_idx += 1;
-				if (page_idx >= nr_pages) {
-					WARN_ON(1);
+				if (WARN_ON(page_idx >= nr_pages)) {
 					err = -EINVAL;
 					goto out;
 				}
diff --git a/fs/ntfs3/index.c b/fs/ntfs3/index.c
index b7caeb3da..64855f9c0 100644
--- a/fs/ntfs3/index.c
+++ b/fs/ntfs3/index.c
@@ -238,8 +238,8 @@ static int bmp_buf_get(struct ntfs_index *indx, struct ntfs_inode *ni,
 	}
 
 	data_size = le64_to_cpu(b->nres.data_size);
-	if (off >= data_size) {
-		WARN_ON(1);
+	if (WARN_ON(off >= data_size)) {
+		/* looks like filesystem error */
 		return -EINVAL;
 	}
 
diff --git a/fs/ntfs3/inode.c b/fs/ntfs3/inode.c
index 73fdd9cd9..2b64fe8c1 100644
--- a/fs/ntfs3/inode.c
+++ b/fs/ntfs3/inode.c
@@ -108,12 +108,12 @@ static struct inode *ntfs_read_mft(struct inode *inode,
 
 	/*
 	 * to reduce tab pressure use goto instead of
-	 * while( (attr = ni_enum_attr_ex(ni, attr, &le) ))
+	 * while( (attr = ni_enum_attr_ex(ni, attr, &le, NULL) ))
 	 */
 next_attr:
 	run = NULL;
 	err = -EINVAL;
-	attr = ni_enum_attr_ex(ni, attr, &le);
+	attr = ni_enum_attr_ex(ni, attr, &le, NULL);
 	if (!attr)
 		goto end_enum;
 
@@ -1585,11 +1585,14 @@ int ntfs_create_inode(struct inode *dir, struct dentry *dentry,
 
 	inode->i_mode = mode;
 
+#ifdef CONFIG_NTFS3_FS_POSIX_ACL
 	if (!is_link && (sb->s_flags & SB_POSIXACL)) {
 		err = ntfs_init_acl(inode, dir);
 		if (err)
 			goto out6;
-	} else {
+	} else
+#endif
+	{
 		inode->i_flags |= S_NOSEC;
 	}
 
@@ -1963,8 +1966,10 @@ static noinline int ntfs_readlink_hlp(struct inode *inode, char *buffer,
 		goto out;
 
 	default:
-		if (IsReparseTagMicrosoft(rp->ReparseTag))
+		if (IsReparseTagMicrosoft(rp->ReparseTag)) {
+			/* unknown Microsoft Tag */
 			goto out;
+		}
 		if (!IsReparseTagNameSurrogate(rp->ReparseTag) ||
 		    i_size <= sizeof(struct REPARSE_POINT)) {
 			goto out;
diff --git a/fs/ntfs3/lib/common_defs.h b/fs/ntfs3/lib/common_defs.h
deleted file mode 100644
index b1495874a..000000000
--- a/fs/ntfs3/lib/common_defs.h
+++ /dev/null
@@ -1,196 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0-or-later */
-/*
- * Copyright (C) 2012-2016 Eric Biggers
- *
- * Adapted for linux kernel by Alexander Mamaev:
- * - remove implementations of get_unaligned_
- * - remove SSE and AVX instructions
- * - assume GCC is always defined
- * - inlined aligned_malloc/aligned_free
- * - ISO C90
- * - linux kernel code style
- */
-
-#ifndef _COMMON_DEFS_H
-#define _COMMON_DEFS_H
-
-#include <linux/string.h>
-#include <linux/compiler.h>
-#include <linux/types.h>
-#include <linux/slab.h>
-#include <asm/unaligned.h>
-
-
-/* ========================================================================== */
-/*				Type definitions			      */
-/* ========================================================================== */
-
-/*
- * Type of a machine word.  'u32 long' would be logical, but that is only
- * 32 bits on x86_64 Windows.  The same applies to 'uint_fast32_t'.  So the best
- * we can do without a bunch of #ifdefs appears to be 'size_t'.
- */
-
-#define WORDBYTES	sizeof(size_t)
-#define WORDBITS	(8 * WORDBYTES)
-
-/* ========================================================================== */
-/*			   Compiler-specific definitions		      */
-/* ========================================================================== */
-
-#  define forceinline		__always_inline
-#  define _aligned_attribute(n) __aligned(n)
-#  define bsr32(n)		(31 - __builtin_clz(n))
-#  define bsr64(n)		(63 - __builtin_clzll(n))
-#  define bsf32(n)		__builtin_ctz(n)
-#  define bsf64(n)		__builtin_ctzll(n)
-
-/* STATIC_ASSERT() - verify the truth of an expression at compilation time */
-#define STATIC_ASSERT(expr)	((void)sizeof(char[1 - 2 * !(expr)]))
-
-/* STATIC_ASSERT_ZERO() - verify the truth of an expression at compilation time
- * and also produce a result of value '0' to be used in constant expressions
- */
-#define STATIC_ASSERT_ZERO(expr) ((int)sizeof(char[-!(expr)]))
-
-/* UNALIGNED_ACCESS_IS_FAST should be defined to 1 if unaligned memory accesses
- * can be performed efficiently on the target platform.
- */
-#if defined(__x86_64__) || defined(__i386__) || defined(__ARM_FEATURE_UNALIGNED)
-#  define UNALIGNED_ACCESS_IS_FAST 1
-#else
-#  define UNALIGNED_ACCESS_IS_FAST 0
-#endif
-
-/* ========================================================================== */
-/*			    Unaligned memory accesses			      */
-/* ========================================================================== */
-
-#define load_word_unaligned(p) get_unaligned((const size_t *)(p))
-#define store_word_unaligned(v, p) put_unaligned((v), (size_t *)(p))
-
-
-/* ========================================================================== */
-/*			       Bit scan functions			      */
-/* ========================================================================== */
-
-/*
- * Bit Scan Reverse (BSR) - find the 0-based index (relative to the least
- * significant end) of the *most* significant 1 bit in the input value.  The
- * input value must be nonzero!
- */
-
-#ifndef bsr32
-static forceinline u32
-bsr32(u32 v)
-{
-	u32 bit = 0;
-
-	while ((v >>= 1) != 0)
-		bit++;
-	return bit;
-}
-#endif
-
-#ifndef bsr64
-static forceinline u32
-bsr64(u64 v)
-{
-	u32 bit = 0;
-
-	while ((v >>= 1) != 0)
-		bit++;
-	return bit;
-}
-#endif
-
-static forceinline u32
-bsrw(size_t v)
-{
-	STATIC_ASSERT(WORDBITS == 32 || WORDBITS == 64);
-	if (WORDBITS == 32)
-		return bsr32(v);
-	else
-		return bsr64(v);
-}
-
-/*
- * Bit Scan Forward (BSF) - find the 0-based index (relative to the least
- * significant end) of the *least* significant 1 bit in the input value.  The
- * input value must be nonzero!
- */
-
-#ifndef bsf32
-static forceinline u32
-bsf32(u32 v)
-{
-	u32 bit;
-
-	for (bit = 0; !(v & 1); bit++, v >>= 1)
-		;
-	return bit;
-}
-#endif
-
-#ifndef bsf64
-static forceinline u32
-bsf64(u64 v)
-{
-	u32 bit;
-
-	for (bit = 0; !(v & 1); bit++, v >>= 1)
-		;
-	return bit;
-}
-#endif
-
-static forceinline u32
-bsfw(size_t v)
-{
-	STATIC_ASSERT(WORDBITS == 32 || WORDBITS == 64);
-	if (WORDBITS == 32)
-		return bsf32(v);
-	else
-		return bsf64(v);
-}
-
-/* Return the log base 2 of 'n', rounded up to the nearest integer. */
-static forceinline u32
-ilog2_ceil(size_t n)
-{
-	if (n <= 1)
-		return 0;
-	return 1 + bsrw(n - 1);
-}
-
-/* ========================================================================== */
-/*			    Aligned memory allocation			      */
-/* ========================================================================== */
-
-static forceinline void *
-aligned_malloc(size_t size, size_t alignment)
-{
-	const uintptr_t mask = alignment - 1;
-	char *ptr = NULL;
-	char *raw_ptr;
-
-	raw_ptr = kmalloc(mask + sizeof(size_t) + size, GFP_NOFS);
-	if (raw_ptr) {
-		ptr = (char *)raw_ptr + sizeof(size_t);
-		ptr = (void *)(((uintptr_t)ptr + mask) & ~mask);
-		*((size_t *)ptr - 1) = ptr - raw_ptr;
-	}
-	return ptr;
-}
-
-static forceinline void
-aligned_free(void *ptr)
-{
-	if (ptr)
-		kfree((char *)ptr - *((size_t *)ptr - 1));
-}
-
-extern void *aligned_malloc(size_t size, size_t alignment);
-extern void aligned_free(void *ptr);
-
-#endif /* _COMMON_DEFS_H */
diff --git a/fs/ntfs3/lib/decompress_common.c b/fs/ntfs3/lib/decompress_common.c
index c8512818b..83c9e93ae 100644
--- a/fs/ntfs3/lib/decompress_common.c
+++ b/fs/ntfs3/lib/decompress_common.c
@@ -1,24 +1,21 @@
 // SPDX-License-Identifier: GPL-2.0-or-later
 /*
- * decompress_common.c
+ * decompress_common.c - Code shared by the XPRESS and LZX decompressors
  *
- * Code for decompression shared among multiple compression formats.
+ * Copyright (C) 2015 Eric Biggers
  *
- * The following copying information applies to this specific source code file:
+ * This program is free software: you can redistribute it and/or modify it under
+ * the terms of the GNU General Public License as published by the Free Software
+ * Foundation, either version 2 of the License, or (at your option) any later
+ * version.
  *
- * Written in 2012-2016 by Eric Biggers <ebiggers3@gmail.com>
- *
- * To the extent possible under law, the author(s) have dedicated all copyright
- * and related and neighboring rights to this software to the public domain
- * worldwide via the Creative Commons Zero 1.0 Universal Public Domain
- * Dedication (the "CC0").
- *
- * This software is distributed in the hope that it will be useful, but WITHOUT
+ * This program is distributed in the hope that it will be useful, but WITHOUT
  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
- * FOR A PARTICULAR PURPOSE. See the CC0 for more details.
+ * FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ * details.
  *
- * You should have received a copy of the CC0 along with this software; if not
- * see <http://creativecommons.org/publicdomain/zero/1.0/>.
+ * You should have received a copy of the GNU General Public License along with
+ * this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include "decompress_common.h"
@@ -26,289 +23,310 @@
 /*
  * make_huffman_decode_table() -
  *
- * Given an alphabet of symbols and the length of each symbol's codeword in a
- * canonical prefix code, build a table for quickly decoding symbols that were
- * encoded with that code.
+ * Build a decoding table for a canonical prefix code, or "Huffman code".
+ *
+ * This is an internal function, not part of the library API!
  *
- * A _prefix code_ is an assignment of bitstrings called _codewords_ to symbols
- * such that no whole codeword is a prefix of any other.  A prefix code might be
- * a _Huffman code_, which means that it is an optimum prefix code for a given
- * list of symbol frequencies and was generated by the Huffman algorithm.
- * Although the prefix codes processed here will ordinarily be "Huffman codes",
- * strictly speaking the decoder cannot know whether a given code was actually
- * generated by the Huffman algorithm or not.
+ * This takes as input the length of the codeword for each symbol in the
+ * alphabet and produces as output a table that can be used for fast
+ * decoding of prefix-encoded symbols using read_huffsym().
  *
- * A prefix code is _canonical_ if and only if a longer codeword never
- * lexicographically precedes a shorter codeword, and the lexicographic ordering
- * of codewords of equal length is the same as the lexicographic ordering of the
- * corresponding symbols.  The advantage of using a canonical prefix code is
- * that the codewords can be reconstructed from only the symbol => codeword
- * length mapping.  This eliminates the need to transmit the codewords
- * explicitly.  Instead, they can be enumerated in lexicographic order after
- * sorting the symbols primarily by increasing codeword length and secondarily
- * by increasing symbol value.
+ * Strictly speaking, a canonical prefix code might not be a Huffman
+ * code.  But this algorithm will work either way; and in fact, since
+ * Huffman codes are defined in terms of symbol frequencies, there is no
+ * way for the decompressor to know whether the code is a true Huffman
+ * code or not until all symbols have been decoded.
  *
- * However, the decoder's real goal is to decode symbols with the code, not just
- * generate the list of codewords.  Consequently, this function directly builds
- * a table for efficiently decoding symbols using the code.  The basic idea is
- * that given the next 'max_codeword_len' bits of input, the decoder can look up
- * the next decoded symbol by indexing a table containing '2^max_codeword_len'
- * entries.  A codeword with length 'max_codeword_len' will have exactly one
- * entry in this table, whereas a codeword shorter than 'max_codeword_len' will
- * have multiple entries in this table.  Precisely, a codeword of length 'n'
- * will have '2^(max_codeword_len - n)' entries.  The index of each such entry,
- * considered as a bitstring of length 'max_codeword_len', will contain the
- * corresponding codeword as a prefix.
+ * Because the prefix code is assumed to be "canonical", it can be
+ * reconstructed directly from the codeword lengths.  A prefix code is
+ * canonical if and only if a longer codeword never lexicographically
+ * precedes a shorter codeword, and the lexicographic ordering of
+ * codewords of the same length is the same as the lexicographic ordering
+ * of the corresponding symbols.  Consequently, we can sort the symbols
+ * primarily by codeword length and secondarily by symbol value, then
+ * reconstruct the prefix code by generating codewords lexicographically
+ * in that order.
  *
- * That's the basic idea, but we extend it in two ways:
+ * This function does not, however, generate the prefix code explicitly.
+ * Instead, it directly builds a table for decoding symbols using the
+ * code.  The basic idea is this: given the next 'max_codeword_len' bits
+ * in the input, we can look up the decoded symbol by indexing a table
+ * containing 2**max_codeword_len entries.  A codeword with length
+ * 'max_codeword_len' will have exactly one entry in this table, whereas
+ * a codeword shorter than 'max_codeword_len' will have multiple entries
+ * in this table.  Precisely, a codeword of length n will be represented
+ * by 2**(max_codeword_len - n) entries in this table.  The 0-based index
+ * of each such entry will contain the corresponding codeword as a prefix
+ * when zero-padded on the left to 'max_codeword_len' binary digits.
  *
- * - Often the maximum codeword length is too long for it to be efficient to
- *   build the full decode table whenever a new code is used.  Instead, we build
- *   a "root" table using only '2^table_bits' entries, where 'table_bits <=
- *   max_codeword_len'.  Then, a lookup of 'table_bits' bits produces either a
- *   symbol directly (for codewords not longer than 'table_bits'), or the index
- *   of a subtable which must be indexed with additional bits of input to fully
- *   decode the symbol (for codewords longer than 'table_bits').
+ * That's the basic idea, but we implement two optimizations regarding
+ * the format of the decode table itself:
  *
- * - Whenever the decoder decodes a symbol, it needs to know the codeword length
- *   so that it can remove the appropriate number of input bits.  The obvious
- *   solution would be to simply retain the codeword lengths array and use the
- *   decoded symbol as an index into it.  However, that would require two array
- *   accesses when decoding each symbol.  Our strategy is to instead store the
- *   codeword length directly in the decode table entry along with the symbol.
+ * - For many compression formats, the maximum codeword length is too
+ *   long for it to be efficient to build the full decoding table
+ *   whenever a new prefix code is used.  Instead, we can build the table
+ *   using only 2**table_bits entries, where 'table_bits' is some number
+ *   less than or equal to 'max_codeword_len'.  Then, only codewords of
+ *   length 'table_bits' and shorter can be directly looked up.  For
+ *   longer codewords, the direct lookup instead produces the root of a
+ *   binary tree.  Using this tree, the decoder can do traditional
+ *   bit-by-bit decoding of the remainder of the codeword.  Child nodes
+ *   are allocated in extra entries at the end of the table; leaf nodes
+ *   contain symbols.  Note that the long-codeword case is, in general,
+ *   not performance critical, since in Huffman codes the most frequently
+ *   used symbols are assigned the shortest codeword lengths.
  *
- * See MAKE_DECODE_TABLE_ENTRY() for full details on the format of decode table
- * entries, and see read_huffsym() for full details on how symbols are decoded.
+ * - When we decode a symbol using a direct lookup of the table, we still
+ *   need to know its length so that the bitstream can be advanced by the
+ *   appropriate number of bits.  The simple solution is to simply retain
+ *   the 'lens' array and use the decoded symbol as an index into it.
+ *   However, this requires two separate array accesses in the fast path.
+ *   The optimization is to store the length directly in the decode
+ *   table.  We use the bottom 11 bits for the symbol and the top 5 bits
+ *   for the length.  In addition, to combine this optimization with the
+ *   previous one, we introduce a special case where the top 2 bits of
+ *   the length are both set if the entry is actually the root of a
+ *   binary tree.
  *
  * @decode_table:
- *	The array in which to build the decode table.  This must have been
- *	declared by the DECODE_TABLE() macro.  This may alias @lens, since all
- *	@lens are consumed before the decode table is written to.
+ *	The array in which to create the decoding table.  This must have
+ *	a length of at least ((2**table_bits) + 2 * num_syms) entries.
  *
  * @num_syms:
- *	The number of symbols in the alphabet.
+ *	The number of symbols in the alphabet; also, the length of the
+ *	'lens' array.  Must be less than or equal to 2048.
  *
  * @table_bits:
- *	The log base 2 of the number of entries in the root table.
+ *	The order of the decode table size, as explained above.  Must be
+ *	less than or equal to 13.
  *
  * @lens:
- *	An array of length @num_syms, indexed by symbol, that gives the length
- *	of the codeword, in bits, for each symbol.  The length can be 0, which
- *	means that the symbol does not have a codeword assigned.  In addition,
- *	@lens may alias @decode_table, as noted above.
+ *	An array of length @num_syms, indexable by symbol, that gives the
+ *	length of the codeword, in bits, for that symbol.  The length can
+ *	be 0, which means that the symbol does not have a codeword
+ *	assigned.
  *
  * @max_codeword_len:
- *	The maximum codeword length permitted for this code.  All entries in
- *	'lens' must be less than or equal to this value.
+ *	The longest codeword length allowed in the compression format.
+ *	All entries in 'lens' must be less than or equal to this value.
+ *	This must be less than or equal to 23.
  *
  * @working_space
- *	A temporary array that was declared with DECODE_TABLE_WORKING_SPACE().
+ *	A temporary array of length '2 * (max_codeword_len + 1) +
+ *	num_syms'.
  *
- * Returns 0 on success, or -1 if the lengths do not form a valid prefix code.
+ * Returns 0 on success, or -1 if the lengths do not form a valid prefix
+ * code.
  */
-int
-make_huffman_decode_table(u16 decode_table[], u32 num_syms,
-			  u32 table_bits, const u8 lens[],
-			  u32 max_codeword_len, u16 working_space[])
+int make_huffman_decode_table(u16 decode_table[], const u32 num_syms,
+			      const u32 table_bits, const u8 lens[],
+			      const u32 max_codeword_len,
+			      u16 working_space[])
 {
+	const u32 table_num_entries = 1 << table_bits;
 	u16 * const len_counts = &working_space[0];
 	u16 * const offsets = &working_space[1 * (max_codeword_len + 1)];
 	u16 * const sorted_syms = &working_space[2 * (max_codeword_len + 1)];
-	s32 remainder = 1;
-	void *entry_ptr = decode_table;
-	u32 codeword_len = 1;
+	int left;
+	void *decode_table_ptr;
 	u32 sym_idx;
-	u32 codeword;
-	u32 subtable_pos;
-	u32 subtable_bits;
-	u32 subtable_prefix;
+	u32 codeword_len;
+	u32 stores_per_loop;
+	u32 decode_table_pos;
 	u32 len;
 	u32 sym;
-	u32 stores_per_loop;
 
-	/* Count how many codewords have each length, including 0.  */
+	/* Count how many symbols have each possible codeword length.
+	 * Note that a length of 0 indicates the corresponding symbol is not
+	 * used in the code and therefore does not have a codeword.
+	 */
 	for (len = 0; len <= max_codeword_len; len++)
 		len_counts[len] = 0;
 	for (sym = 0; sym < num_syms; sym++)
 		len_counts[lens[sym]]++;
 
-	/* It is already guaranteed that all lengths are <= max_codeword_len,
-	 * but it cannot be assumed they form a complete prefix code.  A
-	 * codeword of length n should require a proportion of the codespace
-	 * equaling (1/2)^n.  The code is complete if and only if, by this
-	 * measure, the codespace is exactly filled by the lengths.
+	/* We can assume all lengths are <= max_codeword_len, but we
+	 * cannot assume they form a valid prefix code.  A codeword of
+	 * length n should require a proportion of the codespace equaling
+	 * (1/2)^n.  The code is valid if and only if the codespace is
+	 * exactly filled by the lengths, by this measure.
 	 */
+	left = 1;
 	for (len = 1; len <= max_codeword_len; len++) {
-		remainder = (remainder << 1) - len_counts[len];
-		/* Do the lengths overflow the codespace? */
-		if (unlikely(remainder < 0))
+		left <<= 1;
+		left -= len_counts[len];
+		if (left < 0) {
+			/* The lengths overflow the codespace; that is, the code
+			 * is over-subscribed.
+			 */
 			return -1;
+		}
 	}
 
-	if (remainder != 0) {
+	if (left) {
 		/* The lengths do not fill the codespace; that is, they form an
-		 * incomplete code.  This is permitted only if the code is empty
-		 * (contains no symbols).
+		 * incomplete set.
 		 */
-
-		if (unlikely(remainder != 1U << max_codeword_len))
-			return -1;
-
-		/* The code is empty.  When processing a well-formed stream, the
-		 * decode table need not be initialized in this case.  However,
-		 * we cannot assume the stream is well-formed, so we must
-		 * initialize the decode table anyway.  Setting all entries to 0
-		 * makes the decode table always produce symbol '0' without
-		 * consuming any bits, which is good enough.
-		 */
-		memset(decode_table, 0, sizeof(decode_table[0]) << table_bits);
-		return 0;
+		if (left == (1 << max_codeword_len)) {
+			/* The code is completely empty.  This is arguably
+			 * invalid, but in fact it is valid in LZX and XPRESS,
+			 * so we must allow it.  By definition, no symbols can
+			 * be decoded with an empty code.  Consequently, we
+			 * technically don't even need to fill in the decode
+			 * table.  However, to avoid accessing uninitialized
+			 * memory if the algorithm nevertheless attempts to
+			 * decode symbols using such a code, we zero out the
+			 * decode table.
+			 */
+			memset(decode_table, 0,
+			       table_num_entries * sizeof(decode_table[0]));
+			return 0;
+		}
+		return -1;
 	}
 
-	/* Sort the symbols primarily by increasing codeword length and
-	 * secondarily by increasing symbol value.
+	/* Sort the symbols primarily by length and secondarily by symbol order.
 	 */
 
-	/* Initialize 'offsets' so that 'offsets[len]' is the number of
-	 * codewords shorter than 'len' bits, including length 0.
+	/* Initialize 'offsets' so that offsets[len] for 1 <= len <=
+	 * max_codeword_len is the number of codewords shorter than 'len' bits.
 	 */
-	offsets[0] = 0;
-	for (len = 0; len < max_codeword_len; len++)
+	offsets[1] = 0;
+	for (len = 1; len < max_codeword_len; len++)
 		offsets[len + 1] = offsets[len] + len_counts[len];
 
-	/* Use the 'offsets' array to sort the symbols. */
+	/* Use the 'offsets' array to sort the symbols.  Note that we do not
+	 * include symbols that are not used in the code.  Consequently, fewer
+	 * than 'num_syms' entries in 'sorted_syms' may be filled.
+	 */
 	for (sym = 0; sym < num_syms; sym++)
-		sorted_syms[offsets[lens[sym]]++] = sym;
+		if (lens[sym])
+			sorted_syms[offsets[lens[sym]]++] = sym;
 
-	/*
-	 * Fill the root table entries for codewords no longer than table_bits.
+	/* Fill entries for codewords with length <= table_bits
+	 * --- that is, those short enough for a direct mapping.
 	 *
 	 * The table will start with entries for the shortest codeword(s), which
-	 * will have the most entries.  From there, the number of entries per
-	 * codeword will decrease.  As an optimization, we may begin filling
-	 * entries with SSE2 vector accesses (8 entries/store), then change to
-	 * word accesses (2 or 4 entries/store), then change to 16-bit accesses
-	 * (1 entry/store).
+	 * have the most entries.  From there, the number of entries per
+	 * codeword will decrease.
 	 */
-	sym_idx = offsets[0];
-
-	/* Fill entries one word (2 or 4 entries) at a time. */
-	for (stores_per_loop = (1U << (table_bits - codeword_len)) /
-					(WORDBYTES / sizeof(decode_table[0]));
-	     stores_per_loop != 0; codeword_len++, stores_per_loop >>= 1){
+	decode_table_ptr = decode_table;
+	sym_idx = 0;
+	codeword_len = 1;
+	stores_per_loop = (1 << (table_bits - codeword_len));
+	for (; stores_per_loop != 0; codeword_len++, stores_per_loop >>= 1) {
 		u32 end_sym_idx = sym_idx + len_counts[codeword_len];
 
 		for (; sym_idx < end_sym_idx; sym_idx++) {
-			/* Accessing the array of u16 as u32 or u64 would
-			 * violate strict aliasing and would require compiling
-			 * the code with -fno-strict-aliasing to guarantee
-			 * correctness.  To work around this problem, use the
-			 * gcc 'may_alias' extension.
-			 */
-			size_t v = repeat_u16(
-				MAKE_DECODE_TABLE_ENTRY(sorted_syms[sym_idx],
-							codeword_len));
-			u32 n = stores_per_loop;
-
-			do {
-				*(size_t __attribute__((may_alias)) *)entry_ptr = v;
-				entry_ptr += sizeof(v);
-			} while (--n);
-		}
-	}
-
-	/* Fill entries one at a time. */
-	for (stores_per_loop = (1U << (table_bits - codeword_len));
-	     stores_per_loop != 0; codeword_len++, stores_per_loop >>= 1){
-		u32 end_sym_idx = sym_idx + len_counts[codeword_len];
+			u16 entry;
+			u16 *p;
+			u32 n;
 
-		for (; sym_idx < end_sym_idx; sym_idx++) {
-			u16 v = MAKE_DECODE_TABLE_ENTRY(sorted_syms[sym_idx],
-							codeword_len);
-			u32 n = stores_per_loop;
+			entry = ((u32)codeword_len << 11) | sorted_syms[sym_idx];
+			p = (u16 *)decode_table_ptr;
+			n = stores_per_loop;
 
 			do {
-				*(u16 *)entry_ptr = v;
-				entry_ptr += sizeof(v);
+				*p++ = entry;
 			} while (--n);
-		}
-	}
 
-	/* If all symbols were processed, then no subtables are required. */
-	if (sym_idx == num_syms)
-		return 0;
-
-	/* At least one subtable is required.  Process the remaining symbols. */
-	codeword = ((u16 *)entry_ptr - decode_table) << 1;
-	subtable_pos = 1U << table_bits;
-	subtable_bits = table_bits;
-	subtable_prefix = -1;
-	do {
-		u32 prefix;
-		u16 entry;
-		u32 n;
-
-		while (len_counts[codeword_len] == 0) {
-			codeword_len++;
-			codeword <<= 1;
+			decode_table_ptr = p;
 		}
+	}
 
-		prefix = codeword >> (codeword_len - table_bits);
+	/* If we've filled in the entire table, we are done.  Otherwise,
+	 * there are codewords longer than table_bits for which we must
+	 * generate binary trees.
+	 */
+	decode_table_pos = (u16 *)decode_table_ptr - decode_table;
+	if (decode_table_pos != table_num_entries) {
+		u32 j;
+		u32 next_free_tree_slot;
+		u32 cur_codeword;
 
-		/* Start a new subtable if the first 'table_bits' bits of the
-		 * codeword don't match the prefix for the previous subtable, or
-		 * if this will be the first subtable.
+		/* First, zero out the remaining entries.  This is
+		 * necessary so that these entries appear as
+		 * "unallocated" in the next part.  Each of these entries
+		 * will eventually be filled with the representation of
+		 * the root node of a binary tree.
 		 */
-		if (prefix != subtable_prefix) {
+		j = decode_table_pos;
+		do {
+			decode_table[j] = 0;
+		} while (++j != table_num_entries);
 
-			subtable_prefix = prefix;
+		/* We allocate child nodes starting at the end of the
+		 * direct lookup table.  Note that there should be
+		 * 2*num_syms extra entries for this purpose, although
+		 * fewer than this may actually be needed.
+		 */
+		next_free_tree_slot = table_num_entries;
 
-			/*
-			 * Calculate the subtable length.  If the codeword
-			 * length exceeds 'table_bits' by n, then the subtable
-			 * needs at least 2^n entries.  But it may need more; if
-			 * there are fewer than 2^n codewords of length
-			 * 'table_bits + n' remaining, then n will need to be
-			 * incremented to bring in longer codewords until the
-			 * subtable can be filled completely.  Note that it
-			 * always will, eventually, be possible to fill the
-			 * subtable, since it was previously verified that the
-			 * code is complete.
-			 */
-			subtable_bits = codeword_len - table_bits;
-			remainder = (s32)1 << subtable_bits;
-			for (;;) {
-				remainder -= len_counts[table_bits +
-							subtable_bits];
-				if (remainder <= 0)
-					break;
-				subtable_bits++;
-				remainder <<= 1;
-			}
+		/* Iterate through each codeword with length greater than
+		 * 'table_bits', primarily in order of codeword length
+		 * and secondarily in order of symbol.
+		 */
+		for (cur_codeword = decode_table_pos << 1;
+		     codeword_len <= max_codeword_len;
+		     codeword_len++, cur_codeword <<= 1) {
+			u32 end_sym_idx = sym_idx + len_counts[codeword_len];
 
-			/* Create the entry that points from the root table to
-			 * the subtable.  This entry contains the index of the
-			 * start of the subtable and the number of bits with
-			 * which the subtable is indexed (the log base 2 of the
-			 * number of entries it contains).
-			 */
-			decode_table[subtable_prefix] =
-				MAKE_DECODE_TABLE_ENTRY(subtable_pos,
-							subtable_bits);
-		}
+			for (; sym_idx < end_sym_idx; sym_idx++, cur_codeword++) {
+				/* 'sorted_sym' is the symbol represented by the
+				 * codeword.
+				 */
+				u32 sorted_sym = sorted_syms[sym_idx];
+				u32 extra_bits = codeword_len - table_bits;
+				u32 node_idx = cur_codeword >> extra_bits;
 
-		/* Fill the subtable entries for this symbol. */
-		entry = MAKE_DECODE_TABLE_ENTRY(sorted_syms[sym_idx],
-						    codeword_len - table_bits);
-		n = 1U << (subtable_bits - (codeword_len -
-						     table_bits));
-		do {
-			decode_table[subtable_pos++] = entry;
-		} while (--n);
+				/* Go through each bit of the current codeword
+				 * beyond the prefix of length @table_bits and
+				 * walk the appropriate binary tree, allocating
+				 * any slots that have not yet been allocated.
+				 *
+				 * Note that the 'pointer' entry to the binary
+				 * tree, which is stored in the direct lookup
+				 * portion of the table, is represented
+				 * identically to other internal (non-leaf)
+				 * nodes of the binary tree; it can be thought
+				 * of as simply the root of the tree.  The
+				 * representation of these internal nodes is
+				 * simply the index of the left child combined
+				 * with the special bits 0xC000 to distingush
+				 * the entry from direct mapping and leaf node
+				 * entries.
+				 */
+				do {
+					/* At least one bit remains in the
+					 * codeword, but the current node is an
+					 * unallocated leaf.  Change it to an
+					 * internal node.
+					 */
+					if (decode_table[node_idx] == 0) {
+						decode_table[node_idx] =
+							next_free_tree_slot | 0xC000;
+						decode_table[next_free_tree_slot++] = 0;
+						decode_table[next_free_tree_slot++] = 0;
+					}
 
-		len_counts[codeword_len]--;
-		codeword++;
-	} while (++sym_idx < num_syms);
+					/* Go to the left child if the next bit
+					 * in the codeword is 0; otherwise go to
+					 * the right child.
+					 */
+					node_idx = decode_table[node_idx] & 0x3FFF;
+					--extra_bits;
+					node_idx += (cur_codeword >> extra_bits) & 1;
+				} while (extra_bits != 0);
 
+				/* We've traversed the tree using the entire
+				 * codeword, and we're now at the entry where
+				 * the actual symbol will be stored.  This is
+				 * distinguished from internal nodes by not
+				 * having its high two bits set.
+				 */
+				decode_table[node_idx] = sorted_sym;
+			}
+		}
+	}
 	return 0;
 }
diff --git a/fs/ntfs3/lib/decompress_common.h b/fs/ntfs3/lib/decompress_common.h
index c839fdbb2..66297f398 100644
--- a/fs/ntfs3/lib/decompress_common.h
+++ b/fs/ntfs3/lib/decompress_common.h
@@ -1,35 +1,62 @@
 /* SPDX-License-Identifier: GPL-2.0-or-later */
 
 /*
- * decompress_common.h
+ * decompress_common.h - Code shared by the XPRESS and LZX decompressors
  *
- * Header for decompression code shared by multiple compression formats.
+ * Copyright (C) 2015 Eric Biggers
  *
- * The following copying information applies to this specific source code file:
+ * This program is free software: you can redistribute it and/or modify it under
+ * the terms of the GNU General Public License as published by the Free Software
+ * Foundation, either version 2 of the License, or (at your option) any later
+ * version.
  *
- * Written in 2012-2016 by Eric Biggers <ebiggers3@gmail.com>
- *
- * To the extent possible under law, the author(s) have dedicated all copyright
- * and related and neighboring rights to this software to the public domain
- * worldwide via the Creative Commons Zero 1.0 Universal Public Domain
- * Dedication (the "CC0").
- *
- * This software is distributed in the hope that it will be useful, but WITHOUT
+ * This program is distributed in the hope that it will be useful, but WITHOUT
  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
- * FOR A PARTICULAR PURPOSE. See the CC0 for more details.
+ * FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ * details.
  *
- * You should have received a copy of the CC0 along with this software; if not
- * see <http://creativecommons.org/publicdomain/zero/1.0/>.
+ * You should have received a copy of the GNU General Public License along with
+ * this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
-#ifndef _DECOMPRESS_COMMON_H
-#define _DECOMPRESS_COMMON_H
+#include <linux/string.h>
+#include <linux/compiler.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <asm/unaligned.h>
+
+
+/* "Force inline" macro (not required, but helpful for performance)  */
+#define forceinline __always_inline
 
-#include "common_defs.h"
+/* Enable whole-word match copying on selected architectures  */
+#if defined(__i386__) || defined(__x86_64__) || defined(__ARM_FEATURE_UNALIGNED)
+#  define FAST_UNALIGNED_ACCESS
+#endif
 
-/******************************************************************************/
-/*                   Input bitstream for XPRESS and LZX                       */
-/*----------------------------------------------------------------------------*/
+/* Size of a machine word  */
+#define WORDBYTES (sizeof(size_t))
+
+static forceinline void
+copy_unaligned_word(const void *src, void *dst)
+{
+	put_unaligned(get_unaligned((const size_t *)src), (size_t *)dst);
+}
+
+
+/* Generate a "word" with platform-dependent size whose bytes all contain the
+ * value 'b'.
+ */
+static forceinline size_t repeat_byte(u8 b)
+{
+	size_t v;
+
+	v = b;
+	v |= v << 8;
+	v |= v << 16;
+	v |= v << ((WORDBYTES == 8) ? 32 : 0);
+	return v;
+}
 
 /* Structure that encapsulates a block of in-memory data being interpreted as a
  * stream of bits, optionally with interwoven literal bytes.  Bits are assumed
@@ -49,13 +76,13 @@ struct input_bitstream {
 	/* Pointer to the next byte to be retrieved from the input buffer.  */
 	const u8 *next;
 
-	/* Pointer past the end of the input buffer.  */
+	/* Pointer to just past the end of the input buffer.  */
 	const u8 *end;
 };
 
 /* Initialize a bitstream to read from the specified input buffer.  */
-static forceinline void
-init_input_bitstream(struct input_bitstream *is, const void *buffer, u32 size)
+static forceinline void init_input_bitstream(struct input_bitstream *is,
+					     const void *buffer, u32 size)
 {
 	is->bitbuf = 0;
 	is->bitsleft = 0;
@@ -63,46 +90,22 @@ init_input_bitstream(struct input_bitstream *is, const void *buffer, u32 size)
 	is->end = is->next + size;
 }
 
-/* Note: for performance reasons, the following methods don't return error codes
- * to the caller if the input buffer is overrun.  Instead, they just assume that
- * all overrun data is zeroes.  This has no effect on well-formed compressed
- * data.  The only disadvantage is that bad compressed data may go undetected,
- * but even this is irrelevant if higher level code checksums the uncompressed
- * data anyway.
- */
-
 /* Ensure the bit buffer variable for the bitstream contains at least @num_bits
  * bits.  Following this, bitstream_peek_bits() and/or bitstream_remove_bits()
- * may be called on the bitstream to peek or remove up to @num_bits bits.
+ * may be called on the bitstream to peek or remove up to @num_bits bits.  Note
+ * that @num_bits must be <= 16.
  */
-static forceinline void
-bitstream_ensure_bits(struct input_bitstream *is, const u32 num_bits)
+static forceinline void bitstream_ensure_bits(struct input_bitstream *is,
+					      u32 num_bits)
 {
-	/* This currently works for at most 17 bits.  */
-
-	if (is->bitsleft >= num_bits)
-		return;
-
-	if (unlikely(is->end - is->next < 2))
-		goto overflow;
-
-	is->bitbuf |= (u32)get_unaligned_le16(is->next) << (16 - is->bitsleft);
-	is->next += 2;
-	is->bitsleft += 16;
-
-	if (unlikely(num_bits == 17 && is->bitsleft == 16)) {
-		if (unlikely(is->end - is->next < 2))
-			goto overflow;
-
-		is->bitbuf |= (u32)get_unaligned_le16(is->next);
-		is->next += 2;
-		is->bitsleft = 32;
+	if (is->bitsleft < num_bits) {
+		if (is->end - is->next >= 2) {
+			is->bitbuf |= (u32)get_unaligned_le16(is->next)
+					<< (16 - is->bitsleft);
+			is->next += 2;
+		}
+		is->bitsleft += 16;
 	}
-
-	return;
-
-overflow:
-	is->bitsleft = 32;
 }
 
 /* Return the next @num_bits bits from the bitstream, without removing them.
@@ -183,376 +186,167 @@ bitstream_read_u32(struct input_bitstream *is)
 }
 
 /* Read into @dst_buffer an array of literal bytes embedded in the bitstream.
- * Return 0 if there were enough bytes remaining in the input, otherwise -1.
+ * Return either a pointer to the byte past the last written, or NULL if the
+ * read overflows the input buffer.
  */
-static forceinline int
-bitstream_read_bytes(struct input_bitstream *is, void *dst_buffer, size_t count)
+static forceinline void *bitstream_read_bytes(struct input_bitstream *is,
+					      void *dst_buffer, size_t count)
 {
-	if (unlikely(is->end - is->next < count))
-		return -1;
+	if ((size_t)(is->end - is->next) < count)
+		return NULL;
 	memcpy(dst_buffer, is->next, count);
 	is->next += count;
-	return 0;
+	return (u8 *)dst_buffer + count;
 }
 
 /* Align the input bitstream on a coding-unit boundary.  */
-static forceinline void
-bitstream_align(struct input_bitstream *is)
+static forceinline void bitstream_align(struct input_bitstream *is)
 {
 	is->bitsleft = 0;
 	is->bitbuf = 0;
 }
 
-/******************************************************************************/
-/*                             Huffman decoding                               */
-/*----------------------------------------------------------------------------*/
+extern int make_huffman_decode_table(u16 decode_table[], const u32 num_syms,
+				     const u32 num_bits, const u8 lens[],
+				     const u32 max_codeword_len,
+				     u16 working_space[]);
 
-/*
- * Required alignment for the Huffman decode tables.  We require this alignment
- * so that we can fill the entries with vector or word instructions and not have
- * to deal with misaligned buffers.
- */
-#define DECODE_TABLE_ALIGNMENT 16
 
-/*
- * Each decode table entry is 16 bits divided into two fields: 'symbol' (high 12
- * bits) and 'length' (low 4 bits).  The precise meaning of these fields depends
- * on the type of entry:
- *
- * Root table entries which are *not* subtable pointers:
- *	symbol: symbol to decode
- *	length: codeword length in bits
- *
- * Root table entries which are subtable pointers:
- *	symbol: index of start of subtable
- *	length: number of bits with which the subtable is indexed
- *
- * Subtable entries:
- *	symbol: symbol to decode
- *	length: codeword length in bits, minus the number of bits with which the
- *		root table is indexed
+/* Reads and returns the next Huffman-encoded symbol from a bitstream.  If the
+ * input data is exhausted, the Huffman symbol is decoded as if the missing bits
+ * are all zeroes.
  */
-#define DECODE_TABLE_SYMBOL_SHIFT  4
-#define DECODE_TABLE_MAX_SYMBOL	   ((1 << (16 - DECODE_TABLE_SYMBOL_SHIFT)) - 1)
-#define DECODE_TABLE_MAX_LENGTH    ((1 << DECODE_TABLE_SYMBOL_SHIFT) - 1)
-#define DECODE_TABLE_LENGTH_MASK   DECODE_TABLE_MAX_LENGTH
-#define MAKE_DECODE_TABLE_ENTRY(symbol, length) \
-	(((symbol) << DECODE_TABLE_SYMBOL_SHIFT) | (length))
-
-/*
- * Read and return the next Huffman-encoded symbol from the given bitstream
- * using the given decode table.
- *
- * If the input data is exhausted, then the Huffman symbol will be decoded as if
- * the missing bits were all zeroes.
- *
- * XXX: This is mostly duplicated in lzms_decode_huffman_symbol() in
- * lzms_decompress.c; keep them in sync!
- */
-static forceinline u32
-read_huffsym(struct input_bitstream *is, const u16 decode_table[],
-	     u32 table_bits, u32 max_codeword_len)
+static forceinline u32 read_huffsym(struct input_bitstream *istream,
+					 const u16 decode_table[],
+					 u32 table_bits,
+					 u32 max_codeword_len)
 {
 	u32 entry;
-	u32 symbol;
-	u32 length;
-
-	/* Preload the bitbuffer with 'max_codeword_len' bits so that we're
-	 * guaranteed to be able to fully decode a codeword.
-	 */
-	bitstream_ensure_bits(is, max_codeword_len);
-
-	/* Index the root table by the next 'table_bits' bits of input. */
-	entry = decode_table[bitstream_peek_bits(is, table_bits)];
+	u32 key_bits;
 
-	/* Extract the "symbol" and "length" from the entry. */
-	symbol = entry >> DECODE_TABLE_SYMBOL_SHIFT;
-	length = entry & DECODE_TABLE_LENGTH_MASK;
+	bitstream_ensure_bits(istream, max_codeword_len);
 
-	/* If the root table is indexed by the full 'max_codeword_len' bits,
-	 * then there cannot be any subtables, and this will be known at compile
-	 * time.  Otherwise, we must check whether the decoded symbol is really
-	 * a subtable pointer.  If so, we must discard the bits with which the
-	 * root table was indexed, then index the subtable by the next 'length'
-	 * bits of input to get the real entry.
-	 */
-	if (max_codeword_len > table_bits &&
-	    entry >= (1U << (table_bits + DECODE_TABLE_SYMBOL_SHIFT))) {
-		/* Subtable required */
-		bitstream_remove_bits(is, table_bits);
-		entry = decode_table[symbol + bitstream_peek_bits(is, length)];
-		symbol = entry >> DECODE_TABLE_SYMBOL_SHIFT;
-		length = entry & DECODE_TABLE_LENGTH_MASK;
+	/* Index the decode table by the next table_bits bits of the input.  */
+	key_bits = bitstream_peek_bits(istream, table_bits);
+	entry = decode_table[key_bits];
+	if (entry < 0xC000) {
+		/* Fast case: The decode table directly provided the
+		 * symbol and codeword length.  The low 11 bits are the
+		 * symbol, and the high 5 bits are the codeword length.
+		 */
+		bitstream_remove_bits(istream, entry >> 11);
+		return entry & 0x7FF;
 	}
-
-	/* Discard the bits (or the remaining bits, if a subtable was required)
-	 * of the codeword.
+	/* Slow case: The codeword for the symbol is longer than
+	 * table_bits, so the symbol does not have an entry
+	 * directly in the first (1 << table_bits) entries of the
+	 * decode table.  Traverse the appropriate binary tree
+	 * bit-by-bit to decode the symbol.
 	 */
-	bitstream_remove_bits(is, length);
-
-	/* Return the decoded symbol. */
-	return symbol;
+	bitstream_remove_bits(istream, table_bits);
+	do {
+		key_bits = (entry & 0x3FFF) + bitstream_pop_bits(istream, 1);
+	} while ((entry = decode_table[key_bits]) >= 0xC000);
+	return entry;
 }
 
 /*
- * The DECODE_TABLE_ENOUGH() macro evaluates to the maximum number of decode
- * table entries, including all subtable entries, that may be required for
- * decoding a given Huffman code.  This depends on three parameters:
- *
- *	num_syms: the maximum number of symbols in the code
- *	table_bits: the number of bits with which the root table will be indexed
- *	max_codeword_len: the maximum allowed codeword length in the code
+ * Copy an LZ77 match at (dst - offset) to dst.
  *
- * Given these parameters, the utility program 'enough' from zlib, when passed
- * the three arguments 'num_syms', 'table_bits', and 'max_codeword_len', will
- * compute the maximum number of entries required.  This has already been done
- * for the combinations we need and incorporated into the macro below so that
- * the mapping can be done at compilation time.  If an unknown combination is
- * used, then a compilation error will result.  To fix this, use 'enough' to
- * find the missing value and add it below.  If that still doesn't fix the
- * compilation error, then most likely a constraint would be violated by the
- * requested parameters, so they cannot be used, at least without other changes
- * to the decode table --- see DECODE_TABLE_SIZE().
- */
-#define DECODE_TABLE_ENOUGH(num_syms, table_bits, max_codeword_len) ( \
-	((num_syms) == 8 && (table_bits) == 7 && (max_codeword_len) == 15) ? 128 : \
-	((num_syms) == 8 && (table_bits) == 5 && (max_codeword_len) == 7) ? 36 : \
-	((num_syms) == 8 && (table_bits) == 6 && (max_codeword_len) == 7) ? 66 : \
-	((num_syms) == 8 && (table_bits) == 7 && (max_codeword_len) == 7) ? 128 : \
-	((num_syms) == 20 && (table_bits) == 5 && (max_codeword_len) == 15) ? 1062 : \
-	((num_syms) == 20 && (table_bits) == 6 && (max_codeword_len) == 15) ? 582 : \
-	((num_syms) == 20 && (table_bits) == 7 && (max_codeword_len) == 15) ? 390 : \
-	((num_syms) == 54 && (table_bits) == 9 && (max_codeword_len) == 15) ? 618 : \
-	((num_syms) == 54 && (table_bits) == 10 && (max_codeword_len) == 15) ? 1098 : \
-	((num_syms) == 249 && (table_bits) == 9 && (max_codeword_len) == 16) ? 878 : \
-	((num_syms) == 249 && (table_bits) == 10 && (max_codeword_len) == 16) ? 1326 : \
-	((num_syms) == 249 && (table_bits) == 11 && (max_codeword_len) == 16) ? 2318 : \
-	((num_syms) == 256 && (table_bits) == 9 && (max_codeword_len) == 15) ? 822 : \
-	((num_syms) == 256 && (table_bits) == 10 && (max_codeword_len) == 15) ? 1302 : \
-	((num_syms) == 256 && (table_bits) == 11 && (max_codeword_len) == 15) ? 2310 : \
-	((num_syms) == 512 && (table_bits) == 10 && (max_codeword_len) == 15) ? 1558 : \
-	((num_syms) == 512 && (table_bits) == 11 && (max_codeword_len) == 15) ? 2566 : \
-	((num_syms) == 512 && (table_bits) == 12 && (max_codeword_len) == 15) ? 4606 : \
-	((num_syms) == 656 && (table_bits) == 10 && (max_codeword_len) == 16) ? 1734 : \
-	((num_syms) == 656 && (table_bits) == 11 && (max_codeword_len) == 16) ? 2726 : \
-	((num_syms) == 656 && (table_bits) == 12 && (max_codeword_len) == 16) ? 4758 : \
-	((num_syms) == 799 && (table_bits) == 9 && (max_codeword_len) == 15) ? 1366 : \
-	((num_syms) == 799 && (table_bits) == 10 && (max_codeword_len) == 15) ? 1846 : \
-	((num_syms) == 799 && (table_bits) == 11 && (max_codeword_len) == 15) ? 2854 : \
-	-1)
-
-/* Wrapper around DECODE_TABLE_ENOUGH() that does additional compile-time
- * validation.
- */
-#define DECODE_TABLE_SIZE(num_syms, table_bits, max_codeword_len) (	\
-									\
-	/* All values must be positive. */				\
-	STATIC_ASSERT_ZERO((num_syms) > 0) +				\
-	STATIC_ASSERT_ZERO((table_bits) > 0) +				\
-	STATIC_ASSERT_ZERO((max_codeword_len) > 0) +			\
-									\
-	/* There cannot be more symbols than possible codewords. */	\
-	STATIC_ASSERT_ZERO((num_syms) <= 1U << (max_codeword_len)) +	\
-									\
-	/* There is no reason for the root table to be indexed with */	\
-	/* more bits than the maximum codeword length. */		\
-	STATIC_ASSERT_ZERO((table_bits) <= (max_codeword_len)) +	\
-									\
-	/* The maximum symbol value must fit in the 'symbol' field. */	\
-	STATIC_ASSERT_ZERO((num_syms) - 1 <= DECODE_TABLE_MAX_SYMBOL) +	\
-									\
-	/* The maximum codeword length in the root table must fit in */ \
-	/* the 'length' field. */					\
-	STATIC_ASSERT_ZERO((table_bits) <= DECODE_TABLE_MAX_LENGTH) +	\
-									\
-	/* The maximum codeword length in a subtable must fit in the */	\
-	/* 'length' field. */						\
-	STATIC_ASSERT_ZERO((max_codeword_len) - (table_bits) <=		\
-				DECODE_TABLE_MAX_LENGTH) +		\
-									\
-	/* The minimum subtable index must be greater than the maximum */\
-	/* symbol value.  If this were not the case, then there would */\
-	/* be no way to tell whether a given root table entry is a */	\
-	/* "subtable pointer" or not.  (An alternate solution would */	\
-	/* be to reserve a flag bit specifically for this purpose.) */	\
-	STATIC_ASSERT_ZERO((1U << (table_bits)) > (num_syms) - 1) +	\
-									\
-	/* The needed 'enough' value must have been defined. */		\
-	STATIC_ASSERT_ZERO(DECODE_TABLE_ENOUGH(				\
-				(num_syms), (table_bits),		\
-				(max_codeword_len)) > 0) +		\
-									\
-	/* The maximum subtable index must fit in the 'symbol' field. */\
-	STATIC_ASSERT_ZERO(DECODE_TABLE_ENOUGH(				\
-				(num_syms), (table_bits),		\
-				(max_codeword_len)) - 1 <=		\
-					DECODE_TABLE_MAX_SYMBOL) +	\
-									\
-	/* Finally, make the macro evaluate to the needed maximum */	\
-	/* number of decode table entries. */				\
-	DECODE_TABLE_ENOUGH((num_syms), (table_bits),			\
-			    (max_codeword_len))				\
-)
-
-
-/*
- * Declare the decode table for a Huffman code, given several compile-time
- * constants that describe the code.  See DECODE_TABLE_ENOUGH() for details.
- *
- * Decode tables must be aligned to a DECODE_TABLE_ALIGNMENT-byte boundary.
- * This implies that if a decode table is nested inside a dynamically allocated
- * structure, then the outer structure must be allocated on a
- * DECODE_TABLE_ALIGNMENT-byte aligned boundary as well.
- */
-#define DECODE_TABLE(name, num_syms, table_bits, max_codeword_len) \
-	u16 name[DECODE_TABLE_SIZE((num_syms), (table_bits), \
-				   (max_codeword_len))]	\
-		_aligned_attribute(DECODE_TABLE_ALIGNMENT)
-
-/*
- * Declare the temporary "working_space" array needed for building the decode
- * table for a Huffman code.
- */
-#define DECODE_TABLE_WORKING_SPACE(name, num_syms, max_codeword_len)	\
-	u16 name[2 * ((max_codeword_len) + 1)  + (num_syms)]
-
-extern int
-make_huffman_decode_table(u16 decode_table[], u32 num_syms,
-			  u32 table_bits, const u8 lens[],
-			  u32 max_codeword_len, u16 working_space[]);
-
-/******************************************************************************/
-/*                             LZ match copying                               */
-/*----------------------------------------------------------------------------*/
-
-static forceinline void
-copy_word_unaligned(const void *src, void *dst)
-{
-	store_word_unaligned(load_word_unaligned(src), dst);
-}
-
-static forceinline size_t
-repeat_u16(u16 b)
-{
-	size_t v = b;
-
-	STATIC_ASSERT(WORDBITS == 32 || WORDBITS == 64);
-	v |= v << 16;
-	v |= v << ((WORDBITS == 64) ? 32 : 0);
-	return v;
-}
-
-static forceinline size_t
-repeat_byte(u8 b)
-{
-	return repeat_u16(((u16)b << 8) | b);
-}
-
-/*
- * Copy an LZ77 match of 'length' bytes from the match source at 'out_next -
- * offset' to the match destination at 'out_next'.  The source and destination
- * may overlap.
+ * The length and offset must be already validated --- that is, (dst - offset)
+ * can't underrun the output buffer, and (dst + length) can't overrun the output
+ * buffer.  Also, the length cannot be 0.
  *
- * This handles validating the length and offset.  It is validated that the
- * beginning of the match source is '>= out_begin' and that end of the match
- * destination is '<= out_end'.  The return value is 0 if the match was valid
- * (and was copied), otherwise -1.
+ * @bufend points to the byte past the end of the output buffer.  This function
+ * won't write any data beyond this position.
  *
- * 'min_length' is a hint which specifies the minimum possible match length.
- * This should be a compile-time constant.
+ * Returns dst + length.
  */
-static forceinline int
-lz_copy(u32 length, u32 offset, u8 *out_begin, u8 *out_next, u8 *out_end,
-	u32 min_length)
+static forceinline u8 *lz_copy(u8 *dst, u32 length, u32 offset, const u8 *bufend,
+			       u32 min_length)
 {
-	const u8 *src;
-	u8 *end;
-
-	/* Validate the offset. */
-	if (unlikely(offset > out_next - out_begin))
-		return -1;
+	const u8 *src = dst - offset;
 
 	/*
-	 * Fast path: copy a match which is no longer than a few words, is not
-	 * overlapped such that copying a word at a time would produce incorrect
-	 * results, and is not too close to the end of the buffer.  Note that
-	 * this might copy more than the length of the match, but that's okay in
-	 * this scenario.
-	 */
-	src = out_next - offset;
-	if (UNALIGNED_ACCESS_IS_FAST && length <= 3 * WORDBYTES &&
-	    offset >= WORDBYTES && out_end - out_next >= 3 * WORDBYTES) {
-		copy_word_unaligned(src + WORDBYTES*0, out_next + WORDBYTES*0);
-		copy_word_unaligned(src + WORDBYTES*1, out_next + WORDBYTES*1);
-		copy_word_unaligned(src + WORDBYTES*2, out_next + WORDBYTES*2);
-		return 0;
-	}
-
-	/* Validate the length.  This isn't needed in the fast path above, due
-	 * to the additional conditions tested, but we do need it here.
-	 */
-	if (unlikely(length > out_end - out_next))
-		return -1;
-	end = out_next + length;
-
-	/*
-	 * Try to copy one word at a time.  On i386 and x86_64 this is faster
-	 * than copying one byte at a time, unless the data is near-random and
-	 * all the matches have very short lengths.  Note that since this
-	 * requires unaligned memory accesses, it won't necessarily be faster on
-	 * every architecture.
+	 * Try to copy one machine word at a time.  On i386 and x86_64 this is
+	 * faster than copying one byte at a time, unless the data is
+	 * near-random and all the matches have very short lengths.  Note that
+	 * since this requires unaligned memory accesses, it won't necessarily
+	 * be faster on every architecture.
 	 *
 	 * Also note that we might copy more than the length of the match.  For
 	 * example, if a word is 8 bytes and the match is of length 5, then
 	 * we'll simply copy 8 bytes.  This is okay as long as we don't write
-	 * beyond the end of the output buffer, hence the check for (out_end -
+	 * beyond the end of the output buffer, hence the check for (bufend -
 	 * end >= WORDBYTES - 1).
 	 */
-	if (UNALIGNED_ACCESS_IS_FAST && likely(out_end - end >= WORDBYTES - 1)) {
+#ifdef FAST_UNALIGNED_ACCESS
+	u8 * const end = dst + length;
+
+	if (bufend - end >= (ptrdiff_t)(WORDBYTES - 1)) {
+
 		if (offset >= WORDBYTES) {
-			/* The source and destination words don't overlap. */
-			do {
-				copy_word_unaligned(src, out_next);
-				src += WORDBYTES;
-				out_next += WORDBYTES;
-			} while (out_next < end);
-			return 0;
+			/* The source and destination words don't overlap.  */
+
+			/* To improve branch prediction, one iteration of this
+			 * loop is unrolled.  Most matches are short and will
+			 * fail the first check.  But if that check passes, then
+			 * it becomes increasing likely that the match is long
+			 * and we'll need to continue copying.
+			 */
+
+			copy_unaligned_word(src, dst);
+			src += WORDBYTES;
+			dst += WORDBYTES;
+
+			if (dst < end) {
+				do {
+					copy_unaligned_word(src, dst);
+					src += WORDBYTES;
+					dst += WORDBYTES;
+				} while (dst < end);
+			}
+			return end;
 		} else if (offset == 1) {
+
 			/* Offset 1 matches are equivalent to run-length
 			 * encoding of the previous byte.  This case is common
 			 * if the data contains many repeated bytes.
 			 */
-			size_t v = repeat_byte(*(out_next - 1));
+			size_t v = repeat_byte(*(dst - 1));
 
 			do {
-				store_word_unaligned(v, out_next);
+				put_unaligned(v, (size_t *)dst);
 				src += WORDBYTES;
-				out_next += WORDBYTES;
-			} while (out_next < end);
-			return 0;
+				dst += WORDBYTES;
+			} while (dst < end);
+			return end;
 		}
 		/*
 		 * We don't bother with special cases for other 'offset <
-		 * WORDBYTES', which are usually rarer than 'offset == 1'.
-		 * Extra checks will just slow things down.  Actually, it's
-		 * possible to handle all the 'offset < WORDBYTES' cases using
-		 * the same code, but it still becomes more complicated doesn't
-		 * seem any faster overall; it definitely slows down the more
-		 * common 'offset == 1' case.
+		 * WORDBYTES', which are usually rarer than 'offset == 1'.  Extra
+		 * checks will just slow things down.  Actually, it's possible
+		 * to handle all the 'offset < WORDBYTES' cases using the same
+		 * code, but it still becomes more complicated doesn't seem any
+		 * faster overall; it definitely slows down the more common
+		 * 'offset == 1' case.
 		 */
 	}
+#endif /* FAST_UNALIGNED_ACCESS */
 
 	/* Fall back to a bytewise copy.  */
-	if (min_length >= 2)
-		*out_next++ = *src++;
-	if (min_length >= 3)
-		*out_next++ = *src++;
-	if (min_length >= 4)
-		*out_next++ = *src++;
+
+	if (min_length >= 2) {
+		*dst++ = *src++;
+		length--;
+	}
+	if (min_length >= 3) {
+		*dst++ = *src++;
+		length--;
+	}
 	do {
-		*out_next++ = *src++;
-	} while (out_next != end);
-	return 0;
-}
+		*dst++ = *src++;
+	} while (--length);
 
-#endif /* _DECOMPRESS_COMMON_H */
+	return dst;
+}
diff --git a/fs/ntfs3/lib/lib.h b/fs/ntfs3/lib/lib.h
index a7fe82b76..f508fbad2 100644
--- a/fs/ntfs3/lib/lib.h
+++ b/fs/ntfs3/lib/lib.h
@@ -1,4 +1,12 @@
 /* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Adapted for linux kernel by Alexander Mamaev:
+ * - remove implementations of get_unaligned_
+ * - assume GCC is always defined
+ * - ISO C90
+ * - linux kernel code style
+ */
+
 
 /* globals from xpress_decompress.c */
 struct xpress_decompressor *xpress_allocate_decompressor(void);
@@ -10,7 +18,7 @@ int xpress_decompress(struct xpress_decompressor *__restrict d,
 		      size_t uncompressed_size);
 
 /* globals from lzx_decompress.c */
-struct lzx_decompressor *lzx_allocate_decompressor(size_t max_block_size);
+struct lzx_decompressor *lzx_allocate_decompressor(void);
 void lzx_free_decompressor(struct lzx_decompressor *d);
 int lzx_decompress(struct lzx_decompressor *__restrict d,
 		   const void *__restrict compressed_data,
diff --git a/fs/ntfs3/lib/lzx_common.c b/fs/ntfs3/lib/lzx_common.c
deleted file mode 100644
index 7c1e44af7..000000000
--- a/fs/ntfs3/lib/lzx_common.c
+++ /dev/null
@@ -1,204 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0-or-later
-/*
- * lzx_common.c - Common code for LZX compression and decompression.
- */
-
-/*
- * Copyright (C) 2012-2016 Eric Biggers
- *
- * This program is free software: you can redistribute it and/or modify it under
- * the terms of the GNU General Public License as published by the Free Software
- * Foundation, either version 2 of the License, or (at your option) any later
- * version.
- *
- * This program is distributed in the hope that it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
- * FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License along with
- * this program.  If not, see <http://www.gnu.org/licenses/>.
- */
-
-#include "lzx_common.h"
-
-/* Mapping: offset slot => first match offset that uses that offset slot.
- * The offset slots for repeat offsets map to "fake" offsets < 1.
- */
-const s32 lzx_offset_slot_base[LZX_MAX_OFFSET_SLOTS + 1] = {
-	-2,	 -1,	  0,	   1,	    2,		/* 0  --- 4  */
-	4,	 6,	  10,	   14,	    22,		/* 5  --- 9  */
-	30,	 46,	  62,	   94,	    126,	/* 10 --- 14 */
-	190,	 254,	  382,	   510,	    766,	/* 15 --- 19 */
-	1022,	 1534,	  2046,	   3070,    4094,	/* 20 --- 24 */
-	6142,	 8190,	  12286,   16382,   24574,	/* 25 --- 29 */
-	32766,	 49150,	  65534,   98302,   131070,	/* 30 --- 34 */
-	196606,	 262142,  393214,  524286,  655358,	/* 35 --- 39 */
-	786430,	 917502,  1048574, 1179646, 1310718,	/* 40 --- 44 */
-	1441790, 1572862, 1703934, 1835006, 1966078,	/* 45 --- 49 */
-	2097150						/* extra     */
-};
-
-/* Mapping: offset slot => how many extra bits must be read and added to the
- * corresponding offset slot base to decode the match offset.
- */
-const u8 lzx_extra_offset_bits[LZX_MAX_OFFSET_SLOTS] = {
-	0,	0,	0,	0,	1,
-	1,	2,	2,	3,	3,
-	4,	4,	5,	5,	6,
-	6,	7,	7,	8,	8,
-	9,	9,	10,	10,	11,
-	11,	12,	12,	13,	13,
-	14,	14,	15,	15,	16,
-	16,	17,	17,	17,	17,
-	17,	17,	17,	17,	17,
-	17,	17,	17,	17,	17,
-};
-
-
-/* Round the specified buffer size up to the next valid LZX window size, and
- * return its order (log2).  Or, if the buffer size is 0 or greater than the
- * largest valid LZX window size, return 0.
- */
-u32
-lzx_get_window_order(size_t max_bufsize)
-{
-	if (max_bufsize == 0 || max_bufsize > LZX_MAX_WINDOW_SIZE)
-		return 0;
-
-	return max(ilog2_ceil(max_bufsize), LZX_MIN_WINDOW_ORDER);
-}
-
-/* Given a valid LZX window order, return the number of symbols that will exist
- * in the main Huffman code.
- */
-u32
-lzx_get_num_main_syms(u32 window_order)
-{
-	/* Note: one would expect that the maximum match offset would be
-	 * 'window_size - LZX_MIN_MATCH_LEN', which would occur if the first two
-	 * bytes were to match the last two bytes.  However, the format
-	 * disallows this case.  This reduces the number of needed offset slots
-	 * by 1.
-	 */
-	u32 window_size = (u32)1 << window_order;
-	u32 max_offset = window_size - LZX_MIN_MATCH_LEN - 1;
-	u32 num_offset_slots = 30;
-
-	while (max_offset >= lzx_offset_slot_base[num_offset_slots])
-		num_offset_slots++;
-
-	return LZX_NUM_CHARS + (num_offset_slots * LZX_NUM_LEN_HEADERS);
-}
-
-static void
-do_translate_target(void *target, s32 input_pos)
-{
-	s32 abs_offset, rel_offset;
-
-	rel_offset = get_unaligned_le32(target);
-	if (rel_offset >= -input_pos && rel_offset < LZX_WIM_MAGIC_FILESIZE) {
-		if (rel_offset < LZX_WIM_MAGIC_FILESIZE - input_pos) {
-			/* "good translation" */
-			abs_offset = rel_offset + input_pos;
-		} else {
-			/* "compensating translation" */
-			abs_offset = rel_offset - LZX_WIM_MAGIC_FILESIZE;
-		}
-		put_unaligned_le32(abs_offset, target);
-	}
-}
-
-static void
-undo_translate_target(void *target, s32 input_pos)
-{
-	s32 abs_offset, rel_offset;
-
-	abs_offset = get_unaligned_le32(target);
-	if (abs_offset >= 0) {
-		if (abs_offset < LZX_WIM_MAGIC_FILESIZE) {
-			/* "good translation" */
-			rel_offset = abs_offset - input_pos;
-			put_unaligned_le32(rel_offset, target);
-		}
-	} else {
-		if (abs_offset >= -input_pos) {
-			/* "compensating translation" */
-			rel_offset = abs_offset + LZX_WIM_MAGIC_FILESIZE;
-			put_unaligned_le32(rel_offset, target);
-		}
-	}
-}
-
-/*
- * Do or undo the 'E8' preprocessing used in LZX.  Before compression, the
- * uncompressed data is preprocessed by changing the targets of x86 CALL
- * instructions from relative offsets to absolute offsets.  After decompression,
- * the translation is undone by changing the targets of x86 CALL instructions
- * from absolute offsets to relative offsets.
- *
- * Note that despite its intent, E8 preprocessing can be done on any data even
- * if it is not actually x86 machine code.  In fact, E8 preprocessing appears to
- * always be used in LZX-compressed resources in WIM files; there is no bit to
- * indicate whether it is used or not, unlike in the LZX compressed format as
- * used in cabinet files, where a bit is reserved for that purpose.
- *
- * E8 preprocessing is disabled in the last 6 bytes of the uncompressed data,
- * which really means the 5-byte call instruction cannot start in the last 10
- * bytes of the uncompressed data.  This is one of the errors in the LZX
- * documentation.
- *
- * E8 preprocessing does not appear to be disabled after the 32768th chunk of a
- * WIM resource, which apparently is another difference from the LZX compression
- * used in cabinet files.
- *
- * E8 processing is supposed to take the file size as a parameter, as it is used
- * in calculating the translated jump targets.	But in WIM files, this file size
- * is always the same (LZX_WIM_MAGIC_FILESIZE == 12000000).
- */
-static void
-lzx_e8_filter(u8 *data, u32 size, void (*process_target)(void *, s32))
-{
-	/*
-	 * A worthwhile optimization is to push the end-of-buffer check into the
-	 * relatively rare E8 case.  This is possible if we replace the last six
-	 * bytes of data with E8 bytes; then we are guaranteed to hit an E8 byte
-	 * before reaching end-of-buffer.  In addition, this scheme guarantees
-	 * that no translation can begin following an E8 byte in the last 10
-	 * bytes because a 4-byte offset containing E8 as its high byte is a
-	 * large negative number that is not valid for translation.  That is
-	 * exactly what we need.
-	 */
-	u8 *tail;
-	u8 saved_bytes[6];
-	u8 *p;
-
-	if (size <= 10)
-		return;
-
-	tail = &data[size - 6];
-	memcpy(saved_bytes, tail, 6);
-	memset(tail, 0xE8, 6);
-	p = data;
-	for (;;) {
-		while (*p != 0xE8)
-			p++;
-		if (p >= tail)
-			break;
-		(*process_target)(p + 1, p - data);
-		p += 5;
-	}
-	memcpy(tail, saved_bytes, 6);
-}
-
-void
-lzx_preprocess(u8 *data, u32 size)
-{
-	lzx_e8_filter(data, size, do_translate_target);
-}
-
-void
-lzx_postprocess(u8 *data, u32 size)
-{
-	lzx_e8_filter(data, size, undo_translate_target);
-}
diff --git a/fs/ntfs3/lib/lzx_common.h b/fs/ntfs3/lib/lzx_common.h
deleted file mode 100644
index 08324df55..000000000
--- a/fs/ntfs3/lib/lzx_common.h
+++ /dev/null
@@ -1,31 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0-or-later */
-
-/*
- * lzx_common.h
- *
- * Declarations shared between LZX compression and decompression.
- */
-
-#ifndef _LZX_COMMON_H
-#define _LZX_COMMON_H
-
-#include "lzx_constants.h"
-#include "common_defs.h"
-
-extern const s32 lzx_offset_slot_base[LZX_MAX_OFFSET_SLOTS + 1];
-
-extern const u8 lzx_extra_offset_bits[LZX_MAX_OFFSET_SLOTS];
-
-extern u32
-lzx_get_window_order(size_t max_bufsize);
-
-extern u32
-lzx_get_num_main_syms(u32 window_order);
-
-extern void
-lzx_preprocess(u8 *data, u32 size);
-
-extern void
-lzx_postprocess(u8 *data, u32 size);
-
-#endif /* _LZX_COMMON_H */
diff --git a/fs/ntfs3/lib/lzx_constants.h b/fs/ntfs3/lib/lzx_constants.h
deleted file mode 100644
index e2ee53f91..000000000
--- a/fs/ntfs3/lib/lzx_constants.h
+++ /dev/null
@@ -1,113 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0-or-later */
-/*
- * lzx_constants.h
- *
- * Constants for the LZX compression format.
- */
-
-#ifndef _LZX_CONSTANTS_H
-#define _LZX_CONSTANTS_H
-
-/* Number of literal byte values.  */
-#define LZX_NUM_CHARS	256
-
-/* The smallest and largest allowed match lengths.  */
-#define LZX_MIN_MATCH_LEN	2
-#define LZX_MAX_MATCH_LEN	257
-
-/* Number of distinct match lengths that can be represented.  */
-#define LZX_NUM_LENS		(LZX_MAX_MATCH_LEN - LZX_MIN_MATCH_LEN + 1)
-
-/* Number of match lengths for which no length symbol is required.  */
-#define LZX_NUM_PRIMARY_LENS	7
-#define LZX_NUM_LEN_HEADERS	(LZX_NUM_PRIMARY_LENS + 1)
-
-/* Valid values of the 3-bit block type field.  */
-#define LZX_BLOCKTYPE_VERBATIM       1
-#define LZX_BLOCKTYPE_ALIGNED        2
-#define LZX_BLOCKTYPE_UNCOMPRESSED   3
-
-/* 'LZX_MIN_WINDOW_SIZE' and 'LZX_MAX_WINDOW_SIZE' are the minimum and maximum
- * sizes of the sliding window.
- */
-#define LZX_MIN_WINDOW_ORDER	15u
-#define LZX_MAX_WINDOW_ORDER	21
-#define LZX_MIN_WINDOW_SIZE	(1UL << LZX_MIN_WINDOW_ORDER)  /* 32768   */
-#define LZX_MAX_WINDOW_SIZE	(1UL << LZX_MAX_WINDOW_ORDER)  /* 2097152 */
-
-/* Maximum number of offset slots.  (The actual number of offset slots depends
- * on the window size.)
- */
-#define LZX_MAX_OFFSET_SLOTS	50
-
-/* Maximum number of symbols in the main code.  (The actual number of symbols in
- * the main code depends on the window size.)
- */
-#define LZX_MAINCODE_MAX_NUM_SYMBOLS	\
-	(LZX_NUM_CHARS + (LZX_MAX_OFFSET_SLOTS * LZX_NUM_LEN_HEADERS))
-
-/* Number of symbols in the length code.  */
-#define LZX_LENCODE_NUM_SYMBOLS		(LZX_NUM_LENS - LZX_NUM_PRIMARY_LENS)
-
-/* Number of symbols in the pre-code.  */
-#define LZX_PRECODE_NUM_SYMBOLS		20
-
-/* Number of bits in which each pre-code codeword length is represented.  */
-#define LZX_PRECODE_ELEMENT_SIZE	4
-
-/* Number of low-order bits of each match offset that are entropy-encoded in
- * aligned offset blocks.
- */
-#define LZX_NUM_ALIGNED_OFFSET_BITS	3
-
-/* Number of symbols in the aligned offset code.  */
-#define LZX_ALIGNEDCODE_NUM_SYMBOLS	(1 << LZX_NUM_ALIGNED_OFFSET_BITS)
-
-/* Mask for the match offset bits that are entropy-encoded in aligned offset
- * blocks.
- */
-#define LZX_ALIGNED_OFFSET_BITMASK	((1 << LZX_NUM_ALIGNED_OFFSET_BITS) - 1)
-
-/* Number of bits in which each aligned offset codeword length is represented.  */
-#define LZX_ALIGNEDCODE_ELEMENT_SIZE	3
-
-/* The first offset slot which requires an aligned offset symbol in aligned
- * offset blocks.
- */
-#define LZX_MIN_ALIGNED_OFFSET_SLOT	8
-
-/* The offset slot base for LZX_MIN_ALIGNED_OFFSET_SLOT.  */
-#define LZX_MIN_ALIGNED_OFFSET		14
-
-/* The maximum number of extra offset bits in verbatim blocks.  (One would need
- * to subtract LZX_NUM_ALIGNED_OFFSET_BITS to get the number of extra offset
- * bits in *aligned* blocks.)
- */
-#define LZX_MAX_NUM_EXTRA_BITS		17
-
-/* Maximum lengths (in bits) for length-limited Huffman code construction.  */
-#define LZX_MAX_MAIN_CODEWORD_LEN	16
-#define LZX_MAX_LEN_CODEWORD_LEN	16
-#define LZX_MAX_PRE_CODEWORD_LEN	((1 << LZX_PRECODE_ELEMENT_SIZE) - 1)
-#define LZX_MAX_ALIGNED_CODEWORD_LEN	((1 << LZX_ALIGNEDCODE_ELEMENT_SIZE) - 1)
-
-/* For LZX-compressed blocks in WIM resources, this value is always used as the
- * filesize parameter for the call instruction (0xe8 byte) preprocessing, even
- * though the blocks themselves are not this size, and the size of the actual
- * file resource in the WIM file is very likely to be something entirely
- * different as well.
- */
-#define LZX_WIM_MAGIC_FILESIZE	12000000
-
-/* Assumed LZX block size when the encoded block size begins with a 0 bit.
- * This is probably WIM-specific.
- */
-#define LZX_DEFAULT_BLOCK_SIZE	32768
-
-/* Number of offsets in the recent (or "repeat") offsets queue.  */
-#define LZX_NUM_RECENT_OFFSETS	3
-
-/* An offset of n bytes is actually encoded as (n + LZX_OFFSET_ADJUSTMENT).  */
-#define LZX_OFFSET_ADJUSTMENT	(LZX_NUM_RECENT_OFFSETS - 1)
-
-#endif /* _LZX_CONSTANTS_H */
diff --git a/fs/ntfs3/lib/lzx_decompress.c b/fs/ntfs3/lib/lzx_decompress.c
index a048dd8bd..77a381a69 100644
--- a/fs/ntfs3/lib/lzx_decompress.c
+++ b/fs/ntfs3/lib/lzx_decompress.c
@@ -1,12 +1,11 @@
 // SPDX-License-Identifier: GPL-2.0-or-later
 /*
- * lzx_decompress.c
+ * lzx_decompress.c - A decompressor for the LZX compression format, which can
+ * be used in "System Compressed" files.  This is based on the code from wimlib.
+ * This code only supports a window size (dictionary size) of 32768 bytes, since
+ * this is the only size used in System Compression.
  *
- * A decompressor for the LZX compression format, as used in WIM files.
- */
-
-/*
- * Copyright (C) 2012-2016 Eric Biggers
+ * Copyright (C) 2015 Eric Biggers
  *
  * This program is free software: you can redistribute it and/or modify it under
  * the terms of the GNU General Public License as published by the Free Software
@@ -22,140 +21,258 @@
  * this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
-/*
- * LZX is an LZ77 and Huffman-code based compression format that has many
- * similarities to DEFLATE (the format used by zlib/gzip).  The compression
- * ratio is as good or better than DEFLATE.  See lzx_compress.c for a format
- * overview, and see https://en.wikipedia.org/wiki/LZX_(algorithm) for a
- * historical overview.  Here I make some pragmatic notes.
- *
- * The old specification for LZX is the document "Microsoft LZX Data Compression
- * Format" (1997).  It defines the LZX format as used in cabinet files.  Allowed
- * window sizes are 2^n where 15 <= n <= 21.  However, this document contains
- * several errors, so don't read too much into it...
- *
- * The new specification for LZX is the document "[MS-PATCH]: LZX DELTA
- * Compression and Decompression" (2014).  It defines the LZX format as used by
- * Microsoft's binary patcher.	It corrects several errors in the 1997 document
- * and extends the format in several ways --- namely, optional reference data,
- * up to 2^25 byte windows, and longer match lengths.
- *
- * WIM files use a more restricted form of LZX.  No LZX DELTA extensions are
- * present, the window is not "sliding", E8 preprocessing is done
- * unconditionally with a fixed file size, and the maximum window size is always
- * 2^15 bytes (equal to the size of each "chunk" in a compressed WIM resource).
- * This code is primarily intended to implement this form of LZX.  But although
- * not compatible with WIMGAPI, this code also supports maximum window sizes up
- * to 2^21 bytes.
- *
- * TODO: Add support for window sizes up to 2^25 bytes.
- */
-
 #include "decompress_common.h"
-#include "lzx_common.h"
 #include "lib.h"
 
+/* Number of literal byte values  */
+#define LZX_NUM_CHARS			256
+
+/* The smallest and largest allowed match lengths  */
+#define LZX_MIN_MATCH_LEN		2
+#define LZX_MAX_MATCH_LEN		257
+
+/* Number of distinct match lengths that can be represented  */
+#define LZX_NUM_LENS			(LZX_MAX_MATCH_LEN - LZX_MIN_MATCH_LEN + 1)
+
+/* Number of match lengths for which no length symbol is required  */
+#define LZX_NUM_PRIMARY_LENS		7
+#define LZX_NUM_LEN_HEADERS		(LZX_NUM_PRIMARY_LENS + 1)
+
+/* Valid values of the 3-bit block type field  */
+#define LZX_BLOCKTYPE_VERBATIM		1
+#define LZX_BLOCKTYPE_ALIGNED		2
+#define LZX_BLOCKTYPE_UNCOMPRESSED	3
+
+/* Number of offset slots for a window size of 32768  */
+#define LZX_NUM_OFFSET_SLOTS		30
+
+/* Number of symbols in the main code for a window size of 32768  */
+#define LZX_MAINCODE_NUM_SYMBOLS	\
+	(LZX_NUM_CHARS + (LZX_NUM_OFFSET_SLOTS * LZX_NUM_LEN_HEADERS))
+
+/* Number of symbols in the length code  */
+#define LZX_LENCODE_NUM_SYMBOLS		(LZX_NUM_LENS - LZX_NUM_PRIMARY_LENS)
+
+/* Number of symbols in the precode  */
+#define LZX_PRECODE_NUM_SYMBOLS		20
+
+/* Number of bits in which each precode codeword length is represented  */
+#define LZX_PRECODE_ELEMENT_SIZE	4
+
+/* Number of low-order bits of each match offset that are entropy-encoded in
+ * aligned offset blocks
+ */
+#define LZX_NUM_ALIGNED_OFFSET_BITS	3
+
+/* Number of symbols in the aligned offset code  */
+#define LZX_ALIGNEDCODE_NUM_SYMBOLS	(1 << LZX_NUM_ALIGNED_OFFSET_BITS)
+
+/* Mask for the match offset bits that are entropy-encoded in aligned offset
+ * blocks
+ */
+#define LZX_ALIGNED_OFFSET_BITMASK	((1 << LZX_NUM_ALIGNED_OFFSET_BITS) - 1)
+
+/* Number of bits in which each aligned offset codeword length is represented  */
+#define LZX_ALIGNEDCODE_ELEMENT_SIZE	3
+
+/* Maximum lengths (in bits) of the codewords in each Huffman code  */
+#define LZX_MAX_MAIN_CODEWORD_LEN	16
+#define LZX_MAX_LEN_CODEWORD_LEN	16
+#define LZX_MAX_PRE_CODEWORD_LEN	((1 << LZX_PRECODE_ELEMENT_SIZE) - 1)
+#define LZX_MAX_ALIGNED_CODEWORD_LEN	((1 << LZX_ALIGNEDCODE_ELEMENT_SIZE) - 1)
+
+/* The default "filesize" value used in pre/post-processing.  In the LZX format
+ * used in cabinet files this value must be given to the decompressor, whereas
+ * in the LZX format used in WIM files and system-compressed files this value is
+ * fixed at 12000000.
+ */
+#define LZX_DEFAULT_FILESIZE		12000000
+
+/* Assumed block size when the encoded block size begins with a 0 bit.  */
+#define LZX_DEFAULT_BLOCK_SIZE		32768
+
+/* Number of offsets in the recent (or "repeat") offsets queue.  */
+#define LZX_NUM_RECENT_OFFSETS		3
+
 /* These values are chosen for fast decompression.  */
 #define LZX_MAINCODE_TABLEBITS		11
-#define LZX_LENCODE_TABLEBITS		9
+#define LZX_LENCODE_TABLEBITS		10
 #define LZX_PRECODE_TABLEBITS		6
 #define LZX_ALIGNEDCODE_TABLEBITS	7
 
-#define LZX_READ_LENS_MAX_OVERRUN 50
+#define LZX_READ_LENS_MAX_OVERRUN	50
 
+/* Mapping: offset slot => first match offset that uses that offset slot.
+ */
+static const u32 lzx_offset_slot_base[LZX_NUM_OFFSET_SLOTS + 1] = {
+	0,	1,	2,	3,	4,	/* 0  --- 4  */
+	6,	8,	12,	16,	24,	/* 5  --- 9  */
+	32,	48,	64,	96,	128,	/* 10 --- 14 */
+	192,	256,	384,	512,	768,	/* 15 --- 19 */
+	1024,	1536,	2048,	3072,	4096,   /* 20 --- 24 */
+	6144,	8192,	12288,	16384,	24576,	/* 25 --- 29 */
+	32768,					/* extra     */
+};
+
+/* Mapping: offset slot => how many extra bits must be read and added to the
+ * corresponding offset slot base to decode the match offset.
+ */
+static const u8 lzx_extra_offset_bits[LZX_NUM_OFFSET_SLOTS] = {
+	0,	0,	0,	0,	1,
+	1,	2,	2,	3,	3,
+	4,	4,	5,	5,	6,
+	6,	7,	7,	8,	8,
+	9,	9,	10,	10,	11,
+	11,	12,	12,	13,	13,
+};
+
+/* Reusable heap-allocated memory for LZX decompression  */
 struct lzx_decompressor {
 
-	DECODE_TABLE(maincode_decode_table, LZX_MAINCODE_MAX_NUM_SYMBOLS,
-		     LZX_MAINCODE_TABLEBITS, LZX_MAX_MAIN_CODEWORD_LEN);
-	u8 maincode_lens[LZX_MAINCODE_MAX_NUM_SYMBOLS + LZX_READ_LENS_MAX_OVERRUN];
+	/* Huffman decoding tables, and arrays that map symbols to codeword
+	 * lengths
+	 */
+
+	u16 maincode_decode_table[(1 << LZX_MAINCODE_TABLEBITS) +
+					(LZX_MAINCODE_NUM_SYMBOLS * 2)];
+	u8 maincode_lens[LZX_MAINCODE_NUM_SYMBOLS + LZX_READ_LENS_MAX_OVERRUN];
+
 
-	DECODE_TABLE(lencode_decode_table, LZX_LENCODE_NUM_SYMBOLS,
-		     LZX_LENCODE_TABLEBITS, LZX_MAX_LEN_CODEWORD_LEN);
+	u16 lencode_decode_table[(1 << LZX_LENCODE_TABLEBITS) +
+					(LZX_LENCODE_NUM_SYMBOLS * 2)];
 	u8 lencode_lens[LZX_LENCODE_NUM_SYMBOLS + LZX_READ_LENS_MAX_OVERRUN];
 
-	union {
-		DECODE_TABLE(alignedcode_decode_table, LZX_ALIGNEDCODE_NUM_SYMBOLS,
-			     LZX_ALIGNEDCODE_TABLEBITS, LZX_MAX_ALIGNED_CODEWORD_LEN);
-		u8 alignedcode_lens[LZX_ALIGNEDCODE_NUM_SYMBOLS];
-	};
-
-	union {
-		DECODE_TABLE(precode_decode_table, LZX_PRECODE_NUM_SYMBOLS,
-			     LZX_PRECODE_TABLEBITS, LZX_MAX_PRE_CODEWORD_LEN);
-		u8 precode_lens[LZX_PRECODE_NUM_SYMBOLS];
-		u8 extra_offset_bits[LZX_MAX_OFFSET_SLOTS];
-	};
-
-	union {
-		DECODE_TABLE_WORKING_SPACE(maincode_working_space,
-					   LZX_MAINCODE_MAX_NUM_SYMBOLS,
-					   LZX_MAX_MAIN_CODEWORD_LEN);
-		DECODE_TABLE_WORKING_SPACE(lencode_working_space,
-					   LZX_LENCODE_NUM_SYMBOLS,
-					   LZX_MAX_LEN_CODEWORD_LEN);
-		DECODE_TABLE_WORKING_SPACE(alignedcode_working_space,
-					   LZX_ALIGNEDCODE_NUM_SYMBOLS,
-					   LZX_MAX_ALIGNED_CODEWORD_LEN);
-		DECODE_TABLE_WORKING_SPACE(precode_working_space,
-					   LZX_PRECODE_NUM_SYMBOLS,
-					   LZX_MAX_PRE_CODEWORD_LEN);
-	};
-
-	u32 window_order;
-	u32 num_main_syms;
-
-	/* Like lzx_extra_offset_bits[], but does not include the entropy-coded
-	 * bits of aligned offset blocks
-	 */
-	u8 extra_offset_bits_minus_aligned[LZX_MAX_OFFSET_SLOTS];
 
-} _aligned_attribute(DECODE_TABLE_ALIGNMENT);
+	u16 alignedcode_decode_table[(1 << LZX_ALIGNEDCODE_TABLEBITS) +
+					(LZX_ALIGNEDCODE_NUM_SYMBOLS * 2)];
+	u8 alignedcode_lens[LZX_ALIGNEDCODE_NUM_SYMBOLS];
+
+	u16 precode_decode_table[(1 << LZX_PRECODE_TABLEBITS) +
+				 (LZX_PRECODE_NUM_SYMBOLS * 2)];
+	u8 precode_lens[LZX_PRECODE_NUM_SYMBOLS];
 
-/* Read a Huffman-encoded symbol using the precode. */
-static forceinline u32
-read_presym(const struct lzx_decompressor *d, struct input_bitstream *is)
+	/* Temporary space for make_huffman_decode_table()  */
+	u16 working_space[2 * (1 + LZX_MAX_MAIN_CODEWORD_LEN) +
+			  LZX_MAINCODE_NUM_SYMBOLS];
+};
+
+static void undo_e8_translation(void *target, s32 input_pos)
+{
+	s32 abs_offset, rel_offset;
+
+	abs_offset = get_unaligned_le32(target);
+	if (abs_offset >= 0) {
+		if (abs_offset < LZX_DEFAULT_FILESIZE) {
+			/* "good translation" */
+			rel_offset = abs_offset - input_pos;
+			put_unaligned_le32(rel_offset, target);
+		}
+	} else {
+		if (abs_offset >= -input_pos) {
+			/* "compensating translation" */
+			rel_offset = abs_offset + LZX_DEFAULT_FILESIZE;
+			put_unaligned_le32(rel_offset, target);
+		}
+	}
+}
+
+/*
+ * Undo the 'E8' preprocessing used in LZX.  Before compression, the
+ * uncompressed data was preprocessed by changing the targets of suspected x86
+ * CALL instructions from relative offsets to absolute offsets.  After
+ * match/literal decoding, the decompressor must undo the translation.
+ */
+static void lzx_postprocess(u8 *data, u32 size)
+{
+	/*
+	 * A worthwhile optimization is to push the end-of-buffer check into the
+	 * relatively rare E8 case.  This is possible if we replace the last six
+	 * bytes of data with E8 bytes; then we are guaranteed to hit an E8 byte
+	 * before reaching end-of-buffer.  In addition, this scheme guarantees
+	 * that no translation can begin following an E8 byte in the last 10
+	 * bytes because a 4-byte offset containing E8 as its high byte is a
+	 * large negative number that is not valid for translation.  That is
+	 * exactly what we need.
+	 */
+	u8 *tail;
+	u8 saved_bytes[6];
+	u8 *p;
+
+	if (size <= 10)
+		return;
+
+	tail = &data[size - 6];
+	memcpy(saved_bytes, tail, 6);
+	memset(tail, 0xE8, 6);
+	p = data;
+	for (;;) {
+		while (*p != 0xE8)
+			p++;
+		if (p >= tail)
+			break;
+		undo_e8_translation(p + 1, p - data);
+		p += 5;
+	}
+	memcpy(tail, saved_bytes, 6);
+}
+
+/* Read a Huffman-encoded symbol using the precode.  */
+static forceinline u32 read_presym(const struct lzx_decompressor *d,
+					struct input_bitstream *is)
 {
 	return read_huffsym(is, d->precode_decode_table,
 			    LZX_PRECODE_TABLEBITS, LZX_MAX_PRE_CODEWORD_LEN);
 }
 
-/* Read a Huffman-encoded symbol using the main code. */
-static forceinline u32
-read_mainsym(const struct lzx_decompressor *d, struct input_bitstream *is)
+/* Read a Huffman-encoded symbol using the main code.  */
+static forceinline u32 read_mainsym(const struct lzx_decompressor *d,
+					 struct input_bitstream *is)
 {
 	return read_huffsym(is, d->maincode_decode_table,
 			    LZX_MAINCODE_TABLEBITS, LZX_MAX_MAIN_CODEWORD_LEN);
 }
 
-/* Read a Huffman-encoded symbol using the length code. */
-static forceinline u32
-read_lensym(const struct lzx_decompressor *d, struct input_bitstream *is)
+/* Read a Huffman-encoded symbol using the length code.  */
+static forceinline u32 read_lensym(const struct lzx_decompressor *d,
+					struct input_bitstream *is)
 {
 	return read_huffsym(is, d->lencode_decode_table,
 			    LZX_LENCODE_TABLEBITS, LZX_MAX_LEN_CODEWORD_LEN);
 }
 
-/* Read a Huffman-encoded symbol using the aligned offset code. */
-static forceinline u32
-read_alignedsym(const struct lzx_decompressor *d, struct input_bitstream *is)
+/* Read a Huffman-encoded symbol using the aligned offset code.  */
+static forceinline u32 read_alignedsym(const struct lzx_decompressor *d,
+					    struct input_bitstream *is)
 {
 	return read_huffsym(is, d->alignedcode_decode_table,
-			    LZX_ALIGNEDCODE_TABLEBITS, LZX_MAX_ALIGNED_CODEWORD_LEN);
+			    LZX_ALIGNEDCODE_TABLEBITS,
+			    LZX_MAX_ALIGNED_CODEWORD_LEN);
 }
 
 /*
- * Read a precode from the compressed input bitstream, then use it to decode
- * @num_lens codeword length values and write them to @lens.
+ * Read the precode from the compressed input bitstream, then use it to decode
+ * @num_lens codeword length values.
+ *
+ * @is:		The input bitstream.
+ *
+ * @lens:	An array that contains the length values from the previous time
+ *		the codeword lengths for this Huffman code were read, or all 0's
+ *		if this is the first time.  This array must have at least
+ *		(@num_lens + LZX_READ_LENS_MAX_OVERRUN) entries.
+ *
+ * @num_lens:	Number of length values to decode.
+ *
+ * Returns 0 on success, or -1 if the data was invalid.
  */
-static int
-lzx_read_codeword_lens(struct lzx_decompressor *d, struct input_bitstream *is,
-		       u8 *lens, u32 num_lens)
+static int lzx_read_codeword_lens(struct lzx_decompressor *d,
+				  struct input_bitstream *is,
+				  u8 *lens, u32 num_lens)
 {
 	u8 *len_ptr = lens;
 	u8 *lens_end = lens + num_lens;
 	int i;
 
-	/* Read the lengths of the precode codewords.  These are stored
+	/* Read the lengths of the precode codewords.  These are given
 	 * explicitly.
 	 */
 	for (i = 0; i < LZX_PRECODE_NUM_SYMBOLS; i++) {
@@ -163,13 +280,13 @@ lzx_read_codeword_lens(struct lzx_decompressor *d, struct input_bitstream *is,
 			bitstream_read_bits(is, LZX_PRECODE_ELEMENT_SIZE);
 	}
 
-	/* Build the decoding table for the precode. */
+	/* Make the decoding table for the precode.  */
 	if (make_huffman_decode_table(d->precode_decode_table,
 				      LZX_PRECODE_NUM_SYMBOLS,
 				      LZX_PRECODE_TABLEBITS,
 				      d->precode_lens,
 				      LZX_MAX_PRE_CODEWORD_LEN,
-				      d->precode_working_space))
+				      d->working_space))
 		return -1;
 
 	/* Decode the codeword lengths.  */
@@ -202,7 +319,7 @@ lzx_read_codeword_lens(struct lzx_decompressor *d, struct input_bitstream *is,
 				/* Run of identical lengths  */
 				run_len = 4 + bitstream_read_bits(is, 1);
 				presym = read_presym(d, is);
-				if (unlikely(presym > 17))
+				if (presym > 17)
 					return -1;
 				len = *len_ptr - presym;
 				if ((s8)len < 0)
@@ -212,8 +329,7 @@ lzx_read_codeword_lens(struct lzx_decompressor *d, struct input_bitstream *is,
 			do {
 				*len_ptr++ = len;
 			} while (--run_len);
-			/*
-			 * The worst case overrun is when presym == 18,
+			/* Worst case overrun is when presym == 18,
 			 * run_len == 20 + 31, and only 1 length was remaining.
 			 * So LZX_READ_LENS_MAX_OVERRUN == 50.
 			 *
@@ -230,15 +346,20 @@ lzx_read_codeword_lens(struct lzx_decompressor *d, struct input_bitstream *is,
 }
 
 /*
- * Read the header of an LZX block.  For all block types, the block type and
- * size is saved in *block_type_ret and *block_size_ret, respectively.	For
- * compressed blocks, the codeword lengths are also saved.  For uncompressed
- * blocks, the recent offsets queue is also updated.
+ * Read the header of an LZX block and save the block type and (uncompressed)
+ * size in *block_type_ret and *block_size_ret, respectively.
+ *
+ * If the block is compressed, also update the Huffman decode @tables with the
+ * new Huffman codes.  If the block is uncompressed, also update the match
+ * offset @queue with the new match offsets.
+ *
+ * Return 0 on success, or -1 if the data was invalid.
  */
-static int
-lzx_read_block_header(struct lzx_decompressor *d, struct input_bitstream *is,
-		      u32 recent_offsets[], int *block_type_ret,
-		      u32 *block_size_ret)
+static int lzx_read_block_header(struct lzx_decompressor *d,
+				 struct input_bitstream *is,
+				 int *block_type_ret,
+				 u32 *block_size_ret,
+				 u32 recent_offsets[])
 {
 	int block_type;
 	u32 block_size;
@@ -246,25 +367,27 @@ lzx_read_block_header(struct lzx_decompressor *d, struct input_bitstream *is,
 
 	bitstream_ensure_bits(is, 4);
 
-	/* Read the block type. */
+	/* The first three bits tell us what kind of block it is, and should be
+	 * one of the LZX_BLOCKTYPE_* values.
+	 */
 	block_type = bitstream_pop_bits(is, 3);
 
-	/* Read the block size. */
+	/* Read the block size.  */
 	if (bitstream_pop_bits(is, 1)) {
 		block_size = LZX_DEFAULT_BLOCK_SIZE;
 	} else {
-		block_size = bitstream_read_bits(is, 16);
-		if (d->window_order >= 16) {
-			block_size <<= 8;
-			block_size |= bitstream_read_bits(is, 8);
-		}
+		block_size = 0;
+		block_size |= bitstream_read_bits(is, 8);
+		block_size <<= 8;
+		block_size |= bitstream_read_bits(is, 8);
 	}
 
 	switch (block_type) {
 
 	case LZX_BLOCKTYPE_ALIGNED:
 
-		/* Read the aligned offset codeword lengths. */
+		/* Read the aligned offset code and prepare its decode table.
+		 */
 
 		for (i = 0; i < LZX_ALIGNEDCODE_NUM_SYMBOLS; i++) {
 			d->alignedcode_lens[i] =
@@ -272,6 +395,14 @@ lzx_read_block_header(struct lzx_decompressor *d, struct input_bitstream *is,
 						    LZX_ALIGNEDCODE_ELEMENT_SIZE);
 		}
 
+		if (make_huffman_decode_table(d->alignedcode_decode_table,
+					      LZX_ALIGNEDCODE_NUM_SYMBOLS,
+					      LZX_ALIGNEDCODE_TABLEBITS,
+					      d->alignedcode_lens,
+					      LZX_MAX_ALIGNED_CODEWORD_LEN,
+					      d->working_space))
+			return -1;
+
 		/* Fall though, since the rest of the header for aligned offset
 		 * blocks is the same as that for verbatim blocks.
 		 */
@@ -279,36 +410,56 @@ lzx_read_block_header(struct lzx_decompressor *d, struct input_bitstream *is,
 
 	case LZX_BLOCKTYPE_VERBATIM:
 
-		/* Read the main codeword lengths, which are divided into two
-		 * parts: literal symbols and match headers.
+		/* Read the main code and prepare its decode table.
+		 *
+		 * Note that the codeword lengths in the main code are encoded
+		 * in two parts: one part for literal symbols, and one part for
+		 * match symbols.
 		 */
+
 		if (lzx_read_codeword_lens(d, is, d->maincode_lens,
 					   LZX_NUM_CHARS))
 			return -1;
 
-		if (lzx_read_codeword_lens(d, is, d->maincode_lens + LZX_NUM_CHARS,
-					   d->num_main_syms - LZX_NUM_CHARS))
+		if (lzx_read_codeword_lens(d, is,
+					   d->maincode_lens + LZX_NUM_CHARS,
+					   LZX_MAINCODE_NUM_SYMBOLS - LZX_NUM_CHARS))
 			return -1;
 
+		if (make_huffman_decode_table(d->maincode_decode_table,
+					      LZX_MAINCODE_NUM_SYMBOLS,
+					      LZX_MAINCODE_TABLEBITS,
+					      d->maincode_lens,
+					      LZX_MAX_MAIN_CODEWORD_LEN,
+					      d->working_space))
+			return -1;
 
-		/* Read the length codeword lengths. */
+		/* Read the length code and prepare its decode table.  */
 
 		if (lzx_read_codeword_lens(d, is, d->lencode_lens,
 					   LZX_LENCODE_NUM_SYMBOLS))
 			return -1;
 
+		if (make_huffman_decode_table(d->lencode_decode_table,
+					      LZX_LENCODE_NUM_SYMBOLS,
+					      LZX_LENCODE_TABLEBITS,
+					      d->lencode_lens,
+					      LZX_MAX_LEN_CODEWORD_LEN,
+					      d->working_space))
+			return -1;
+
 		break;
 
 	case LZX_BLOCKTYPE_UNCOMPRESSED:
-		/*
-		 * The header of an uncompressed block contains new values for
-		 * the recent offsets queue, starting on the next 16-bit
-		 * boundary in the bitstream.  Careful: if the stream is
-		 * *already* aligned, the correct thing to do is to throw away
-		 * the next 16 bits (this is probably a mistake in the format).
+
+		/* Before reading the three recent offsets from the uncompressed
+		 * block header, the stream must be aligned on a 16-bit
+		 * boundary.  But if the stream is *already* aligned, then the
+		 * next 16 bits must be discarded.
 		 */
 		bitstream_ensure_bits(is, 1);
 		bitstream_align(is);
+
 		recent_offsets[0] = bitstream_read_u32(is);
 		recent_offsets[1] = bitstream_read_u32(is);
 		recent_offsets[2] = bitstream_read_u32(is);
@@ -329,226 +480,204 @@ lzx_read_block_header(struct lzx_decompressor *d, struct input_bitstream *is,
 	return 0;
 }
 
-/* Decompress a block of LZX-compressed data. */
-static int
-lzx_decompress_block(struct lzx_decompressor *d, struct input_bitstream *is,
-		     int block_type, u32 block_size,
-		     u8 * const out_begin, u8 *out_next, u32 recent_offsets[])
+/* Decompress a block of LZX-compressed data.  */
+static int lzx_decompress_block(const struct lzx_decompressor *d,
+				struct input_bitstream *is,
+				int block_type, u32 block_size,
+				u8 * const out_begin, u8 *out_next,
+				u32 recent_offsets[])
 {
 	u8 * const block_end = out_next + block_size;
-	u32 min_aligned_offset_slot;
-
-	/*
-	 * Build the Huffman decode tables.  We always need to build the main
-	 * and length decode tables.  For aligned blocks we additionally need to
-	 * build the aligned offset decode table.
-	 */
-
-	if (make_huffman_decode_table(d->maincode_decode_table,
-				      d->num_main_syms,
-				      LZX_MAINCODE_TABLEBITS,
-				      d->maincode_lens,
-				      LZX_MAX_MAIN_CODEWORD_LEN,
-				      d->maincode_working_space))
-		return -1;
-
-	if (make_huffman_decode_table(d->lencode_decode_table,
-				      LZX_LENCODE_NUM_SYMBOLS,
-				      LZX_LENCODE_TABLEBITS,
-				      d->lencode_lens,
-				      LZX_MAX_LEN_CODEWORD_LEN,
-				      d->lencode_working_space))
-		return -1;
-
-	if (block_type == LZX_BLOCKTYPE_ALIGNED) {
-		if (make_huffman_decode_table(d->alignedcode_decode_table,
-					      LZX_ALIGNEDCODE_NUM_SYMBOLS,
-					      LZX_ALIGNEDCODE_TABLEBITS,
-					      d->alignedcode_lens,
-					      LZX_MAX_ALIGNED_CODEWORD_LEN,
-					      d->alignedcode_working_space))
-			return -1;
-		min_aligned_offset_slot = LZX_MIN_ALIGNED_OFFSET_SLOT;
-		memcpy(d->extra_offset_bits, d->extra_offset_bits_minus_aligned,
-		       sizeof(lzx_extra_offset_bits));
-	} else {
-		min_aligned_offset_slot = LZX_MAX_OFFSET_SLOTS;
-		memcpy(d->extra_offset_bits, lzx_extra_offset_bits,
-		       sizeof(lzx_extra_offset_bits));
-	}
-
-	/* Decode the literals and matches. */
+	u32 ones_if_aligned = 0U - (block_type == LZX_BLOCKTYPE_ALIGNED);
 
 	do {
 		u32 mainsym;
-		u32 length;
-		u32 offset;
+		u32 match_len;
+		u32 match_offset;
 		u32 offset_slot;
+		u32 num_extra_bits;
 
 		mainsym = read_mainsym(d, is);
 		if (mainsym < LZX_NUM_CHARS) {
-			/* Literal */
+			/* Literal  */
 			*out_next++ = mainsym;
 			continue;
 		}
 
-		/* Match */
+		/* Match  */
 
 		/* Decode the length header and offset slot.  */
-		STATIC_ASSERT(LZX_NUM_CHARS % LZX_NUM_LEN_HEADERS == 0);
-		length = mainsym % LZX_NUM_LEN_HEADERS;
-		offset_slot = (mainsym - LZX_NUM_CHARS) / LZX_NUM_LEN_HEADERS;
+		mainsym -= LZX_NUM_CHARS;
+		match_len = mainsym % LZX_NUM_LEN_HEADERS;
+		offset_slot = mainsym / LZX_NUM_LEN_HEADERS;
 
 		/* If needed, read a length symbol to decode the full length. */
-		if (length == LZX_NUM_PRIMARY_LENS)
-			length += read_lensym(d, is);
-		length += LZX_MIN_MATCH_LEN;
+		if (match_len == LZX_NUM_PRIMARY_LENS)
+			match_len += read_lensym(d, is);
+		match_len += LZX_MIN_MATCH_LEN;
 
 		if (offset_slot < LZX_NUM_RECENT_OFFSETS) {
 			/* Repeat offset  */
 
 			/* Note: This isn't a real LRU queue, since using the R2
-			 * offset doesn't bump the R1 offset down to R2.
+			 * offset doesn't bump the R1 offset down to R2.  This
+			 * quirk allows all 3 recent offsets to be handled by
+			 * the same code.  (For R0, the swap is a no-op.)
 			 */
-			offset = recent_offsets[offset_slot];
+			match_offset = recent_offsets[offset_slot];
 			recent_offsets[offset_slot] = recent_offsets[0];
+			recent_offsets[0] = match_offset;
 		} else {
 			/* Explicit offset  */
-			offset = bitstream_read_bits(is, d->extra_offset_bits[offset_slot]);
-			if (offset_slot >= min_aligned_offset_slot) {
-				offset = (offset << LZX_NUM_ALIGNED_OFFSET_BITS) |
-					 read_alignedsym(d, is);
+
+			/* Look up the number of extra bits that need to be read
+			 * to decode offsets with this offset slot.
+			 */
+			num_extra_bits = lzx_extra_offset_bits[offset_slot];
+
+			/* Start with the offset slot base value.  */
+			match_offset = lzx_offset_slot_base[offset_slot];
+
+			/* In aligned offset blocks, the low-order 3 bits of
+			 * each offset are encoded using the aligned offset
+			 * code.  Otherwise, all the extra bits are literal.
+			 */
+
+			if ((num_extra_bits & ones_if_aligned) >= LZX_NUM_ALIGNED_OFFSET_BITS) {
+				match_offset +=
+					bitstream_read_bits(is, num_extra_bits -
+								LZX_NUM_ALIGNED_OFFSET_BITS)
+							<< LZX_NUM_ALIGNED_OFFSET_BITS;
+				match_offset += read_alignedsym(d, is);
+			} else {
+				match_offset += bitstream_read_bits(is, num_extra_bits);
 			}
-			offset += lzx_offset_slot_base[offset_slot];
 
-			/* Update the match offset LRU queue.  */
-			STATIC_ASSERT(LZX_NUM_RECENT_OFFSETS == 3);
+			/* Adjust the offset.  */
+			match_offset -= (LZX_NUM_RECENT_OFFSETS - 1);
+
+			/* Update the recent offsets.  */
 			recent_offsets[2] = recent_offsets[1];
 			recent_offsets[1] = recent_offsets[0];
+			recent_offsets[0] = match_offset;
 		}
-		recent_offsets[0] = offset;
 
-		/* Validate the match and copy it to the current position.  */
-		if (unlikely(lz_copy(length, offset, out_begin,
-				     out_next, block_end, LZX_MIN_MATCH_LEN)))
+		/* Validate the match, then copy it to the current position.  */
+
+		if (match_len > (size_t)(block_end - out_next))
+			return -1;
+
+		if (match_offset > (size_t)(out_next - out_begin))
 			return -1;
-		out_next += length;
+
+		out_next = lz_copy(out_next, match_len, match_offset,
+				   block_end, LZX_MIN_MATCH_LEN);
+
 	} while (out_next != block_end);
 
 	return 0;
 }
 
-int
-lzx_decompress(struct lzx_decompressor *__restrict d,
-	       const void *__restrict compressed_data, size_t compressed_size,
-	       void *__restrict uncompressed_data, size_t uncompressed_size)
+/*
+ * lzx_allocate_decompressor - Allocate an LZX decompressor
+ *
+ * Return the pointer to the decompressor on success, or return NULL and set
+ * errno on failure.
+ */
+struct lzx_decompressor *lzx_allocate_decompressor(void)
+{
+	return kmalloc(sizeof(struct lzx_decompressor), GFP_NOFS);
+}
+
+/*
+ * lzx_decompress - Decompress a buffer of LZX-compressed data
+ *
+ * @decompressor:      A decompressor allocated with lzx_allocate_decompressor()
+ * @compressed_data:	The buffer of data to decompress
+ * @compressed_size:	Number of bytes of compressed data
+ * @uncompressed_data:	The buffer in which to store the decompressed data
+ * @uncompressed_size:	The number of bytes the data decompresses into
+ *
+ * Return 0 on success, or return -1 and set errno on failure.
+ */
+int lzx_decompress(struct lzx_decompressor *decompressor,
+		   const void *compressed_data, size_t compressed_size,
+		   void *uncompressed_data, size_t uncompressed_size)
 {
+	struct lzx_decompressor *d = decompressor;
 	u8 * const out_begin = uncompressed_data;
 	u8 *out_next = out_begin;
 	u8 * const out_end = out_begin + uncompressed_size;
 	struct input_bitstream is;
 	u32 recent_offsets[LZX_NUM_RECENT_OFFSETS] = {1, 1, 1};
-	u32 may_have_e8_byte = 0;
-
-	STATIC_ASSERT(LZX_NUM_RECENT_OFFSETS == 3);
+	int e8_status = 0;
 
 	init_input_bitstream(&is, compressed_data, compressed_size);
 
-	/* Codeword lengths begin as all 0's for delta encoding purposes. */
-	memset(d->maincode_lens, 0, d->num_main_syms);
+	/* Codeword lengths begin as all 0's for delta encoding purposes.  */
+	memset(d->maincode_lens, 0, LZX_MAINCODE_NUM_SYMBOLS);
 	memset(d->lencode_lens, 0, LZX_LENCODE_NUM_SYMBOLS);
 
-	/* Decompress blocks until we have all the uncompressed data. */
+	/* Decompress blocks until we have all the uncompressed data.  */
 
 	while (out_next != out_end) {
 		int block_type;
 		u32 block_size;
 
-		if (lzx_read_block_header(d, &is, recent_offsets,
-					  &block_type, &block_size))
-			return -1;
+		if (lzx_read_block_header(d, &is, &block_type, &block_size,
+					  recent_offsets))
+			goto invalid;
 
-		if (block_size < 1 || block_size > out_end - out_next)
-			return -1;
+		if (block_size < 1 || block_size > (size_t)(out_end - out_next))
+			goto invalid;
 
-		if (likely(block_type != LZX_BLOCKTYPE_UNCOMPRESSED)) {
+		if (block_type != LZX_BLOCKTYPE_UNCOMPRESSED) {
 
-			/* Compressed block */
-			if (lzx_decompress_block(d, &is, block_type, block_size,
-						 out_begin, out_next,
+			/* Compressed block  */
+
+			if (lzx_decompress_block(d,
+						 &is,
+						 block_type,
+						 block_size,
+						 out_begin,
+						 out_next,
 						 recent_offsets))
-				return -1;
+				goto invalid;
 
-			/* If the first E8 byte was in this block, then it must
-			 * have been encoded as a literal using mainsym E8.
-			 */
-			may_have_e8_byte |= d->maincode_lens[0xE8];
+			e8_status |= d->maincode_lens[0xe8];
+			out_next += block_size;
 		} else {
+			/* Uncompressed block  */
 
-			/* Uncompressed block */
-			if (bitstream_read_bytes(&is, out_next, block_size))
-				return -1;
+			out_next = bitstream_read_bytes(&is, out_next,
+							block_size);
+			if (!out_next)
+				goto invalid;
 
-			/* Re-align the bitstream if needed. */
 			if (block_size & 1)
 				bitstream_read_byte(&is);
 
-			/* There may have been an E8 byte in the block. */
-			may_have_e8_byte = 1;
+			e8_status = 1;
 		}
-		out_next += block_size;
 	}
 
-	/* Postprocess the data unless it cannot possibly contain E8 bytes. */
-	if (may_have_e8_byte)
+	/* Postprocess the data unless it cannot possibly contain 0xe8 bytes. */
+	if (e8_status)
 		lzx_postprocess(uncompressed_data, uncompressed_size);
 
 	return 0;
-}
-
-struct lzx_decompressor *
-lzx_allocate_decompressor(size_t max_block_size)
-{
-	u32 window_order;
-	struct lzx_decompressor *d;
-	u32 offset_slot;
-
-	/*
-	 * ntfs uses lzx only as max_block_size == 0x8000
-	 * this value certainly will not fail
-	 * we can remove lzx_get_window_order + ilog2_ceil + bsrw
-	 */
-	WARN_ON(max_block_size != 0x8000);
-
-	window_order = lzx_get_window_order(max_block_size);
-	if (window_order == 0)
-		return ERR_PTR(-EINVAL);
-
-	d = aligned_malloc(sizeof(*d), DECODE_TABLE_ALIGNMENT);
-	if (!d)
-		return NULL;
-
-	d->window_order = window_order;
-	d->num_main_syms = lzx_get_num_main_syms(window_order);
-
-	/* Initialize 'd->extra_offset_bits_minus_aligned'. */
-	STATIC_ASSERT(sizeof(d->extra_offset_bits_minus_aligned) ==
-		      sizeof(lzx_extra_offset_bits));
-	STATIC_ASSERT(sizeof(d->extra_offset_bits) ==
-		      sizeof(lzx_extra_offset_bits));
-	memcpy(d->extra_offset_bits_minus_aligned, lzx_extra_offset_bits,
-	       sizeof(lzx_extra_offset_bits));
-	for (offset_slot = LZX_MIN_ALIGNED_OFFSET_SLOT;
-	     offset_slot < LZX_MAX_OFFSET_SLOTS; offset_slot++) {
-		d->extra_offset_bits_minus_aligned[offset_slot] -=
-				LZX_NUM_ALIGNED_OFFSET_BITS;
-	}
 
-	return d;
+invalid:
+	return -1;
 }
 
-void
-lzx_free_decompressor(struct lzx_decompressor *d)
+/*
+ * lzx_free_decompressor - Free an LZX decompressor
+ *
+ * @decompressor:       A decompressor that was allocated with
+ *			lzx_allocate_decompressor(), or NULL.
+ */
+void lzx_free_decompressor(struct lzx_decompressor *decompressor)
 {
-	aligned_free(d);
+	kfree(decompressor);
 }
diff --git a/fs/ntfs3/lib/xpress_constants.h b/fs/ntfs3/lib/xpress_constants.h
deleted file mode 100644
index 1e9675a39..000000000
--- a/fs/ntfs3/lib/xpress_constants.h
+++ /dev/null
@@ -1,23 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0-or-later */
-/*
- * xpress_constants.h
- *
- * Constants for the XPRESS compression format.
- */
-
-#ifndef _XPRESS_CONSTANTS_H
-#define _XPRESS_CONSTANTS_H
-
-#define XPRESS_NUM_CHARS	256
-#define XPRESS_NUM_SYMBOLS	512
-#define XPRESS_MAX_CODEWORD_LEN	15
-
-#define XPRESS_END_OF_DATA	256
-
-#define XPRESS_MIN_OFFSET	1
-#define XPRESS_MAX_OFFSET	65535
-
-#define XPRESS_MIN_MATCH_LEN	3
-#define XPRESS_MAX_MATCH_LEN	65538
-
-#endif /* _XPRESS_CONSTANTS_H */
diff --git a/fs/ntfs3/lib/xpress_decompress.c b/fs/ntfs3/lib/xpress_decompress.c
index ab3f3c0b0..3d98f36a9 100644
--- a/fs/ntfs3/lib/xpress_decompress.c
+++ b/fs/ntfs3/lib/xpress_decompress.c
@@ -1,13 +1,10 @@
 // SPDX-License-Identifier: GPL-2.0-or-later
 /*
- * xpress_decompress.c
+ * xpress_decompress.c - A decompressor for the XPRESS compression format
+ * (Huffman variant), which can be used in "System Compressed" files.  This is
+ * based on the code from wimlib.
  *
- * A decompressor for the XPRESS compression format (Huffman variant).
- */
-
-/*
- *
- * Copyright (C) 2012-2016 Eric Biggers
+ * Copyright (C) 2015 Eric Biggers
  *
  * This program is free software: you can redistribute it and/or modify it under
  * the terms of the GNU General Public License as published by the Free Software
@@ -23,82 +20,71 @@
  * this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
-
-/*
- * The XPRESS compression format is an LZ77 and Huffman-code based algorithm.
- * That means it is fairly similar to LZX compression, but XPRESS is simpler, so
- * it is a little faster to compress and decompress.
- *
- * The XPRESS compression format is mostly documented in a file called "[MS-XCA]
- * Xpress Compression Algorithm".  In the MSDN library, it can currently be
- * found under Open Specifications => Protocols => Windows Protocols => Windows
- * Server Protocols => [MS-XCA] Xpress Compression Algorithm".  The format in
- * WIMs is specifically the algorithm labeled as the "LZ77+Huffman Algorithm"
- * (there apparently are some other versions of XPRESS as well).
- *
- * If you are already familiar with the LZ77 algorithm and Huffman coding, the
- * XPRESS format is fairly simple.  The compressed data begins with 256 bytes
- * that contain 512 4-bit integers that are the lengths of the symbols in the
- * Huffman code used for match/literal headers.  In contrast with more
- * complicated formats such as DEFLATE and LZX, this is the only Huffman code
- * that is used for the entirety of the XPRESS compressed data, and the codeword
- * lengths are not encoded with a pretree.
- *
- * The rest of the compressed data is Huffman-encoded symbols.	Values 0 through
- * 255 represent the corresponding literal bytes.  Values 256 through 511
- * represent matches and may require extra bits or bytes to be read to get the
- * match offset and match length.
- *
- * The trickiest part is probably the way in which literal bytes for match
- * lengths are interleaved in the bitstream.
- *
- * Also, a caveat--- according to Microsoft's documentation for XPRESS,
- *
- *	"Some implementation of the decompression algorithm expect an extra
- *	symbol to mark the end of the data.  Specifically, some implementations
- *	fail during decompression if the Huffman symbol 256 is not found after
- *	the actual data."
- *
- * This is the case with Microsoft's implementation in WIMGAPI, for example.  So
- * although our implementation doesn't currently check for this extra symbol,
- * compressors would be wise to add it.
- */
-
 #include "decompress_common.h"
-#include "xpress_constants.h"
 #include "lib.h"
 
+#define XPRESS_NUM_SYMBOLS	512
+#define XPRESS_MAX_CODEWORD_LEN	15
+#define XPRESS_MIN_MATCH_LEN	3
+
 /* This value is chosen for fast decompression.  */
-#define XPRESS_TABLEBITS 11
+#define XPRESS_TABLEBITS 12
 
+/* Reusable heap-allocated memory for XPRESS decompression  */
 struct xpress_decompressor {
-	union {
-		DECODE_TABLE(decode_table, XPRESS_NUM_SYMBOLS,
-			     XPRESS_TABLEBITS, XPRESS_MAX_CODEWORD_LEN);
-		u8 lens[XPRESS_NUM_SYMBOLS];
-	};
-	DECODE_TABLE_WORKING_SPACE(working_space, XPRESS_NUM_SYMBOLS,
-				   XPRESS_MAX_CODEWORD_LEN);
-} _aligned_attribute(DECODE_TABLE_ALIGNMENT);
-
-int
-xpress_decompress(struct xpress_decompressor *__restrict d,
-		  const void *__restrict compressed_data, size_t compressed_size,
-		  void *__restrict uncompressed_data, size_t uncompressed_size)
+
+	/* The Huffman decoding table  */
+	u16 decode_table[(1 << XPRESS_TABLEBITS) + 2 * XPRESS_NUM_SYMBOLS];
+
+	/* An array that maps symbols to codeword lengths  */
+	u8 lens[XPRESS_NUM_SYMBOLS];
+
+	/* Temporary space for make_huffman_decode_table()  */
+	u16 working_space[2 * (1 + XPRESS_MAX_CODEWORD_LEN) +
+			  XPRESS_NUM_SYMBOLS];
+};
+
+/*
+ * xpress_allocate_decompressor - Allocate an XPRESS decompressor
+ *
+ * Return the pointer to the decompressor on success, or return NULL and set
+ * errno on failure.
+ */
+struct xpress_decompressor *xpress_allocate_decompressor(void)
+{
+	return kmalloc(sizeof(struct xpress_decompressor), GFP_NOFS);
+}
+
+/*
+ * xpress_decompress - Decompress a buffer of XPRESS-compressed data
+ *
+ * @decompressor:       A decompressor that was allocated with
+ *			xpress_allocate_decompressor()
+ * @compressed_data:	The buffer of data to decompress
+ * @compressed_size:	Number of bytes of compressed data
+ * @uncompressed_data:	The buffer in which to store the decompressed data
+ * @uncompressed_size:	The number of bytes the data decompresses into
+ *
+ * Return 0 on success, or return -1 and set errno on failure.
+ */
+int xpress_decompress(struct xpress_decompressor *decompressor,
+		      const void *compressed_data, size_t compressed_size,
+		      void *uncompressed_data, size_t uncompressed_size)
 {
+	struct xpress_decompressor *d = decompressor;
 	const u8 * const in_begin = compressed_data;
 	u8 * const out_begin = uncompressed_data;
 	u8 *out_next = out_begin;
 	u8 * const out_end = out_begin + uncompressed_size;
 	struct input_bitstream is;
-	int i;
+	u32 i;
 
 	/* Read the Huffman codeword lengths.  */
 	if (compressed_size < XPRESS_NUM_SYMBOLS / 2)
-		return -1;
+		goto invalid;
 	for (i = 0; i < XPRESS_NUM_SYMBOLS / 2; i++) {
-		d->lens[2 * i + 0] = in_begin[i] & 0xf;
-		d->lens[2 * i + 1] = in_begin[i] >> 4;
+		d->lens[i*2 + 0] = in_begin[i] & 0xF;
+		d->lens[i*2 + 1] = in_begin[i] >> 4;
 	}
 
 	/* Build a decoding table for the Huffman code.  */
@@ -106,7 +92,7 @@ xpress_decompress(struct xpress_decompressor *__restrict d,
 				      XPRESS_TABLEBITS, d->lens,
 				      XPRESS_MAX_CODEWORD_LEN,
 				      d->working_space))
-		return -1;
+		goto invalid;
 
 	/* Decode the matches and literals.  */
 
@@ -121,7 +107,7 @@ xpress_decompress(struct xpress_decompressor *__restrict d,
 
 		sym = read_huffsym(&is, d->decode_table,
 				   XPRESS_TABLEBITS, XPRESS_MAX_CODEWORD_LEN);
-		if (sym < XPRESS_NUM_CHARS) {
+		if (sym < 256) {
 			/* Literal  */
 			*out_next++ = sym;
 		} else {
@@ -141,26 +127,29 @@ xpress_decompress(struct xpress_decompressor *__restrict d,
 			}
 			length += XPRESS_MIN_MATCH_LEN;
 
-			if (unlikely(lz_copy(length, offset,
-					     out_begin, out_next, out_end,
-					     XPRESS_MIN_MATCH_LEN)))
-				return -1;
+			if (offset > (size_t)(out_next - out_begin))
+				goto invalid;
 
-			out_next += length;
+			if (length > (size_t)(out_end - out_next))
+				goto invalid;
+
+			out_next = lz_copy(out_next, length, offset, out_end,
+					   XPRESS_MIN_MATCH_LEN);
 		}
 	}
 	return 0;
-}
 
-struct xpress_decompressor *
-xpress_allocate_decompressor(void)
-{
-	return aligned_malloc(sizeof(struct xpress_decompressor),
-			      DECODE_TABLE_ALIGNMENT);
+invalid:
+	return -1;
 }
 
-void
-xpress_free_decompressor(struct xpress_decompressor *d)
+/*
+ * xpress_free_decompressor - Free an XPRESS decompressor
+ *
+ * @decompressor:       A decompressor that was allocated with
+ *			xpress_allocate_decompressor(), or NULL.
+ */
+void xpress_free_decompressor(struct xpress_decompressor *decompressor)
 {
-	aligned_free(d);
+	kfree(decompressor);
 }
diff --git a/fs/ntfs3/ntfs_fs.h b/fs/ntfs3/ntfs_fs.h
index 443b005d0..6bdd2d35a 100644
--- a/fs/ntfs3/ntfs_fs.h
+++ b/fs/ntfs3/ntfs_fs.h
@@ -259,14 +259,12 @@ struct ntfs_sb_info {
 	} objid;
 
 	struct {
-		/*
-		 * protect 'lznt/xpress/lzx'
-		 * Should we use different spinlocks for each ctx?
-		 */
-		spinlock_t lock;
+		struct mutex mtx_lznt;
 		struct lznt *lznt;
 #ifdef CONFIG_NTFS3_LZX_XPRESS
+		struct mutex mtx_xpress;
 		struct xpress_decompressor *xpress;
+		struct mutex mtx_lzx;
 		struct lzx_decompressor *lzx;
 #endif
 	} compress;
@@ -310,7 +308,7 @@ struct ntfs_inode {
 	 * Range [i_valid - inode->i_size) - contains 0
 	 * Usually i_valid <= inode->i_size
 	 */
-	loff_t i_valid;
+	u64 i_valid;
 	struct timespec64 i_crtime;
 
 	struct mutex ni_lock;
@@ -399,6 +397,8 @@ int attr_is_frame_compressed(struct ntfs_inode *ni, struct ATTRIB *attr,
 			     CLST frame, CLST *clst_data);
 int attr_allocate_frame(struct ntfs_inode *ni, CLST frame, size_t compr_size,
 			u64 new_valid);
+int attr_collapse_range(struct ntfs_inode *ni, u64 vbo, u64 bytes);
+int attr_punch_hole(struct ntfs_inode *ni, u64 vbo, u64 bytes);
 
 /* functions from attrlist.c*/
 void al_destroy(struct ntfs_inode *ni);
@@ -448,7 +448,6 @@ int ntfs_getattr(const struct path *path, struct kstat *stat, u32 request_mask,
 void ntfs_sparse_cluster(struct inode *inode, struct page *page0, CLST vcn,
 			 CLST len);
 int ntfs_file_fsync(struct file *filp, loff_t start, loff_t end, int datasync);
-void ntfs_truncate_blocks(struct inode *inode, loff_t offset);
 int ntfs3_setattr(struct dentry *dentry, struct iattr *attr);
 int ntfs_file_open(struct inode *inode, struct file *file);
 int ntfs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
@@ -471,7 +470,8 @@ struct ATTRIB *ni_find_attr(struct ntfs_inode *ni, struct ATTRIB *attr,
 			    u8 name_len, const CLST *vcn,
 			    struct mft_inode **mi);
 struct ATTRIB *ni_enum_attr_ex(struct ntfs_inode *ni, struct ATTRIB *attr,
-			       struct ATTR_LIST_ENTRY **le);
+			       struct ATTR_LIST_ENTRY **le,
+			       struct mft_inode **mi);
 struct ATTRIB *ni_load_attr(struct ntfs_inode *ni, enum ATTR_TYPE type,
 			    const __le16 *name, u8 name_len, CLST vcn,
 			    struct mft_inode **pmi);
@@ -499,7 +499,6 @@ struct ATTR_FILE_NAME *ni_fname_name(struct ntfs_inode *ni,
 				     struct ATTR_LIST_ENTRY **entry);
 struct ATTR_FILE_NAME *ni_fname_type(struct ntfs_inode *ni, u8 name_type,
 				     struct ATTR_LIST_ENTRY **entry);
-u16 ni_fnames_count(struct ntfs_inode *ni);
 int ni_new_attr_flags(struct ntfs_inode *ni, enum FILE_ATTRIBUTE new_fa);
 enum REPARSE_SIGN ni_parse_reparse(struct ntfs_inode *ni, struct ATTRIB *attr,
 				   void *buffer);
@@ -696,6 +695,7 @@ void run_truncate_around(struct runs_tree *run, CLST vcn);
 bool run_lookup(const struct runs_tree *run, CLST vcn, size_t *Index);
 bool run_add_entry(struct runs_tree *run, CLST vcn, CLST lcn, CLST len,
 		   bool is_mft);
+bool run_collapse_range(struct runs_tree *run, CLST vcn, CLST len);
 bool run_get_entry(const struct runs_tree *run, size_t index, CLST *vcn,
 		   CLST *lcn, CLST *len);
 bool run_is_mapped_full(const struct runs_tree *run, CLST svcn, CLST evcn);
@@ -727,13 +727,11 @@ static inline size_t wnd_zeroes(const struct wnd_bitmap *wnd)
 {
 	return wnd->total_zeroes;
 }
-void wnd_trace(struct wnd_bitmap *wnd);
-void wnd_trace_tree(struct wnd_bitmap *wnd, u32 nExtents, const char *Hint);
-int wnd_init(struct wnd_bitmap *wnd, struct super_block *sb, size_t nBits);
-int wnd_set_free(struct wnd_bitmap *wnd, size_t FirstBit, size_t Bits);
-int wnd_set_used(struct wnd_bitmap *wnd, size_t FirstBit, size_t Bits);
-bool wnd_is_free(struct wnd_bitmap *wnd, size_t FirstBit, size_t Bits);
-bool wnd_is_used(struct wnd_bitmap *wnd, size_t FirstBit, size_t Bits);
+int wnd_init(struct wnd_bitmap *wnd, struct super_block *sb, size_t nbits);
+int wnd_set_free(struct wnd_bitmap *wnd, size_t bit, size_t bits);
+int wnd_set_used(struct wnd_bitmap *wnd, size_t bit, size_t bits);
+bool wnd_is_free(struct wnd_bitmap *wnd, size_t bit, size_t bits);
+bool wnd_is_used(struct wnd_bitmap *wnd, size_t bit, size_t bits);
 
 /* Possible values for 'flags' 'wnd_find' */
 #define BITMAP_FIND_MARK_AS_USED 0x01
@@ -751,12 +749,18 @@ int ntfs_cmp_names_cpu(const struct cpu_str *uni1, const struct le_str *uni2,
 		       const u16 *upcase);
 
 /* globals from xattr.c */
+#ifdef CONFIG_NTFS3_FS_POSIX_ACL
 struct posix_acl *ntfs_get_acl(struct inode *inode, int type);
 int ntfs_set_acl(struct inode *inode, struct posix_acl *acl, int type);
+int ntfs_init_acl(struct inode *inode, struct inode *dir);
+#else
+#define ntfs_get_acl NULL
+#define ntfs_set_acl NULL
+#endif
+
 int ntfs_acl_chmod(struct inode *inode);
 int ntfs_permission(struct inode *inode, int mask);
 ssize_t ntfs_listxattr(struct dentry *dentry, char *buffer, size_t size);
-int ntfs_init_acl(struct inode *inode, struct inode *dir);
 extern const struct xattr_handler *ntfs_xattr_handlers[];
 
 /* globals from lznt.c */
@@ -1044,4 +1048,3 @@ static inline void le64_sub_cpu(__le64 *var, u64 val)
 {
 	*var = cpu_to_le64(le64_to_cpu(*var) - val);
 }
-
diff --git a/fs/ntfs3/run.c b/fs/ntfs3/run.c
index 32d43ecb5..2b1eeb980 100644
--- a/fs/ntfs3/run.c
+++ b/fs/ntfs3/run.c
@@ -481,6 +481,68 @@ bool run_add_entry(struct runs_tree *run, CLST vcn, CLST lcn, CLST len,
 	return true;
 }
 
+/*helper for attr_collapse_range, which is helper for fallocate(collapse_range)*/
+bool run_collapse_range(struct runs_tree *run, CLST vcn, CLST len)
+{
+	size_t index, eat;
+	struct ntfs_run *r, *e, *eat_start, *eat_end;
+	CLST end;
+
+	if (WARN_ON(!run_lookup(run, vcn, &index)))
+		return true; /* should never be here */
+
+	e = run->runs_ + run->count;
+	r = run->runs_ + index;
+	end = vcn + len;
+
+	if (vcn > r->vcn) {
+		if (r->vcn + r->len <= end) {
+			/* collapse tail of run */
+			r->len = vcn - r->vcn;
+		} else if (r->lcn == SPARSE_LCN) {
+			/* collapse a middle part of sparsed run */
+			r->len -= len;
+		} else {
+			/* collapse a middle part of normal run, split */
+			if (!run_add_entry(run, vcn, SPARSE_LCN, len, false))
+				return false;
+			return run_collapse_range(run, vcn, len);
+		}
+
+		r += 1;
+	}
+
+	eat_start = r;
+	eat_end = r;
+
+	for (; r < e; r++) {
+		CLST d;
+
+		if (r->vcn >= end) {
+			r->vcn -= len;
+			continue;
+		}
+
+		if (r->vcn + r->len <= end) {
+			/* eat this run */
+			eat_end = r + 1;
+			continue;
+		}
+
+		d = end - r->vcn;
+		if (r->lcn != SPARSE_LCN)
+			r->lcn += d;
+		r->len -= d;
+		r->vcn -= len - d;
+	}
+
+	eat = eat_end - eat_start;
+	memmove(eat_start, eat_end, (e - eat_end) * sizeof(*r));
+	run->count -= eat;
+
+	return true;
+}
+
 /*
  * run_get_entry
  *
diff --git a/fs/ntfs3/super.c b/fs/ntfs3/super.c
index 886f00f76..f53c3f770 100644
--- a/fs/ntfs3/super.c
+++ b/fs/ntfs3/super.c
@@ -314,8 +314,13 @@ static noinline int ntfs_parse_options(struct super_block *sb, char *options,
 			opts->nohidden = 1;
 			break;
 		case Opt_acl:
+#ifdef CONFIG_NTFS3_FS_POSIX_ACL
 			sb->s_flags |= SB_POSIXACL;
 			break;
+#else
+			ntfs_err(sb, "support for ACL not compiled in!");
+			return -EINVAL;
+#endif
 		case Opt_noatime:
 			sb->s_flags |= SB_NOATIME;
 			break;
@@ -733,6 +738,7 @@ static int ntfs_init_from_boot(struct super_block *sb, u32 sector_size,
 		goto out;
 	}
 
+	/* cluster size: 512, 1K, 2K, 4K, ... 2M */
 	sct_per_clst = true_sectors_per_clst(boot);
 	if (!is_power_of2(sct_per_clst))
 		goto out;
@@ -946,7 +952,11 @@ static int ntfs_fill_super(struct super_block *sb, void *data, int silent)
 	if (err)
 		goto out;
 
-	spin_lock_init(&sbi->compress.lock);
+	mutex_init(&sbi->compress.mtx_lznt);
+#ifdef CONFIG_NTFS3_LZX_XPRESS
+	mutex_init(&sbi->compress.mtx_xpress);
+	mutex_init(&sbi->compress.mtx_lzx);
+#endif
 
 	/*
 	 * Load $Volume. This should be done before $LogFile
diff --git a/fs/ntfs3/xattr.c b/fs/ntfs3/xattr.c
index 06eafeb04..86f95c166 100644
--- a/fs/ntfs3/xattr.c
+++ b/fs/ntfs3/xattr.c
@@ -472,6 +472,7 @@ static noinline int ntfs_set_ea(struct inode *inode, const char *name,
 	return err;
 }
 
+#ifdef CONFIG_NTFS3_FS_POSIX_ACL
 static inline void ntfs_posix_acl_release(struct posix_acl *acl)
 {
 	if (acl && refcount_dec_and_test(&acl->a_refcount))
@@ -669,15 +670,72 @@ static int ntfs_xattr_set_acl(struct inode *inode, int type, const void *value,
 	return err;
 }
 
+/*
+ * Initialize the ACLs of a new inode. Called from ntfs_create_inode.
+ */
+int ntfs_init_acl(struct inode *inode, struct inode *dir)
+{
+	struct posix_acl *default_acl, *acl;
+	int err;
+
+	/*
+	 * TODO refactoring lock
+	 * ni_lock(dir) ... -> posix_acl_create(dir,...) -> ntfs_get_acl -> ni_lock(dir)
+	 */
+	inode->i_default_acl = NULL;
+
+	default_acl = ntfs_get_acl_ex(dir, ACL_TYPE_DEFAULT, 1);
+
+	if (!default_acl || default_acl == ERR_PTR(-EOPNOTSUPP)) {
+		inode->i_mode &= ~current_umask();
+		err = 0;
+		goto out;
+	}
+
+	if (IS_ERR(default_acl)) {
+		err = PTR_ERR(default_acl);
+		goto out;
+	}
+
+	acl = default_acl;
+	err = __posix_acl_create(&acl, GFP_NOFS, &inode->i_mode);
+	if (err < 0)
+		goto out1;
+	if (!err) {
+		posix_acl_release(acl);
+		acl = NULL;
+	}
+
+	if (!S_ISDIR(inode->i_mode)) {
+		posix_acl_release(default_acl);
+		default_acl = NULL;
+	}
+
+	if (default_acl)
+		err = ntfs_set_acl_ex(inode, default_acl, ACL_TYPE_DEFAULT, 1);
+
+	if (!acl)
+		inode->i_acl = NULL;
+	else if (!err)
+		err = ntfs_set_acl_ex(inode, acl, ACL_TYPE_ACCESS, 1);
+
+	posix_acl_release(acl);
+out1:
+	posix_acl_release(default_acl);
+
+out:
+	return err;
+}
+#endif
+
 /*
  * ntfs_acl_chmod
  *
- * helper for 'ntfs_setattr'
+ * helper for 'ntfs3_setattr'
  */
 int ntfs_acl_chmod(struct inode *inode)
 {
 	struct super_block *sb = inode->i_sb;
-	int err;
 
 	if (!(sb->s_flags & SB_POSIXACL))
 		return 0;
@@ -685,9 +743,7 @@ int ntfs_acl_chmod(struct inode *inode)
 	if (S_ISLNK(inode->i_mode))
 		return -EOPNOTSUPP;
 
-	err = posix_acl_chmod(inode, inode->i_mode);
-
-	return err;
+	return posix_acl_chmod(inode, inode->i_mode);
 }
 
 /*
@@ -697,18 +753,12 @@ int ntfs_acl_chmod(struct inode *inode)
  */
 int ntfs_permission(struct inode *inode, int mask)
 {
-	struct super_block *sb = inode->i_sb;
-	struct ntfs_sb_info *sbi = sb->s_fs_info;
-	int err;
-
-	if (sbi->options.no_acs_rules) {
+	if (ntfs_sb(inode->i_sb)->options.no_acs_rules) {
 		/* "no access rules" mode - allow all changes */
 		return 0;
 	}
 
-	err = generic_permission(inode, mask);
-
-	return err;
+	return generic_permission(inode, mask);
 }
 
 /*
@@ -833,6 +883,7 @@ static int ntfs_getxattr(const struct xattr_handler *handler, struct dentry *de,
 		goto out;
 	}
 
+#ifdef CONFIG_NTFS3_FS_POSIX_ACL
 	if ((name_len == sizeof(XATTR_NAME_POSIX_ACL_ACCESS) - 1 &&
 	     !memcmp(name, XATTR_NAME_POSIX_ACL_ACCESS,
 		     sizeof(XATTR_NAME_POSIX_ACL_ACCESS))) ||
@@ -845,9 +896,11 @@ static int ntfs_getxattr(const struct xattr_handler *handler, struct dentry *de,
 				ACL_TYPE_ACCESS :
 				ACL_TYPE_DEFAULT,
 			buffer, size);
-	} else {
-		err = ntfs_getxattr_hlp(inode, name, buffer, size, NULL);
+		goto out;
 	}
+#endif
+	/* deal with ntfs extended attribute */
+	err = ntfs_getxattr_hlp(inode, name, buffer, size, NULL);
 
 out:
 	return err;
@@ -978,6 +1031,7 @@ static noinline int ntfs_setxattr(const struct xattr_handler *handler,
 		goto out;
 	}
 
+#ifdef CONFIG_NTFS3_FS_POSIX_ACL
 	if ((name_len == sizeof(XATTR_NAME_POSIX_ACL_ACCESS) - 1 &&
 	     !memcmp(name, XATTR_NAME_POSIX_ACL_ACCESS,
 		     sizeof(XATTR_NAME_POSIX_ACL_ACCESS))) ||
@@ -990,66 +1044,11 @@ static noinline int ntfs_setxattr(const struct xattr_handler *handler,
 				ACL_TYPE_ACCESS :
 				ACL_TYPE_DEFAULT,
 			value, size);
-	} else {
-		err = ntfs_set_ea(inode, name, value, size, flags, 0);
-	}
-
-out:
-	return err;
-}
-
-/*
- * Initialize the ACLs of a new inode. Called from ntfs_create_inode.
- */
-int ntfs_init_acl(struct inode *inode, struct inode *dir)
-{
-	struct posix_acl *default_acl, *acl;
-	int err;
-
-	/*
-	 * TODO refactoring lock
-	 * ni_lock(dir) ... -> posix_acl_create(dir,...) -> ntfs_get_acl -> ni_lock(dir)
-	 */
-	inode->i_default_acl = NULL;
-
-	default_acl = ntfs_get_acl_ex(dir, ACL_TYPE_DEFAULT, 1);
-
-	if (!default_acl || default_acl == ERR_PTR(-EOPNOTSUPP)) {
-		inode->i_mode &= ~current_umask();
-		err = 0;
 		goto out;
 	}
-
-	if (IS_ERR(default_acl)) {
-		err = PTR_ERR(default_acl);
-		goto out;
-	}
-
-	acl = default_acl;
-	err = __posix_acl_create(&acl, GFP_NOFS, &inode->i_mode);
-	if (err < 0)
-		goto out1;
-	if (!err) {
-		posix_acl_release(acl);
-		acl = NULL;
-	}
-
-	if (!S_ISDIR(inode->i_mode)) {
-		posix_acl_release(default_acl);
-		default_acl = NULL;
-	}
-
-	if (default_acl)
-		err = ntfs_set_acl_ex(inode, default_acl, ACL_TYPE_DEFAULT, 1);
-
-	if (!acl)
-		inode->i_acl = NULL;
-	else if (!err)
-		err = ntfs_set_acl_ex(inode, acl, ACL_TYPE_ACCESS, 1);
-
-	posix_acl_release(acl);
-out1:
-	posix_acl_release(default_acl);
+#endif
+	/* deal with ntfs extended attribute */
+	err = ntfs_set_ea(inode, name, value, size, flags, 0);
 
 out:
 	return err;
-- 
2.30.0.rc0

