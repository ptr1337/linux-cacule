From cf5af7eedb9571674058a9fa0ec6759744b9cc8f Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 17 Jul 2019 23:40:43 +0800
Subject: [PATCH 15/53] bmq: Refine sg_balance_check() code path.

---
 kernel/sched/bmq.c | 43 +++++++++++++++++--------------------------
 1 file changed, 17 insertions(+), 26 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index aa2e067142fe..face8875d58a 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -2501,9 +2501,7 @@ static inline int active_load_balance_cpu_stop(void *data)
 	raw_spin_lock(&rq->lock);
 
 	rq->active_balance = 0;
-	/*
-	 * _something_ may have changed the task, double check again
-	 */
+	/* _something_ may have changed the task, double check again */
 	if (task_on_rq_queued(p) && task_rq(p) == rq &&
 	    (cpu = cpumask_any_and(p->cpus_ptr, &sched_rq_watermark[0])) < nr_cpu_ids)
 		rq = __migrate_task(rq, p, cpu);
@@ -2517,41 +2515,34 @@ static inline int active_load_balance_cpu_stop(void *data)
 }
 
 /* sg_balance_trigger - trigger slibing group balance for @cpu */
-static inline int sg_balance_trigger(const int cpu)
+static inline int sg_balance_trigger(const int cpu, struct rq *rq)
 {
-	struct rq *rq = cpu_rq(cpu);
 	unsigned long flags;
 	struct task_struct *curr;
+	int res;
 
 	if (!raw_spin_trylock_irqsave(&rq->lock, flags))
 		return 0;
 	curr = rq->curr;
-	if (!is_idle_task(curr) &&
-	    1 == rq->nr_running &&
-	    cpumask_intersects(curr->cpus_ptr, &sched_rq_watermark[0])) {
-		int active_balance = 0;
-
-		if (likely(!rq->active_balance)) {
-			rq->active_balance = 1;
-			active_balance = 1;
-		}
+	res = (!is_idle_task(curr)) && (1 == rq->nr_running) &&\
+	      cpumask_intersects(curr->cpus_ptr, &sched_rq_watermark[0]) &&\
+	      (!rq->active_balance);
 
-		raw_spin_unlock_irqrestore(&rq->lock, flags);
+	if (res)
+		rq->active_balance = 1;
 
-		if (likely(active_balance)) {
-			stop_one_cpu_nowait(cpu, active_load_balance_cpu_stop,
-					    curr, &rq->active_balance_work);
-			return 1;
-		}
-	} else
-		raw_spin_unlock_irqrestore(&rq->lock, flags);
-	return 0;
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+
+	if (res)
+		stop_one_cpu_nowait(cpu, active_load_balance_cpu_stop,
+				    curr, &rq->active_balance_work);
+	return res;
 }
 
 /*
  * sg_balance_check - slibing group balance check for run queue @rq
  */
-static inline void sg_balance_check(const struct rq *rq)
+static inline void sg_balance_check(struct rq *rq)
 {
 	cpumask_t chk;
 	int cpu;
@@ -2576,7 +2567,7 @@ static inline void sg_balance_check(const struct rq *rq)
 				if (cpumask_intersects(cpu_smt_mask(i),
 						       &sched_rq_pending_mask))
 					continue;
-				if (sg_balance_trigger(i))
+				if (sg_balance_trigger(i, cpu_rq(i)))
 					return;
 				if (tried)
 					return;
@@ -2592,7 +2583,7 @@ static inline void sg_balance_check(const struct rq *rq)
 	if (cpumask_andnot(&chk, cpu_smt_mask(cpu), &sched_rq_pending_mask) &&
 	    cpumask_andnot(&chk, &chk, &sched_rq_watermark[IDLE_WM]) &&
 	    cpumask_equal(&chk, cpu_smt_mask(cpu)))
-		sg_balance_trigger(cpu);
+		sg_balance_trigger(cpu, rq);
 }
 #endif /* CONFIG_SCHED_SMT */
 
-- 
2.24.0.155.gd9f6f3b619

