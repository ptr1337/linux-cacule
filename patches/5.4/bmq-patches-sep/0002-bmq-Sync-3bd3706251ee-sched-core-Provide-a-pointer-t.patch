From d592717965daf2c0d468c06bbc27c284432db658 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 20 Aug 2019 00:25:05 +0800
Subject: [PATCH 02/53] bmq: [Sync] 3bd3706251ee sched/core: Provide a pointer
 to the valid CPU mask

---
 kernel/sched/bmq.c | 26 +++++++++++++-------------
 1 file changed, 13 insertions(+), 13 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 269fcb1f653a..de38519c563b 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -1010,7 +1010,7 @@ static inline bool is_per_cpu_kthread(struct task_struct *p)
  */
 static inline bool is_cpu_allowed(struct task_struct *p, int cpu)
 {
-	if (!cpumask_test_cpu(cpu, &p->cpus_allowed))
+	if (!cpumask_test_cpu(cpu, p->cpus_ptr))
 		return false;
 
 	if (is_per_cpu_kthread(p))
@@ -1121,7 +1121,7 @@ static int migration_cpu_stop(void *data)
 static inline void
 set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask)
 {
-	cpumask_copy(&p->cpus_allowed, new_mask);
+	cpumask_copy(&p->cpus_mask, new_mask);
 	p->nr_cpus_allowed = cpumask_weight(new_mask);
 }
 
@@ -1279,7 +1279,7 @@ void kick_process(struct task_struct *p)
 EXPORT_SYMBOL_GPL(kick_process);
 
 /*
- * ->cpus_allowed is protected by both rq->lock and p->pi_lock
+ * ->cpus_ptr is protected by both rq->lock and p->pi_lock
  *
  * A few notes on cpu_active vs cpu_online:
  *
@@ -1319,14 +1319,14 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 		for_each_cpu(dest_cpu, nodemask) {
 			if (!cpu_active(dest_cpu))
 				continue;
-			if (cpumask_test_cpu(dest_cpu, &p->cpus_allowed))
+			if (cpumask_test_cpu(dest_cpu, p->cpus_ptr))
 				return dest_cpu;
 		}
 	}
 
 	for (;;) {
 		/* Any allowed, online CPU? */
-		for_each_cpu(dest_cpu, &p->cpus_allowed) {
+		for_each_cpu(dest_cpu, p->cpus_ptr) {
 			if (!is_cpu_allowed(p, dest_cpu))
 				continue;
 			goto out;
@@ -1394,7 +1394,7 @@ static inline int select_task_rq(struct task_struct *p)
 	cpumask_t chk_mask, tmp;
 	unsigned long preempt_level, level;
 
-	if (unlikely(!cpumask_and(&chk_mask, &p->cpus_allowed, cpu_online_mask)))
+	if (unlikely(!cpumask_and(&chk_mask, p->cpus_ptr, cpu_online_mask)))
 		return select_fallback_rq(task_cpu(p), p);
 
 	preempt_level = SCHED_PRIO2WATERMARK(task_sched_prio(p));
@@ -1922,7 +1922,7 @@ void wake_up_new_task(struct task_struct *p)
 #ifdef CONFIG_SMP
 	/*
 	 * Fork balancing, do it here and not earlier because:
-	 * - cpus_allowed can change in the fork path
+	 * - cpus_ptr can change in the fork path
 	 * - any previously selected CPU might disappear through hotplug
 	 * Use __set_task_cpu() to avoid calling sched_class::migrate_task_rq,
 	 * as we're not fully set-up yet.
@@ -2479,7 +2479,7 @@ static inline int active_load_balance_cpu_stop(void *data)
 	 * _something_ may have changed the task, double check again
 	 */
 	if (task_on_rq_queued(p) && task_rq(p) == rq &&
-	    (cpu = cpumask_any_and(&p->cpus_allowed, &sched_rq_watermark[0])) < nr_cpu_ids)
+	    (cpu = cpumask_any_and(p->cpus_ptr, &sched_rq_watermark[0])) < nr_cpu_ids)
 		rq = __migrate_task(rq, p, cpu);
 
 	raw_spin_unlock(&rq->lock);
@@ -2502,7 +2502,7 @@ static inline int sg_balance_trigger(const int cpu)
 	curr = rq->curr;
 	if (!is_idle_task(curr) &&
 	    1 == rq->nr_running &&
-	    cpumask_intersects(&curr->cpus_allowed, &sched_rq_watermark[0])) {
+	    cpumask_intersects(curr->cpus_ptr, &sched_rq_watermark[0])) {
 		int active_balance = 0;
 
 		if (likely(!rq->active_balance)) {
@@ -2812,7 +2812,7 @@ migrate_pending_tasks(struct rq *rq, struct rq *dest_rq)
 		if (task_running(p))
 			continue;
 		next = rq_next_bmq_task(p, rq);
-		if (cpumask_test_cpu(dest_cpu, &p->cpus_allowed)) {
+		if (cpumask_test_cpu(dest_cpu, p->cpus_ptr)) {
 			dequeue_task(p, rq, 0);
 			set_task_cpu(p, dest_cpu);
 			enqueue_task(p, dest_rq, 0);
@@ -3655,7 +3655,7 @@ static int __set_cpus_allowed_ptr(struct task_struct *p,
 		goto out;
 	}
 
-	if (cpumask_equal(&p->cpus_allowed, new_mask))
+	if (cpumask_equal(p->cpus_ptr, new_mask))
 		goto out;
 
 	if (!cpumask_intersects(new_mask, cpu_valid_mask)) {
@@ -4465,7 +4465,7 @@ long sched_getaffinity(pid_t pid, cpumask_t *mask)
 		goto out_unlock;
 
 	task_access_lock_irqsave(p, &lock, &flags);
-	cpumask_and(mask, &p->cpus_allowed, cpu_active_mask);
+	cpumask_and(mask, &p->cpus_mask, cpu_active_mask);
 	task_access_unlock_irqrestore(p, lock, &flags);
 
 out_unlock:
@@ -5086,7 +5086,7 @@ int task_can_attach(struct task_struct *p,
 	 * allowed nodes is unnecessary.  Thus, cpusets are not
 	 * applicable for such threads.  This prevents checking for
 	 * success of set_cpus_allowed_ptr() on all attached tasks
-	 * before cpus_allowed may be changed.
+	 * before cpus_mask may be changed.
 	 */
 	if (p->flags & PF_NO_SETAFFINITY)
 		ret = -EINVAL;
-- 
2.24.0.155.gd9f6f3b619

