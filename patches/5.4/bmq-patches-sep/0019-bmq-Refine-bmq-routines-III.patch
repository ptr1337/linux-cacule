From 75469c1bd086e28ef387d3c4152038bddd617ff7 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Thu, 25 Jul 2019 10:38:08 +0800
Subject: [PATCH 19/53] bmq: Refine bmq routines III

---
 kernel/sched/bmq.c | 18 +++++++-----------
 1 file changed, 7 insertions(+), 11 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 9d8cee5a617b..0c00e00b7aca 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -588,7 +588,7 @@ static inline void sched_update_tick_dependency(struct rq *rq) { }
 #endif
 
 /*
- * Removing from the runqueue.
+ * Add/Remove/Requeue task to/from the runqueue routines
  * Context: rq->lock
  */
 static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
@@ -615,10 +615,6 @@ static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 	sched_info_dequeued(rq, p);
 }
 
-/*
- * Adding task to the runqueue.
- * Context: rq->lock
- */
 static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 {
 	lockdep_assert_held(&rq->lock);
@@ -710,9 +706,9 @@ void resched_cpu(int cpu)
 	raw_spin_unlock_irqrestore(&rq->lock, flags);
 }
 
-static inline void check_preempt_curr(struct rq *rq, struct task_struct *p)
+static inline void check_preempt_curr(struct rq *rq)
 {
-	if (MAX_PRIO == rq->curr->prio || rq_first_bmq_task(rq) == p)
+	if (rq_first_bmq_task(rq) != rq->curr)
 		resched_curr(rq);
 }
 
@@ -1023,7 +1019,7 @@ static struct rq *move_queued_task(struct rq *rq, struct task_struct *p, int
 	BUG_ON(task_cpu(p) != new_cpu);
 	enqueue_task(p, rq, 0);
 	p->on_rq = TASK_ON_RQ_QUEUED;
-	check_preempt_curr(rq, p);
+	check_preempt_curr(rq);
 
 	return rq;
 }
@@ -1437,7 +1433,7 @@ static inline void ttwu_queue(struct task_struct *p, int cpu, int wake_flags)
 	raw_spin_lock(&rq->lock);
 	update_rq_clock(rq);
 	ttwu_do_activate(rq, p, wake_flags);
-	check_preempt_curr(rq, p);
+	check_preempt_curr(rq);
 	raw_spin_unlock(&rq->lock);
 }
 
@@ -1932,7 +1928,7 @@ void wake_up_new_task(struct task_struct *p)
 	update_rq_clock(rq);
 	activate_task(p, rq);
 	trace_sched_wakeup_new(p);
-	check_preempt_curr(rq, p);
+	check_preempt_curr(rq);
 
 	raw_spin_unlock(&rq->lock);
 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
@@ -3346,7 +3342,7 @@ static inline void check_task_changed(struct rq *rq, struct task_struct *p)
 	/* Trigger resched if task sched_prio has been modified. */
 	if (task_on_rq_queued(p) && task_sched_prio(p) != p->bmq_idx) {
 		requeue_task(p, rq);
-		check_preempt_curr(rq, p);
+		check_preempt_curr(rq);
 	}
 }
 
-- 
2.24.0.155.gd9f6f3b619

