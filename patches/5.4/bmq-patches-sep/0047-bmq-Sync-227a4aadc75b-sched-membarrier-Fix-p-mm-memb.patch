From 6f2edfc6e808fdb58037ded7ffa8e26f8d44c10b Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 29 Oct 2019 16:50:15 +0800
Subject: [PATCH 47/53] bmq: [Sync] 227a4aadc75b sched/membarrier: Fix
 p->mm->membarrier_state racy load

---
 kernel/sched/bmq.c       |  4 ++--
 kernel/sched/bmq_sched.h | 34 ++++++++++++++++++++++++++++++++++
 2 files changed, 36 insertions(+), 2 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 57a6cb2667bd..b4f2300e6c7c 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -2248,15 +2248,15 @@ context_switch(struct rq *rq, struct task_struct *prev,
 		else
 			prev->active_mm = NULL;
 	} else {                                        // to user
+		membarrier_switch_mm(rq, prev->active_mm, next->mm);
 		/*
 		 * sys_membarrier() requires an smp_mb() between setting
-		 * rq->curr and returning to userspace.
+		 * rq->curr / membarrier_switch_mm() and returning to userspace.
 		 *
 		 * The below provides this either through switch_mm(), or in
 		 * case 'prev->active_mm == next->mm' through
 		 * finish_task_switch()'s mmdrop().
 		 */
-
 		switch_mm_irqs_off(prev->active_mm, next->mm, next);
 
 		if (!prev->mm) {                        // from kernel
diff --git a/kernel/sched/bmq_sched.h b/kernel/sched/bmq_sched.h
index d9ea1288f467..3b5a87748c67 100644
--- a/kernel/sched/bmq_sched.h
+++ b/kernel/sched/bmq_sched.h
@@ -89,6 +89,10 @@ struct rq {
 
 	atomic_t nr_iowait;
 
+#ifdef CONFIG_MEMBARRIER
+	int membarrier_state;
+#endif
+
 #ifdef CONFIG_SMP
 	int cpu;		/* cpu of this runqueue */
 	bool online;
@@ -422,6 +426,36 @@ extern void schedule_idle(void);
  */
 #define SCHED_FLAG_SUGOV	0x10000000
 
+#ifdef CONFIG_MEMBARRIER
+/*
+ * The scheduler provides memory barriers required by membarrier between:
+ * - prior user-space memory accesses and store to rq->membarrier_state,
+ * - store to rq->membarrier_state and following user-space memory accesses.
+ * In the same way it provides those guarantees around store to rq->curr.
+ */
+static inline void membarrier_switch_mm(struct rq *rq,
+					struct mm_struct *prev_mm,
+					struct mm_struct *next_mm)
+{
+	int membarrier_state;
+
+	if (prev_mm == next_mm)
+		return;
+
+	membarrier_state = atomic_read(&next_mm->membarrier_state);
+	if (READ_ONCE(rq->membarrier_state) == membarrier_state)
+		return;
+
+	WRITE_ONCE(rq->membarrier_state, membarrier_state);
+}
+#else
+static inline void membarrier_switch_mm(struct rq *rq,
+					struct mm_struct *prev_mm,
+					struct mm_struct *next_mm)
+{
+}
+#endif
+
 static inline int task_running_nice(struct task_struct *p)
 {
 	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
-- 
2.24.0.155.gd9f6f3b619

