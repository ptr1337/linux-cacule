From 2651ccc8622ff2ddf4ec34a24ad3faef45c17da9 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 24 Jul 2019 13:51:52 +0800
Subject: [PATCH 18/53] bmq: Refine bmq routines II.

---
 kernel/sched/bmq.c | 63 ++++++++++++----------------------------------
 1 file changed, 16 insertions(+), 47 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index a571166e5511..9d8cee5a617b 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -216,10 +216,7 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 
 static inline int task_sched_prio(struct task_struct *p)
 {
-	if (p->prio < MAX_RT_PRIO)
-		return 0;
-
-	return (p->prio - MAX_RT_PRIO + p->boost_prio);
+	return (p->prio < MAX_RT_PRIO)? 0:p->prio - MAX_RT_PRIO + p->boost_prio;
 }
 
 static inline void bmq_init(struct bmq *q)
@@ -278,7 +275,6 @@ rq_next_bmq_task(struct task_struct *p, struct rq *rq)
 	}
 
 	return list_next_entry(p, bmq_node);
-
 }
 
 static inline struct task_struct *rq_runnable_task(struct rq *rq)
@@ -673,28 +669,6 @@ static inline void requeue_task(struct task_struct *p, struct rq *rq)
 	}
 }
 
-static inline int requeue_task_lazy(struct task_struct *p, struct rq *rq)
-{
-	int idx = task_sched_prio(p);
-
-	lockdep_assert_held(&rq->lock);
-	WARN_ONCE(task_rq(p) != rq, "bmq: cpu[%d] requeue task lazy reside on cpu%d\n",
-		  cpu_of(rq), task_cpu(p));
-
-	if (idx == p->bmq_idx)
-		return 0;
-
-	list_del(&p->bmq_node);
-	bmq_add_task(p, &rq->queue, idx);
-	if (list_empty(&rq->queue.heads[p->bmq_idx]))
-		clear_bit(p->bmq_idx, rq->queue.bitmap);
-	p->bmq_idx = idx;
-	set_bit(p->bmq_idx, rq->queue.bitmap);
-	update_sched_rq_watermark(rq);
-
-	return 1;
-}
-
 /*
  * resched_curr - mark rq's current task 'to be rescheduled now'.
  *
@@ -725,16 +699,20 @@ void resched_curr(struct rq *rq)
 		trace_sched_wake_idle_without_ipi(cpu);
 }
 
-static inline void check_preempt_curr(struct rq *rq, struct task_struct *p)
+void resched_cpu(int cpu)
 {
-	struct task_struct *curr = rq->curr;
-
-	if (MAX_PRIO == curr->prio)
-		resched_curr(rq);
+	struct rq *rq = cpu_rq(cpu);
+	unsigned long flags;
 
-	/* ToDo: Don't preempt for IDLE/BATCH policy */
+	raw_spin_lock_irqsave(&rq->lock, flags);
+	if (cpu_online(cpu) || cpu == smp_processor_id())
+		resched_curr(cpu_rq(cpu));
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+}
 
-	if (rq_first_bmq_task(rq) == p)
+static inline void check_preempt_curr(struct rq *rq, struct task_struct *p)
+{
+	if (MAX_PRIO == rq->curr->prio || rq_first_bmq_task(rq) == p)
 		resched_curr(rq);
 }
 
@@ -3365,9 +3343,11 @@ EXPORT_SYMBOL(default_wake_function);
 
 static inline void check_task_changed(struct rq *rq, struct task_struct *p)
 {
-	/* Trigger resched if task priority modified. */
-	if (task_on_rq_queued(p) && requeue_task_lazy(p, rq))
+	/* Trigger resched if task sched_prio has been modified. */
+	if (task_on_rq_queued(p) && task_sched_prio(p) != p->bmq_idx) {
+		requeue_task(p, rq);
 		check_preempt_curr(rq, p);
+	}
 }
 
 #ifdef CONFIG_RT_MUTEXES
@@ -4976,17 +4956,6 @@ void init_idle(struct task_struct *idle, int cpu)
 #endif
 }
 
-void resched_cpu(int cpu)
-{
-	struct rq *rq = cpu_rq(cpu);
-	unsigned long flags;
-
-	raw_spin_lock_irqsave(&rq->lock, flags);
-	if (cpu_online(cpu) || cpu == smp_processor_id())
-		resched_curr(cpu_rq(cpu));
-	raw_spin_unlock_irqrestore(&rq->lock, flags);
-}
-
 static bool __wake_q_add(struct wake_q_head *head, struct task_struct *task)
 {
 	struct wake_q_node *node = &task->wake_q;
-- 
2.24.0.155.gd9f6f3b619

