From 09e089206c4b748494ef173a0c7223d3c7b43ad2 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Sat, 18 Jan 2020 19:31:50 +0100
Subject: [PATCH] LL: Implement ll-branding v5.4

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 fs/dcache.c            | 4 ++++
 include/linux/blkdev.h | 4 ++++
 include/linux/mm.h     | 4 ++++
 init/Kconfig           | 4 ++++
 kernel/sched/bmq.c     | 4 ++++
 kernel/sched/core.c    | 8 ++++++++
 mm/compaction.c        | 4 ++++
 mm/huge_memory.c       | 4 ++++
 mm/page-writeback.c    | 8 ++++++++
 mm/vmscan.c            | 4 ++++
 10 files changed, 48 insertions(+)

diff --git a/fs/dcache.c b/fs/dcache.c
index 00d7e6a08..3cdd6bc00 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -71,7 +71,11 @@
  * If no ancestor relationship:
  * arbitrary, since it's serialized on rename_lock
  */
+#ifdef CONFIG_LL_BRANDING
+int sysctl_vfs_cache_pressure __read_mostly = 50;
+#else
 int sysctl_vfs_cache_pressure __read_mostly = 100;
+#endif
 EXPORT_SYMBOL_GPL(sysctl_vfs_cache_pressure);
 
 __cacheline_aligned_in_smp DEFINE_SEQLOCK(rename_lock);
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index f4a471c67..8820f7939 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -45,7 +45,11 @@ struct blk_queue_stats;
 struct blk_stat_callback;
 
 #define BLKDEV_MIN_RQ	4
+#ifdef CONFIG_LL_BRANDING
+#define BLKDEV_MAX_RQ	512
+#else
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
+#endif
 
 /* Must be consistent with blk_mq_poll_stats_bkt() */
 #define BLK_MQ_POLL_STATS_BKTS 16
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 70e1dccc1..41016a3ae 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2438,7 +2438,11 @@ int __must_check write_one_page(struct page *page);
 void task_dirty_inc(struct task_struct *tsk);
 
 /* readahead.c */
+#ifdef CONFIG_LL_BRANDING
+#define VM_READAHEAD_PAGES	(SZ_2M / PAGE_SIZE)
+#else
 #define VM_READAHEAD_PAGES	(SZ_128K / PAGE_SIZE)
+#endif
 
 int force_page_cache_readahead(struct address_space *mapping, struct file *filp,
 			pgoff_t offset, unsigned long nr_to_read);
diff --git a/init/Kconfig b/init/Kconfig
index 3c29ad2eb..743496000 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -73,6 +73,10 @@ config THREAD_INFO_IN_TASK
 
 menu "General setup"
 
+config LL_BRANDING
+	bool "Add Linux Lucjan branding"
+	default y
+
 config BROKEN
 	bool
 
diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 42a2a5b3d..6ebadf234 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -77,7 +77,11 @@ static inline void print_scheduler_version(void)
  * 1: Deboost and requeue task. (default)
  * 2: Set rq skip task.
  */
+#ifdef CONFIG_LL_BRANDING
+int sched_yield_type __read_mostly = 2;
+#else
 int sched_yield_type __read_mostly = 1;
+#endif
 
 #define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
 #define boost_threshold(p)	(SCHED_TIMESLICE_NS >>\
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 8dacda4b0..744ecd440 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -56,7 +56,11 @@ const_debug unsigned int sysctl_sched_features =
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
  */
+#ifdef CONFIG_LL_BRANDING
+const_debug unsigned int sysctl_sched_nr_migrate = 256;
+#else
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
+#endif
 
 /*
  * period over which we measure -rt task CPU usage in us.
@@ -70,7 +74,11 @@ __read_mostly int scheduler_running;
  * part of the period that we allow rt tasks to run in us.
  * default: 0.95s
  */
+#ifdef CONFIG_LL_BRANDING
+int sysctl_sched_rt_runtime = 980000;
+#else
 int sysctl_sched_rt_runtime = 950000;
+#endif
 
 /*
  * __task_rq_lock - lock the rq @p resides on.
diff --git a/mm/compaction.c b/mm/compaction.c
index 672d3c78c..41c5f1d3c 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -1590,7 +1590,11 @@ typedef enum {
  * Allow userspace to control policy on scanning the unevictable LRU for
  * compactable pages.
  */
+#ifdef CONFIG_LL_BRANDING
+int sysctl_compact_unevictable_allowed __read_mostly = 0;
+#else
 int sysctl_compact_unevictable_allowed __read_mostly = 1;
+#endif
 
 static inline void
 update_fast_start_pfn(struct compact_control *cc, unsigned long pfn)
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 13cc93785..e1c2a4ced 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -53,7 +53,11 @@ unsigned long transparent_hugepage_flags __read_mostly =
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE_MADVISE
 	(1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG)|
 #endif
+#ifdef CONFIG_LL_BRANDING
+	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_OR_MADV_FLAG)|
+#else
 	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG)|
+#endif
 	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG)|
 	(1<<TRANSPARENT_HUGEPAGE_USE_ZERO_PAGE_FLAG);
 
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index 50055d2e4..1dc9ced72 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -71,7 +71,11 @@ static long ratelimit_pages = 32;
 /*
  * Start background writeback (via writeback threads) at this percentage
  */
+#ifdef CONFIG_LL_BRANDING
+int dirty_background_ratio = 20;
+#else
 int dirty_background_ratio = 10;
+#endif
 
 /*
  * dirty_background_bytes starts at 0 (disabled) so that it is a function of
@@ -88,7 +92,11 @@ int vm_highmem_is_dirtyable;
 /*
  * The generator of dirty data starts writeback at this percentage
  */
+#ifdef CONFIG_LL_BRANDING
+int vm_dirty_ratio = 50;
+#else
 int vm_dirty_ratio = 20;
+#endif
 
 /*
  * vm_dirty_bytes starts at 0 (disabled) so that it is a function of
diff --git a/mm/vmscan.c b/mm/vmscan.c
index e7f10c4b4..76891af3e 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -164,7 +164,11 @@ struct scan_control {
 /*
  * From 0 .. 100.  Higher means more swappy.
  */
+#ifdef CONFIG_LL_BRANDING
+int vm_swappiness = 30;
+#else
 int vm_swappiness = 60;
+#endif
 /*
  * The total number of pages which are beyond the high watermark within all
  * zones.
-- 
2.25.0.2.g232378479e.dirty

