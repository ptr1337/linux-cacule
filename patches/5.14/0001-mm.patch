diff --git a/Documentation/admin-guide/sysctl/vm.rst b/Documentation/admin-guide/sysctl/vm.rst
index 003d5cc3751b37fe730c27ab911155a00a33973d..cd86faf77ded829eec4e8f575bf48c5a81f54bed 100644
--- a/Documentation/admin-guide/sysctl/vm.rst
+++ b/Documentation/admin-guide/sysctl/vm.rst
@@ -68,6 +68,10 @@ Currently, these files are in /proc/sys/vm:
 - stat_refresh
 - numa_stat
 - swappiness
+- unevictable_file_kbytes_low
+- unevictable_file_kbytes_min
+- unevictable_anon_kbytes_low
+- unevictable_anon_kbytes_min
 - unprivileged_userfaultfd
 - user_reserve_kbytes
 - vfs_cache_pressure
@@ -118,7 +122,8 @@ compaction_proactiveness
 
 This tunable takes a value in the range [0, 100] with a default value of
 20. This tunable determines how aggressively compaction is done in the
-background. Setting it to 0 disables proactive compaction.
+background. Write of non zero value to this tunable will immediately
+trigger the proactive compaction. Setting it to 0 disables proactive compaction.
 
 Note that compaction has a non-trivial system-wide impact as pages
 belonging to different processes are moved around, which could also lead
@@ -880,6 +885,56 @@ calls without any restrictions.
 The default value is 0.
 
 
+unevictable_file_kbytes_low
+===========================
+
+Keep some file pages still mapped under memory pressure to avoid potential
+disk thrashing that may occur due to evicting running executables code.
+This implements soft eviction throttling, and some file pages can still
+be discarded.
+
+Setting it to 0 effectively disables this feature.
+
+The default value is 256 MiB.
+
+
+unevictable_file_kbytes_min
+===========================
+
+Keep all file pages still mapped under memory pressure to avoid potential
+disk thrashing that may occur due to evicting running executables code.
+This is the hard limit.
+
+Setting it to 0 effectively disables this feature.
+
+The default value is 128 MiB.
+
+
+unevictable_anon_kbytes_low
+===========================
+
+Keep some anonymous pages still mapped under memory pressure to avoid
+potential stalls that may occur due to swapping.
+This implements soft swapping throttling, and some anonymous pages can
+still be swapped out.
+
+Setting it to 0 effectively disables this feature.
+
+The default value is 64 MiB.
+
+
+unevictable_anon_kbytes_min
+===========================
+
+Keep all anonymous pages still mapped under memory pressure to avoid
+potential stalls that may occur due to swapping.
+This is the hard limit.
+
+Setting it to 0 effectively disables this feature.
+
+The default value is 32 MiB.
+
+
 user_reserve_kbytes
 ===================
 
diff --git a/include/linux/compaction.h b/include/linux/compaction.h
index c24098c7accaca3a7a15df1f97664e68952348d9..34bce35c808dde309e94c47e605fd1c13e07ab0b 100644
--- a/include/linux/compaction.h
+++ b/include/linux/compaction.h
@@ -84,6 +84,8 @@ static inline unsigned long compact_gap(unsigned int order)
 extern unsigned int sysctl_compaction_proactiveness;
 extern int sysctl_compaction_handler(struct ctl_table *table, int write,
 			void *buffer, size_t *length, loff_t *ppos);
+extern int compaction_proactiveness_sysctl_handler(struct ctl_table *table,
+		int write, void *buffer, size_t *length, loff_t *ppos);
 extern int sysctl_extfrag_threshold;
 extern int sysctl_compact_unevictable_allowed;
 
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fcb535560028fc59a82a4b1e56c6db728fe5bf37..fe96425dc53dbb45312fb66eaa390079b6452eb0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -846,6 +846,7 @@ typedef struct pglist_data {
 	enum zone_type kcompactd_highest_zoneidx;
 	wait_queue_head_t kcompactd_wait;
 	struct task_struct *kcompactd;
+	bool proactive_compact_trigger;
 #endif
 	/*
 	 * This is a per-node reserve of pages that are not available
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 760e8a2d815de89630f7d4287b04b95b3d0ac89d..1f7487a42c48e65de701b0f86e1321f7a010216b 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -116,6 +116,38 @@
 static int sixty = 60;
 #endif
 
+#if defined(CONFIG_UNEVICTABLE_FILE)
+#if CONFIG_UNEVICTABLE_FILE_KBYTES_LOW < 0
+#error "CONFIG_UNEVICTABLE_FILE_KBYTES_LOW should be >= 0"
+#endif
+#if CONFIG_UNEVICTABLE_FILE_KBYTES_MIN < 0
+#error "CONFIG_UNEVICTABLE_FILE_KBYTES_MIN should be >= 0"
+#endif
+#if CONFIG_UNEVICTABLE_FILE_KBYTES_LOW < CONFIG_UNEVICTABLE_FILE_KBYTES_MIN
+#error "CONFIG_UNEVICTABLE_FILE_KBYTES_LOW should be >= CONFIG_UNEVICTABLE_FILE_KBYTES_MIN"
+#endif
+unsigned long sysctl_unevictable_file_kbytes_low __read_mostly =
+	CONFIG_UNEVICTABLE_FILE_KBYTES_LOW;
+unsigned long sysctl_unevictable_file_kbytes_min __read_mostly =
+	CONFIG_UNEVICTABLE_FILE_KBYTES_MIN;
+#endif
+
+#if defined(CONFIG_UNEVICTABLE_ANON)
+#if CONFIG_UNEVICTABLE_ANON_KBYTES_LOW < 0
+#error "CONFIG_UNEVICTABLE_ANON_KBYTES_LOW should be >= 0"
+#endif
+#if CONFIG_UNEVICTABLE_ANON_KBYTES_MIN < 0
+#error "CONFIG_UNEVICTABLE_ANON_KBYTES_MIN should be >= 0"
+#endif
+#if CONFIG_UNEVICTABLE_ANON_KBYTES_LOW < CONFIG_UNEVICTABLE_ANON_KBYTES_MIN
+#error "CONFIG_UNEVICTABLE_ANON_KBYTES_LOW should be >= CONFIG_UNEVICTABLE_ANON_KBYTES_MIN"
+#endif
+unsigned long sysctl_unevictable_anon_kbytes_low __read_mostly =
+	CONFIG_UNEVICTABLE_ANON_KBYTES_LOW;
+unsigned long sysctl_unevictable_anon_kbytes_min __read_mostly =
+	CONFIG_UNEVICTABLE_ANON_KBYTES_MIN;
+#endif
+
 static int __maybe_unused neg_one = -1;
 static int __maybe_unused two = 2;
 static int __maybe_unused four = 4;
@@ -2883,7 +2915,7 @@ static struct ctl_table vm_table[] = {
 		.data		= &sysctl_compaction_proactiveness,
 		.maxlen		= sizeof(sysctl_compaction_proactiveness),
 		.mode		= 0644,
-		.proc_handler	= proc_dointvec_minmax,
+		.proc_handler	= compaction_proactiveness_sysctl_handler,
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= &one_hundred,
 	},
@@ -3101,6 +3133,44 @@ static struct ctl_table vm_table[] = {
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= SYSCTL_ONE,
 	},
+#endif
+#if defined(CONFIG_UNEVICTABLE_FILE)
+	{
+		.procname	= "unevictable_file_kbytes_low",
+		.data		= &sysctl_unevictable_file_kbytes_low,
+		.maxlen		= sizeof(sysctl_unevictable_file_kbytes_low),
+		.mode		= 0644,
+		.proc_handler	= proc_doulongvec_minmax,
+		.extra1		= &sysctl_unevictable_file_kbytes_min,
+	},
+	{
+		.procname	= "unevictable_file_kbytes_min",
+		.data		= &sysctl_unevictable_file_kbytes_min,
+		.maxlen		= sizeof(sysctl_unevictable_file_kbytes_min),
+		.mode		= 0644,
+		.proc_handler	= proc_doulongvec_minmax,
+		.extra1		= &zero_ul,
+		.extra2		= &sysctl_unevictable_file_kbytes_low,
+	},
+#endif
+#if defined(CONFIG_UNEVICTABLE_ANON)
+	{
+		.procname	= "unevictable_anon_kbytes_low",
+		.data		= &sysctl_unevictable_anon_kbytes_low,
+		.maxlen		= sizeof(sysctl_unevictable_anon_kbytes_low),
+		.mode		= 0644,
+		.proc_handler	= proc_doulongvec_minmax,
+		.extra1		= &sysctl_unevictable_anon_kbytes_min,
+	},
+	{
+		.procname	= "unevictable_anon_kbytes_min",
+		.data		= &sysctl_unevictable_anon_kbytes_min,
+		.maxlen		= sizeof(sysctl_unevictable_anon_kbytes_min),
+		.mode		= 0644,
+		.proc_handler	= proc_doulongvec_minmax,
+		.extra1		= &zero_ul,
+		.extra2		= &sysctl_unevictable_anon_kbytes_low,
+	},
 #endif
 	{
 		.procname	= "user_reserve_kbytes",
diff --git a/mm/Kconfig b/mm/Kconfig
index 40a9bfcd5062e1d06313125bdec31d7c4f403553..90a46d8fcadaf7ade075835bb8b4ddc5505f4ab3 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -47,6 +47,79 @@ config SPARSEMEM_MANUAL
 
 endchoice
 
+config UNEVICTABLE_FILE
+	bool "Keep some file pages under memory pressure"
+	depends on SYSCTL
+	def_bool y
+	help
+	  Keep some file pages still mapped under memory pressure
+	  to avoid potential disk thrashing that may occur due to
+	  evicting running executables code.
+
+	  The UNEVICTABLE_FILE_KBYTES_LOW value defines a threshold
+	  to activate file pages eviction throttling.
+	  The vm.unevictable_file_kbytes_low sysctl knob is used to
+	  change the amount in the runtime (setting it to 0
+	  effectively disables this feature).
+
+	  Recommended value: 262144 for a typical desktop workload.
+
+	  The UNEVICTABLE_FILE_KBYTES_MIN value sets the amount of
+	  pages to keep as a hard limit.
+	  The vm.unevictable_file_kbytes_min sysctl knob is used to
+	  change the amount in the runtime (setting it to 0
+	  effectively disables this feature).
+
+	  Recommended value: 131072 for a typical desktop workload.
+
+	  See also: Documentation/admin-guide/sysctl/vm.rst
+
+config UNEVICTABLE_FILE_KBYTES_LOW
+	int "Default value for vm.unevictable_file_kbytes_low"
+	depends on UNEVICTABLE_FILE
+	default "262144"
+
+config UNEVICTABLE_FILE_KBYTES_MIN
+	int "Default value for vm.unevictable_file_kbytes_min"
+	depends on UNEVICTABLE_FILE
+	default "131072"
+
+config UNEVICTABLE_ANON
+	bool "Keep some anonymous pages under memory pressure"
+	depends on SYSCTL
+	def_bool y
+	help
+	  Keep some anonymous pages still mapped under memory pressure
+	  to avoid potential stalls that may occur due to swapping.
+
+	  The UNEVICTABLE_ANON_KBYTES_LOW value defines a threshold
+	  to throttle swapping of anonymous pages.
+	  The vm.unevictable_anon_kbytes_low sysctl knob is used to
+	  change the amount in the runtime (setting it to 0
+	  effectively disables this feature).
+
+	  Recommended value: 65536 for a typical desktop workload.
+
+	  The UNEVICTABLE_ANON_KBYTES_MIN value sets the amount of
+	  pages to keep as a hard limit.
+	  The vm.unevictable_anon_kbytes_min sysctl knob is used to
+	  change the amount in the runtime (setting it to 0
+	  effectively disables this feature).
+
+	  Recommended value: 32768 for a typical desktop workload.
+
+	  See also: Documentation/admin-guide/sysctl/vm.rst
+
+config UNEVICTABLE_ANON_KBYTES_LOW
+	int "Default value for vm.unevictable_anon_kbytes_low"
+	depends on UNEVICTABLE_ANON
+	default "65536"
+
+config UNEVICTABLE_ANON_KBYTES_MIN
+	int "Default value for vm.unevictable_anon_kbytes_min"
+	depends on UNEVICTABLE_ANON
+	default "32768"
+
 config SPARSEMEM
 	def_bool y
 	depends on (!SELECT_MEMORY_MODEL && ARCH_SPARSEMEM_ENABLE) || SPARSEMEM_MANUAL
diff --git a/mm/compaction.c b/mm/compaction.c
index 621508e0ecd5da3c4b82ea8d69832375c1bb3e50..38f17270cdaf732e20948b61f516dfe81db89de1 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -2706,6 +2706,30 @@ static void compact_nodes(void)
  */
 unsigned int __read_mostly sysctl_compaction_proactiveness = 20;
 
+int compaction_proactiveness_sysctl_handler(struct ctl_table *table, int write,
+		void *buffer, size_t *length, loff_t *ppos)
+{
+	int rc, nid;
+
+	rc = proc_dointvec_minmax(table, write, buffer, length, ppos);
+	if (rc)
+		return rc;
+
+	if (write && sysctl_compaction_proactiveness) {
+		for_each_online_node(nid) {
+			pg_data_t *pgdat = NODE_DATA(nid);
+
+			if (pgdat->proactive_compact_trigger)
+				continue;
+
+			pgdat->proactive_compact_trigger = true;
+			wake_up_interruptible(&pgdat->kcompactd_wait);
+		}
+	}
+
+	return 0;
+}
+
 /*
  * This is the entry point for compacting all nodes via
  * /proc/sys/vm/compact_memory
@@ -2750,7 +2774,8 @@ void compaction_unregister_node(struct node *node)
 
 static inline bool kcompactd_work_requested(pg_data_t *pgdat)
 {
-	return pgdat->kcompactd_max_order > 0 || kthread_should_stop();
+	return pgdat->kcompactd_max_order > 0 || kthread_should_stop() ||
+		pgdat->proactive_compact_trigger;
 }
 
 static bool kcompactd_node_suitable(pg_data_t *pgdat)
@@ -2885,7 +2910,8 @@ static int kcompactd(void *p)
 {
 	pg_data_t *pgdat = (pg_data_t *)p;
 	struct task_struct *tsk = current;
-	unsigned int proactive_defer = 0;
+	long default_timeout = msecs_to_jiffies(HPAGE_FRAG_CHECK_INTERVAL_MSEC);
+	long timeout = default_timeout;
 
 	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);
 
@@ -2900,25 +2926,39 @@ static int kcompactd(void *p)
 	while (!kthread_should_stop()) {
 		unsigned long pflags;
 
+		/*
+		 * Avoid the unnecessary wakeup for proactive compaction
+		 * when it is disabled.
+		 */
+		if (!sysctl_compaction_proactiveness)
+			timeout = MAX_SCHEDULE_TIMEOUT;
 		trace_mm_compaction_kcompactd_sleep(pgdat->node_id);
 		if (wait_event_freezable_timeout(pgdat->kcompactd_wait,
-			kcompactd_work_requested(pgdat),
-			msecs_to_jiffies(HPAGE_FRAG_CHECK_INTERVAL_MSEC))) {
+			kcompactd_work_requested(pgdat), timeout) &&
+			!pgdat->proactive_compact_trigger) {
 
 			psi_memstall_enter(&pflags);
 			kcompactd_do_work(pgdat);
 			psi_memstall_leave(&pflags);
+			/*
+			 * Reset the timeout value. The defer timeout from
+			 * proactive compaction is lost here but that is fine
+			 * as the condition of the zone changing substantionally
+			 * then carrying on with the previous defer interval is
+			 * not useful.
+			 */
+			timeout = default_timeout;
 			continue;
 		}
 
-		/* kcompactd wait timeout */
+		/*
+		 * Start the proactive work with default timeout. Based
+		 * on the fragmentation score, this timeout is updated.
+		 */
+		timeout = default_timeout;
 		if (should_proactive_compact_node(pgdat)) {
 			unsigned int prev_score, score;
 
-			if (proactive_defer) {
-				proactive_defer--;
-				continue;
-			}
 			prev_score = fragmentation_score_node(pgdat);
 			proactive_compact_node(pgdat);
 			score = fragmentation_score_node(pgdat);
@@ -2926,9 +2966,12 @@ static int kcompactd(void *p)
 			 * Defer proactive compaction if the fragmentation
 			 * score did not go down i.e. no progress made.
 			 */
-			proactive_defer = score < prev_score ?
-					0 : 1 << COMPACT_MAX_DEFER_SHIFT;
+			if (unlikely(score >= prev_score))
+				timeout =
+				   default_timeout << COMPACT_MAX_DEFER_SHIFT;
 		}
+		if (unlikely(pgdat->proactive_compact_trigger))
+			pgdat->proactive_compact_trigger = false;
 	}
 
 	return 0;
diff --git a/mm/vmscan.c b/mm/vmscan.c
index eeae2f6bc53203926678fb515ec6d1a96b2d6b3f..fd3856e44b52a485c0d8192fe8821a8c47e7be34 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -118,9 +118,25 @@ struct scan_control {
 	/* There is easily reclaimable cold cache in the current node */
 	unsigned int cache_trim_mode:1;
 
+#if defined(CONFIG_UNEVICTABLE_FILE)
+	/* The file pages on the current node are low */
+	unsigned int file_is_low:1;
+
+	/* The file pages on the current node are minimal */
+	unsigned int file_is_min:1;
+#endif
+
 	/* The file pages on the current node are dangerously low */
 	unsigned int file_is_tiny:1;
 
+#if defined(CONFIG_UNEVICTABLE_ANON)
+	/* The anonymous pages on the current node are low */
+	unsigned int anon_is_low:1;
+
+	/* The anonymous pages on the current node are minimal */
+	unsigned int anon_is_min:1;
+#endif
+
 	/* Allocation order */
 	s8 order;
 
@@ -167,6 +183,16 @@ struct scan_control {
 #define prefetchw_prev_lru_page(_page, _base, _field) do { } while (0)
 #endif
 
+#if defined(CONFIG_UNEVICTABLE_FILE)
+extern unsigned long sysctl_unevictable_file_kbytes_low;
+extern unsigned long sysctl_unevictable_file_kbytes_min;
+#endif
+
+#if defined(CONFIG_UNEVICTABLE_ANON)
+extern unsigned long sysctl_unevictable_anon_kbytes_low;
+extern unsigned long sysctl_unevictable_anon_kbytes_min;
+#endif
+
 /*
  * From 0 .. 200.  Higher means more swappy.
  */
@@ -2641,6 +2667,23 @@ static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,
 			BUG();
 		}
 
+#if defined(CONFIG_UNEVICTABLE_FILE)
+		if (file && scan) {
+			if (sc->file_is_low)
+				scan = min(scan, SWAP_CLUSTER_MAX >> sc->priority);
+			else if (sc->file_is_min)
+				scan = 0;
+		}
+#endif
+#if defined(CONFIG_UNEVICTABLE_ANON)
+		if (!file && scan) {
+			if (sc->anon_is_low)
+				scan = min(scan, SWAP_CLUSTER_MAX >> sc->priority);
+			else if (sc->anon_is_min)
+				scan = 0;
+		}
+#endif
+
 		nr[lru] = scan;
 	}
 }
@@ -2887,6 +2930,10 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)
 	} while ((memcg = mem_cgroup_iter(target_memcg, memcg, NULL)));
 }
 
+#if defined(CONFIG_UNEVICTABLE_FILE) || defined(CONFIG_UNEVICTABLE_ANON)
+#define K(x) ((x) << (PAGE_SHIFT - 10))
+#endif
+
 static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 {
 	struct reclaim_state *reclaim_state = current->reclaim_state;
@@ -2964,11 +3011,26 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 	if (!cgroup_reclaim(sc)) {
 		unsigned long total_high_wmark = 0;
 		unsigned long free, anon;
+#if defined(CONFIG_UNEVICTABLE_FILE)
+		unsigned long reclaimable_file, clean_file, dirty_file;
+#endif
+#if defined(CONFIG_UNEVICTABLE_ANON)
+		unsigned long reclaimable_anon;
+#endif
 		int z;
 
 		free = sum_zone_node_page_state(pgdat->node_id, NR_FREE_PAGES);
 		file = node_page_state(pgdat, NR_ACTIVE_FILE) +
 			   node_page_state(pgdat, NR_INACTIVE_FILE);
+#if defined(CONFIG_UNEVICTABLE_FILE)
+		reclaimable_file = file + node_page_state(pgdat, NR_ISOLATED_FILE);
+		dirty_file = node_page_state(pgdat, NR_FILE_DIRTY);
+#endif
+#if defined(CONFIG_UNEVICTABLE_ANON)
+		reclaimable_anon = node_page_state(pgdat, NR_ACTIVE_ANON) +
+				   node_page_state(pgdat, NR_INACTIVE_ANON) +
+				   node_page_state(pgdat, NR_ISOLATED_ANON);
+#endif
 
 		for (z = 0; z < MAX_NR_ZONES; z++) {
 			struct zone *zone = &pgdat->node_zones[z];
@@ -2989,6 +3051,32 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 			file + free <= total_high_wmark &&
 			!(sc->may_deactivate & DEACTIVATE_ANON) &&
 			anon >> sc->priority;
+
+#if defined(CONFIG_UNEVICTABLE_FILE)
+		/*
+		 * node_page_state() sum can go out of sync since
+		 * all the values are not read at once
+		 */
+		if (unlikely(reclaimable_file < dirty_file))
+			/*
+			 * in this case assume the system does not have
+			 * clean file pages anymore
+			 */
+			clean_file = 0;
+		else
+			clean_file = reclaimable_file - dirty_file;
+
+		sc->file_is_low = K(clean_file) < sysctl_unevictable_file_kbytes_low &&
+			          K(clean_file) > sysctl_unevictable_file_kbytes_min;
+
+		sc->file_is_min = K(clean_file) <= sysctl_unevictable_file_kbytes_min;
+#endif
+#if defined(CONFIG_UNEVICTABLE_ANON)
+		sc->anon_is_low = K(reclaimable_anon) < sysctl_unevictable_anon_kbytes_low &&
+				  K(reclaimable_anon) > sysctl_unevictable_anon_kbytes_min;
+
+		sc->anon_is_min = K(reclaimable_anon) <= sysctl_unevictable_anon_kbytes_min;
+#endif
 	}
 
 	shrink_node_memcgs(pgdat, sc);
