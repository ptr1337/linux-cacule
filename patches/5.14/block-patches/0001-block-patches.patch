From 89a681f5b22cf7ec831910d865e45a3338d32f80 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Thu, 2 Jul 2020 19:16:43 +0200
Subject: [PATCH 1/5] block, Kconfig.iosched: set default value of IOSCHED_BFQ
 to yes

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 block/Kconfig.iosched | 3 +--
 1 file changed, 1 insertion(+), 2 deletions(-)

diff --git a/block/Kconfig.iosched b/block/Kconfig.iosched
index 2f2158e05..e58b2953a 100644
--- a/block/Kconfig.iosched
+++ b/block/Kconfig.iosched
@@ -5,13 +5,11 @@ menu "IO Schedulers"
 
 config MQ_IOSCHED_DEADLINE
 	tristate "MQ deadline I/O scheduler"
-	default y
 	help
 	  MQ version of the deadline IO scheduler.
 
 config MQ_IOSCHED_KYBER
 	tristate "Kyber I/O scheduler"
-	default y
 	help
 	  The Kyber I/O scheduler is a low-overhead scheduler suitable for
 	  multiqueue and other fast devices. Given target latencies for reads and
@@ -20,6 +18,7 @@ config MQ_IOSCHED_KYBER
 
 config IOSCHED_BFQ
 	tristate "BFQ I/O scheduler"
+	default y
 	help
 	BFQ I/O scheduler for BLK-MQ. BFQ distributes the bandwidth of
 	of the device among all processes according to their weights,
-- 
2.33.0


From 053d6949b42617aecca77e8289cb0095eed343c9 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Thu, 2 Jul 2020 19:26:24 +0200
Subject: [PATCH 2/5] block: Fix depends for BLK_DEV_ZONED

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 block/Kconfig | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/block/Kconfig b/block/Kconfig
index fd732aede..6a9e172c1 100644
--- a/block/Kconfig
+++ b/block/Kconfig
@@ -83,7 +83,7 @@ config BLK_DEV_INTEGRITY_T10
 
 config BLK_DEV_ZONED
 	bool "Zoned block device support"
-	select MQ_IOSCHED_DEADLINE
+	select IOSCHED_BFQ
 	help
 	Block layer zoned block device support. This option enables
 	support for ZAC/ZBC/ZNS host-managed and host-aware zoned block
-- 
2.33.0


From 2a89dc82aaf42578f6075f5ca89ddb789803187a Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Thu, 22 Oct 2020 23:54:33 +0200
Subject: [PATCH 3/5] block: set rq_affinity = 2 for full multithreading I/O

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 include/linux/blkdev.h | 1 +
 1 file changed, 1 insertion(+)

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d3afea47a..85c501fe7 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -606,6 +606,7 @@ struct request_queue {
 
 #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_SAME_COMP) |		\
+				 (1 << QUEUE_FLAG_SAME_FORCE)	|	\
 				 (1 << QUEUE_FLAG_NOWAIT))
 
 void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
-- 
2.33.0


From 3ca40e7d02e9abe160be9df005b3d2e41e6883b9 Mon Sep 17 00:00:00 2001
From: Tom Saeger <tom.saeger () oracle ! com>
Date: Fri, 26 Mar 2021 03:04:57 +0000
Subject: [PATCH 4/5] block: fix trivial typos in comments

s/Additonal/Additional/
s/assocaited/associated/
s/assocaited/associated/
s/assocating/associating/
s/becasue/because/
s/configred/configured/
s/deactive/deactivate/
s/followings/following/
s/funtion/function/
s/heirarchy/hierarchy/
s/intiailized/initialized/
s/prefered/preferred/
s/readded/read/
s/Secion/Section/
s/soley/solely/

Cc: trivial@kernel.org
Signed-off-by: Tom Saeger <tom.saeger@oracle.com>
---
 block/bfq-iosched.c       |  4 ++--
 block/blk-cgroup-rwstat.c |  2 +-
 block/blk-cgroup.c        |  6 +++---
 block/blk-core.c          |  2 +-
 block/blk-iocost.c        | 10 +++++-----
 block/blk-iolatency.c     |  4 ++--
 block/blk-merge.c         |  6 +++---
 block/blk-mq.c            |  2 +-
 block/blk-settings.c      |  2 +-
 block/blk-stat.h          |  2 +-
 block/blk.h               |  2 +-
 block/kyber-iosched.c     |  2 +-
 block/opal_proto.h        |  4 ++--
 block/partitions/ibm.c    |  2 +-
 block/partitions/sun.c    |  2 +-
 block/scsi_ioctl.c        |  2 +-
 16 files changed, 27 insertions(+), 27 deletions(-)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index 727955918..cb50b4c71 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -1850,7 +1850,7 @@ static void bfq_bfqq_handle_idle_busy_switch(struct bfq_data *bfqd,
 	 * As for throughput, we ask bfq_better_to_idle() whether we
 	 * still need to plug I/O dispatching. If bfq_better_to_idle()
 	 * says no, then plugging is not needed any longer, either to
-	 * boost throughput or to perserve service guarantees. Then
+	 * boost throughput or to preserve service guarantees. Then
 	 * the best option is to stop plugging I/O, as not doing so
 	 * would certainly lower throughput. We may end up in this
 	 * case if: (1) upon a dispatch attempt, we detected that it
@@ -5120,7 +5120,7 @@ void bfq_put_queue(struct bfq_queue *bfqq)
 		 * by the fact that bfqq has just been merged.
 		 * 2) burst_size is greater than 0, to handle
 		 * unbalanced decrements. Unbalanced decrements may
-		 * happen in te following case: bfqq is inserted into
+		 * happen in the following case: bfqq is inserted into
 		 * the current burst list--without incrementing
 		 * bust_size--because of a split, but the current
 		 * burst list is not the burst list bfqq belonged to
diff --git a/block/blk-cgroup-rwstat.c b/block/blk-cgroup-rwstat.c
index 3304e841d..0039e4756 100644
--- a/block/blk-cgroup-rwstat.c
+++ b/block/blk-cgroup-rwstat.c
@@ -37,7 +37,7 @@ EXPORT_SYMBOL_GPL(blkg_rwstat_exit);
  * @pd: policy private data of interest
  * @rwstat: rwstat to print
  *
- * Print @rwstat to @sf for the device assocaited with @pd.
+ * Print @rwstat to @sf for the device associated with @pd.
  */
 u64 __blkg_prfill_rwstat(struct seq_file *sf, struct blkg_policy_data *pd,
 			 const struct blkg_rwstat_sample *rwstat)
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index 31fe9be17..d589f002b 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -146,7 +146,7 @@ static void blkg_async_bio_workfn(struct work_struct *work)
  * @q: request_queue the new blkg is associated with
  * @gfp_mask: allocation mask to use
  *
- * Allocate a new blkg assocating @blkcg and @q.
+ * Allocate a new blkg associating @blkcg and @q.
  */
 static struct blkcg_gq *blkg_alloc(struct blkcg *blkcg, struct request_queue *q,
 				   gfp_t gfp_mask)
@@ -542,7 +542,7 @@ EXPORT_SYMBOL_GPL(blkcg_print_blkgs);
  * @pd: policy private data of interest
  * @v: value to print
  *
- * Print @v to @sf for the device assocaited with @pd.
+ * Print @v to @sf for the device associated with @pd.
  */
 u64 __blkg_prfill_u64(struct seq_file *sf, struct blkg_policy_data *pd, u64 v)
 {
@@ -731,7 +731,7 @@ EXPORT_SYMBOL_GPL(blkg_conf_prep);
 
 /**
  * blkg_conf_finish - finish up per-blkg config update
- * @ctx: blkg_conf_ctx intiailized by blkg_conf_prep()
+ * @ctx: blkg_conf_ctx initialized by blkg_conf_prep()
  *
  * Finish up after per-blkg config update.  This function must be paired
  * with blkg_conf_prep().
diff --git a/block/blk-core.c b/block/blk-core.c
index 4f8449b29..feb26c457 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -1031,7 +1031,7 @@ blk_qc_t submit_bio_noacct(struct bio *bio)
 	/*
 	 * We only want one ->submit_bio to be active at a time, else stack
 	 * usage with stacked devices could be a problem.  Use current->bio_list
-	 * to collect a list of requests submited by a ->submit_bio method while
+	 * to collect a list of requests submitted by a ->submit_bio method while
 	 * it is active, and then process them after it returned.
 	 */
 	if (current->bio_list) {
diff --git a/block/blk-iocost.c b/block/blk-iocost.c
index 0e56557ca..4a23a53c4 100644
--- a/block/blk-iocost.c
+++ b/block/blk-iocost.c
@@ -111,7 +111,7 @@
  * busy signal.
  *
  * As devices can have deep queues and be unfair in how the queued commands
- * are executed, soley depending on rq wait may not result in satisfactory
+ * are executed, solely depending on rq wait may not result in satisfactory
  * control quality.  For a better control quality, completion latency QoS
  * parameters can be configured so that the device is considered saturated
  * if N'th percentile completion latency rises above the set point.
@@ -851,7 +851,7 @@ static int ioc_autop_idx(struct ioc *ioc)
 }
 
 /*
- * Take the followings as input
+ * Take the following as input
  *
  *  @bps	maximum sequential throughput
  *  @seqiops	maximum sequential 4k iops
@@ -1150,7 +1150,7 @@ static void current_hweight(struct ioc_gq *iocg, u32 *hw_activep, u32 *hw_inusep
 	u32 hwa, hwi;
 	int ioc_gen;
 
-	/* hot path - if uptodate, use cached */
+	/* hot path - if up-to-date, use cached */
 	ioc_gen = atomic_read(&ioc->hweight_gen);
 	if (ioc_gen == iocg->hweight_gen)
 		goto out;
@@ -1247,7 +1247,7 @@ static bool iocg_activate(struct ioc_gq *iocg, struct ioc_now *now)
 
 	/*
 	 * If seem to be already active, just update the stamp to tell the
-	 * timer that we're still active.  We don't mind occassional races.
+	 * timer that we're still active.  We don't mind occasional races.
 	 */
 	if (!list_empty(&iocg->active_list)) {
 		ioc_now(ioc, now);
@@ -2129,7 +2129,7 @@ static void ioc_forgive_debts(struct ioc *ioc, u64 usage_us_sum, int nr_debtors,
 }
 
 /*
- * Check the active iocgs' state to avoid oversleeping and deactive
+ * Check the active iocgs' state to avoid oversleeping and deactivate
  * idle iocgs.
  *
  * Since waiters determine the sleep durations based on the vrate
diff --git a/block/blk-iolatency.c b/block/blk-iolatency.c
index d8b0d8bd1..73e79559b 100644
--- a/block/blk-iolatency.c
+++ b/block/blk-iolatency.c
@@ -18,7 +18,7 @@
  * every configured node, and each configured node has it's own independent
  * queue depth.  This means that we only care about our latency targets at the
  * peer level.  Some group at the bottom of the hierarchy isn't going to affect
- * a group at the end of some other path if we're only configred at leaf level.
+ * a group at the end of some other path if we're only configured at leaf level.
  *
  * Consider the following
  *
@@ -34,7 +34,7 @@
  * throttle "unloved", but nobody else.
  *
  * In this example "fast", "slow", and "normal" will be the only groups actually
- * accounting their io latencies.  We have to walk up the heirarchy to the root
+ * accounting their io latencies.  We have to walk up the hierarchy to the root
  * on every submit and complete so we can do the appropriate stat recording and
  * adjust the queue depth of ourselves if needed.
  *
diff --git a/block/blk-merge.c b/block/blk-merge.c
index a11b3b537..246ebd28d 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -283,7 +283,7 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	/*
 	 * Bio splitting may cause subtle trouble such as hang when doing sync
 	 * iopoll in direct IO routine. Given performance gain of iopoll for
-	 * big IO can be trival, disable iopoll when split needed.
+	 * big IO can be trivial, disable iopoll when split needed.
 	 */
 	bio->bi_opf &= ~REQ_HIPRI;
 
@@ -341,7 +341,7 @@ void __blk_queue_split(struct bio **bio, unsigned int *nr_segs)
 	}
 
 	if (split) {
-		/* there isn't chance to merge the splitted bio */
+		/* there isn't chance to merge the split bio */
 		split->bi_opf |= REQ_NOMERGE;
 
 		bio_chain(split, *bio);
@@ -686,7 +686,7 @@ void blk_rq_set_mixed_merge(struct request *rq)
 	/*
 	 * @rq will no longer represent mixable attributes for all the
 	 * contained bios.  It will just track those of the first one.
-	 * Distributes the attributs to each bio.
+	 * Distributes the attributes to each bio.
 	 */
 	for (bio = rq->bio; bio; bio = bio->bi_next) {
 		WARN_ON_ONCE((bio->bi_opf & REQ_FAILFAST_MASK) &&
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 9d4fdc2be..181b2c5ac 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -1626,7 +1626,7 @@ static bool blk_mq_has_sqsched(struct request_queue *q)
 }
 
 /*
- * Return prefered queue to dispatch from (if any) for non-mq aware IO
+ * Return preferred queue to dispatch from (if any) for non-mq aware IO
  * scheduler.
  */
 static struct blk_mq_hw_ctx *blk_mq_get_sq_hctx(struct request_queue *q)
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 902c40d67..a2430c2cf 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -721,7 +721,7 @@ void blk_queue_virt_boundary(struct request_queue *q, unsigned long mask)
 	/*
 	 * Devices that require a virtual boundary do not support scatter/gather
 	 * I/O natively, but instead require a descriptor list entry for each
-	 * page (which might not be idential to the Linux PAGE_SIZE).  Because
+	 * page (which might not be identical to the Linux PAGE_SIZE).  Because
 	 * of that they are not limited by our notion of "segment size".
 	 */
 	if (mask)
diff --git a/block/blk-stat.h b/block/blk-stat.h
index 17b47a86e..0541f7228 100644
--- a/block/blk-stat.h
+++ b/block/blk-stat.h
@@ -105,7 +105,7 @@ void blk_stat_add_callback(struct request_queue *q,
  * @cb: The callback.
  *
  * When this returns, the callback is not running on any CPUs and will not be
- * called again unless readded.
+ * called again unless read.
  */
 void blk_stat_remove_callback(struct request_queue *q,
 			      struct blk_stat_callback *cb);
diff --git a/block/blk.h b/block/blk.h
index cb01429c1..be7dfc7cf 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -248,7 +248,7 @@ static inline void req_set_nomerge(struct request_queue *q, struct request *req)
 }
 
 /*
- * The max size one bio can handle is UINT_MAX becasue bvec_iter.bi_size
+ * The max size one bio can handle is UINT_MAX because bvec_iter.bi_size
  * is defined as 'unsigned int', meantime it has to aligned to with logical
  * block size which is the minimum accepted unit by hardware.
  */
diff --git a/block/kyber-iosched.c b/block/kyber-iosched.c
index 15a8be572..07f575c24 100644
--- a/block/kyber-iosched.c
+++ b/block/kyber-iosched.c
@@ -142,7 +142,7 @@ struct kyber_cpu_latency {
  */
 struct kyber_ctx_queue {
 	/*
-	 * Used to ensure operations on rq_list and kcq_map to be an atmoic one.
+	 * Used to ensure operations on rq_list and kcq_map to be an atomic one.
 	 * Also protect the rqs on rq_list when merge.
 	 */
 	spinlock_t lock;
diff --git a/block/opal_proto.h b/block/opal_proto.h
index b486b3ec7..20190c39f 100644
--- a/block/opal_proto.h
+++ b/block/opal_proto.h
@@ -212,7 +212,7 @@ enum opal_parameter {
 
 /* Packets derived from:
  * TCG_Storage_Architecture_Core_Spec_v2.01_r1.00
- * Secion: 3.2.3 ComPackets, Packets & Subpackets
+ * Section: 3.2.3 ComPackets, Packets & Subpackets
  */
 
 /* Comm Packet (header) for transmissions. */
@@ -405,7 +405,7 @@ struct d0_single_user_mode {
 };
 
 /*
- * Additonal Datastores feature
+ * Additional Datastores feature
  *
  * code == 0x0202
  */
diff --git a/block/partitions/ibm.c b/block/partitions/ibm.c
index 4b044e620..f0b51bf89 100644
--- a/block/partitions/ibm.c
+++ b/block/partitions/ibm.c
@@ -212,7 +212,7 @@ static int find_lnx1_partitions(struct parsed_partitions *state,
 		size = label->lnx.formatted_blocks * secperblk;
 	} else {
 		/*
-		 * Formated w/o large volume support. If the sanity check
+		 * Formatted w/o large volume support. If the sanity check
 		 * 'size based on geo == size based on i_size' is true, then
 		 * we can safely assume that we know the formatted size of
 		 * the disk, otherwise we need additional information
diff --git a/block/partitions/sun.c b/block/partitions/sun.c
index 47dc53ecc..c67b4d145 100644
--- a/block/partitions/sun.c
+++ b/block/partitions/sun.c
@@ -101,7 +101,7 @@ int sun_partition(struct parsed_partitions *state)
 
 	/*
 	 * So that old Linux-Sun partitions continue to work,
-	 * alow the VTOC to be used under the additional condition ...
+	 * allow the VTOC to be used under the additional condition ...
 	 */
 	use_vtoc = use_vtoc || !(label->vtoc.sanity ||
 				 label->vtoc.version || label->vtoc.nparts);
diff --git a/block/scsi_ioctl.c b/block/scsi_ioctl.c
index d247431a6..e3fae9d5d 100644
--- a/block/scsi_ioctl.c
+++ b/block/scsi_ioctl.c
@@ -459,7 +459,7 @@ int sg_scsi_ioctl(struct request_queue *q, struct gendisk *disk, fmode_t mode,
 	if (err)
 		goto error;
 
-	/* default.  possible overriden later */
+	/* default.  possible overridden later */
 	req->retries = 5;
 
 	switch (opcode) {
-- 
2.33.0


From c8ea37e277ecd719b43552a9f211afd1efdade4f Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Mon, 28 Jun 2021 13:12:54 +0200
Subject: [PATCH 5/5] block: Add CONFIG to rename the mq-deadline scheduler

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 block/Kconfig.iosched    | 9 +++++++++
 block/elevator.c         | 4 ++++
 block/mq-deadline.c      | 9 +++++++++
 include/linux/elevator.h | 2 +-
 4 files changed, 23 insertions(+), 1 deletion(-)

diff --git a/block/Kconfig.iosched b/block/Kconfig.iosched
index e58b2953a..9e5a3c5c6 100644
--- a/block/Kconfig.iosched
+++ b/block/Kconfig.iosched
@@ -8,6 +8,15 @@ config MQ_IOSCHED_DEADLINE
 	help
 	  MQ version of the deadline IO scheduler.
 
+config MQ_IOSCHED_DEADLINE_NODEFAULT
+	bool "Rename mq-deadline scheduler to mq-deadline-nodefault"
+	depends on MQ_IOSCHED_DEADLINE
+	default n
+	help
+	  This renames the mq-deadline scheduler to "mq-deadline-nodefault" and
+	  also drops its alias of "deadline". This can prevent existing
+	  userspace from forcing this scheduler over the kernel's choice.
+
 config MQ_IOSCHED_KYBER
 	tristate "Kyber I/O scheduler"
 	help
diff --git a/block/elevator.c b/block/elevator.c
index 52ada14cf..ed90090d4 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -634,7 +634,11 @@ static struct elevator_type *elevator_get_default(struct request_queue *q)
 			!blk_mq_is_sbitmap_shared(q->tag_set->flags))
 		return NULL;
 
+#if defined(CONFIG_MQ_IOSCHED_DEADLINE_NODEFAULT)
+	return elevator_get(q, "mq-deadline-nodefault", false);
+#else
 	return elevator_get(q, "mq-deadline", false);
+#endif
 }
 
 /*
diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index 36920670d..bc796f736 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -1079,12 +1079,21 @@ static struct elevator_type mq_deadline = {
 	.queue_debugfs_attrs = deadline_queue_debugfs_attrs,
 #endif
 	.elevator_attrs = deadline_attrs,
+#ifdef CONFIG_MQ_IOSCHED_DEADLINE_NODEFAULT
+	.elevator_name = "mq-deadline-nodefault",
+	.elevator_alias = "deadline-nodefault",
+#else
 	.elevator_name = "mq-deadline",
 	.elevator_alias = "deadline",
+#endif
 	.elevator_features = ELEVATOR_F_ZBD_SEQ_WRITE,
 	.elevator_owner = THIS_MODULE,
 };
+#ifdef CONFIG_MQ_IOSCHED_DEADLINE_NODEFAULT
+MODULE_ALIAS("mq-deadline-nodefault-iosched");
+#else
 MODULE_ALIAS("mq-deadline-iosched");
+#endif
 
 static int __init deadline_init(void)
 {
diff --git a/include/linux/elevator.h b/include/linux/elevator.h
index ef9ceead3..b6c59500c 100644
--- a/include/linux/elevator.h
+++ b/include/linux/elevator.h
@@ -52,7 +52,7 @@ struct elevator_mq_ops {
 	void (*exit_icq)(struct io_cq *);
 };
 
-#define ELV_NAME_MAX	(16)
+#define ELV_NAME_MAX	(24)
 
 struct elv_fs_entry {
 	struct attribute attr;
-- 
2.33.0

