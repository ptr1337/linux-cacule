From d69f8af1e8154cda947a2736f2775829eb227798 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Mon, 15 Feb 2021 11:17:31 +0100
Subject: [PATCH 01/11] mm-5.11: protect file mappings under memory pressure

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 Documentation/admin-guide/sysctl/vm.rst | 27 ++++++++++++++++++
 kernel/sysctl.c                         | 35 +++++++++++++++++++++++
 mm/Kconfig                              | 37 +++++++++++++++++++++++++
 mm/vmscan.c                             | 32 +++++++++++++++++++++
 4 files changed, 131 insertions(+)

diff --git a/Documentation/admin-guide/sysctl/vm.rst b/Documentation/admin-guide/sysctl/vm.rst
index 586cd4b86..16826b31d 100644
--- a/Documentation/admin-guide/sysctl/vm.rst
+++ b/Documentation/admin-guide/sysctl/vm.rst
@@ -69,6 +69,8 @@ Currently, these files are in /proc/sys/vm:
 - stat_refresh
 - numa_stat
 - swappiness
+- unevictable_file_kbytes_low
+- unevictable_file_kbytes_min
 - unprivileged_userfaultfd
 - user_reserve_kbytes
 - vfs_cache_pressure
@@ -886,6 +888,31 @@ calls without any restrictions.
 The default value is 0.
 
 
+unevictable_file_kbytes_low
+===========================
+
+Keep some file pages still mapped under memory pressure to avoid potential
+disk thrashing that may occur due to evicting running executables code.
+This implements soft eviction throttling, and some file pages can still
+be discarded.
+
+Setting it to 0 effectively disables this feature.
+
+The default value is 256 MiB.
+
+
+unevictable_file_kbytes_min
+===========================
+
+Keep all file pages still mapped under memory pressure to avoid potential
+disk thrashing that may occur due to evicting running executables code.
+This is the hard limit.
+
+Setting it to 0 effectively disables this feature.
+
+The default value is 128 MiB.
+
+
 user_reserve_kbytes
 ===================
 
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 62fbd09b5..b2b92bb22 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -111,6 +111,22 @@
 static int sixty = 60;
 #endif
 
+#if defined(CONFIG_UNEVICTABLE_FILE)
+#if CONFIG_UNEVICTABLE_FILE_KBYTES_LOW < 0
+#error "CONFIG_UNEVICTABLE_FILE_KBYTES_LOW should be >= 0"
+#endif
+#if CONFIG_UNEVICTABLE_FILE_KBYTES_MIN < 0
+#error "CONFIG_UNEVICTABLE_FILE_KBYTES_MIN should be >= 0"
+#endif
+#if CONFIG_UNEVICTABLE_FILE_KBYTES_LOW < CONFIG_UNEVICTABLE_FILE_KBYTES_MIN
+#error "CONFIG_UNEVICTABLE_FILE_KBYTES_LOW should be >= CONFIG_UNEVICTABLE_FILE_KBYTES_MIN"
+#endif
+unsigned long sysctl_unevictable_file_kbytes_low __read_mostly =
+	CONFIG_UNEVICTABLE_FILE_KBYTES_LOW;
+unsigned long sysctl_unevictable_file_kbytes_min __read_mostly =
+	CONFIG_UNEVICTABLE_FILE_KBYTES_MIN;
+#endif
+
 static int __maybe_unused neg_one = -1;
 static int __maybe_unused two = 2;
 static int __maybe_unused four = 4;
@@ -3092,6 +3108,25 @@ static struct ctl_table vm_table[] = {
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= SYSCTL_ONE,
 	},
+#endif
+#if defined(CONFIG_UNEVICTABLE_FILE)
+	{
+		.procname	= "unevictable_file_kbytes_low",
+		.data		= &sysctl_unevictable_file_kbytes_low,
+		.maxlen		= sizeof(sysctl_unevictable_file_kbytes_low),
+		.mode		= 0644,
+		.proc_handler	= proc_doulongvec_minmax,
+		.extra1		= &sysctl_unevictable_file_kbytes_min,
+	},
+	{
+		.procname	= "unevictable_file_kbytes_min",
+		.data		= &sysctl_unevictable_file_kbytes_min,
+		.maxlen		= sizeof(sysctl_unevictable_file_kbytes_min),
+		.mode		= 0644,
+		.proc_handler	= proc_doulongvec_minmax,
+		.extra1		= &zero_ul,
+		.extra2		= &sysctl_unevictable_file_kbytes_low,
+	},
 #endif
 	{
 		.procname	= "user_reserve_kbytes",
diff --git a/mm/Kconfig b/mm/Kconfig
index f730605b8..b5e0e9195 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -63,6 +63,43 @@ config SPARSEMEM_MANUAL
 
 endchoice
 
+config UNEVICTABLE_FILE
+	bool "Keep some file pages under memory pressure"
+	depends on SYSCTL
+	def_bool y
+	help
+	  Keep some file pages still mapped under memory pressure
+	  to avoid potential disk thrashing that may occur due to
+	  evicting running executables code.
+
+	  The UNEVICTABLE_FILE_KBYTES_LOW value defines a threshold
+	  to activate file pages eviction throttling.
+	  The vm.unevictable_file_kbytes_low sysctl knob is used to
+	  change the amount in the runtime (setting it to 0
+	  effectively disables this feature).
+
+	  Recommended value: 262144 for a typical desktop workload.
+
+	  The UNEVICTABLE_FILE_KBYTES_MIN value sets the amount of
+	  pages to keep as a hard limit.
+	  The vm.unevictable_file_kbytes_min sysctl knob is used to
+	  change the amount in the runtime (setting it to 0
+	  effectively disables this feature).
+
+	  Recommended value: 131072 for a typical desktop workload.
+
+	  See also: Documentation/admin-guide/sysctl/vm.rst
+
+config UNEVICTABLE_FILE_KBYTES_LOW
+	int "Default value for vm.unevictable_file_kbytes_low"
+	depends on UNEVICTABLE_FILE
+	default "262144"
+
+config UNEVICTABLE_FILE_KBYTES_MIN
+	int "Default value for vm.unevictable_file_kbytes_min"
+	depends on UNEVICTABLE_FILE
+	default "131072"
+
 config DISCONTIGMEM
 	def_bool y
 	depends on (!SELECT_MEMORY_MODEL && ARCH_DISCONTIGMEM_ENABLE) || DISCONTIGMEM_MANUAL
diff --git a/mm/vmscan.c b/mm/vmscan.c
index ad9f2adaf..3eed6f58b 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -115,6 +115,14 @@ struct scan_control {
 	/* There is easily reclaimable cold cache in the current node */
 	unsigned int cache_trim_mode:1;
 
+#if defined(CONFIG_UNEVICTABLE_FILE)
+	/* The file pages on the current node are low */
+	unsigned int file_is_low:1;
+
+	/* The file pages on the current node are minimal */
+	unsigned int file_is_min:1;
+#endif
+
 	/* The file pages on the current node are dangerously low */
 	unsigned int file_is_tiny:1;
 
@@ -164,6 +172,11 @@ struct scan_control {
 #define prefetchw_prev_lru_page(_page, _base, _field) do { } while (0)
 #endif
 
+#if defined(CONFIG_UNEVICTABLE_FILE)
+extern unsigned long sysctl_unevictable_file_kbytes_low;
+extern unsigned long sysctl_unevictable_file_kbytes_min;
+#endif
+
 /*
  * From 0 .. 200.  Higher means more swappy.
  */
@@ -2421,6 +2434,14 @@ static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,
 			/* Scan one type exclusively */
 			if ((scan_balance == SCAN_FILE) != file)
 				scan = 0;
+#if defined(CONFIG_UNEVICTABLE_FILE)
+			else if (scan_balance == SCAN_FILE && file) {
+				if (sc->file_is_low)
+					scan = SWAP_CLUSTER_MAX >> sc->priority;
+				else if (sc->file_is_min)
+					scan = 0;
+			}
+#endif
 			break;
 		default:
 			/* Look ma, no brain */
@@ -2673,6 +2694,10 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)
 	} while ((memcg = mem_cgroup_iter(target_memcg, memcg, NULL)));
 }
 
+#if defined(CONFIG_UNEVICTABLE_FILE)
+#define K(x) ((x) << (PAGE_SHIFT - 10))
+#endif
+
 static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 {
 	struct reclaim_state *reclaim_state = current->reclaim_state;
@@ -2775,6 +2800,13 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 			file + free <= total_high_wmark &&
 			!(sc->may_deactivate & DEACTIVATE_ANON) &&
 			anon >> sc->priority;
+
+#if defined(CONFIG_UNEVICTABLE_FILE)
+		sc->file_is_low = K(file) < sysctl_unevictable_file_kbytes_low &&
+				  K(file) > sysctl_unevictable_file_kbytes_min;
+
+		sc->file_is_min = K(file) < sysctl_unevictable_file_kbytes_min;
+#endif
 	}
 
 	shrink_node_memcgs(pgdat, sc);
-- 
2.31.0.97.g1424303384


From 5db8a8b793659bcc5e159695d124d041c7311533 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Mon, 15 Feb 2021 11:44:39 +0100
Subject: [PATCH 02/11] mm-5.11: inequality should not exclude lower threshold

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 mm/vmscan.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index 3eed6f58b..f88931dcf 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2805,7 +2805,7 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 		sc->file_is_low = K(file) < sysctl_unevictable_file_kbytes_low &&
 				  K(file) > sysctl_unevictable_file_kbytes_min;
 
-		sc->file_is_min = K(file) < sysctl_unevictable_file_kbytes_min;
+		sc->file_is_min = K(file) <= sysctl_unevictable_file_kbytes_min;
 #endif
 	}
 
-- 
2.31.0.97.g1424303384


From f525bc00335331f725d40eaf21a6a2be597b652d Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Mon, 15 Feb 2021 12:49:51 +0100
Subject: [PATCH 03/11] mm-5.11: throttle file pages scan regardless of scan
 balance

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 mm/vmscan.c | 18 ++++++++++--------
 1 file changed, 10 insertions(+), 8 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index f88931dcf..2032af8d8 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2434,20 +2434,22 @@ static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,
 			/* Scan one type exclusively */
 			if ((scan_balance == SCAN_FILE) != file)
 				scan = 0;
-#if defined(CONFIG_UNEVICTABLE_FILE)
-			else if (scan_balance == SCAN_FILE && file) {
-				if (sc->file_is_low)
-					scan = SWAP_CLUSTER_MAX >> sc->priority;
-				else if (sc->file_is_min)
-					scan = 0;
-			}
-#endif
 			break;
 		default:
 			/* Look ma, no brain */
 			BUG();
 		}
 
+#if defined(CONFIG_UNEVICTABLE_FILE)
+		if (file && scan) {
+			unsigned long low_scan_granularity = SWAP_CLUSTER_MAX >> sc->priority;
+			if (sc->file_is_low && scan > low_scan_granularity)
+				scan = low_scan_granularity;
+			else if (sc->file_is_min)
+				scan = 0;
+		}
+#endif
+
 		nr[lru] = scan;
 	}
 }
-- 
2.31.0.97.g1424303384


From dd227e981dd3499deeb884129de1a92be0b02934 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Sat, 20 Feb 2021 08:37:18 +0100
Subject: [PATCH 04/11] mm-5.11: simplify file_is_low

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 mm/vmscan.c | 5 ++---
 1 file changed, 2 insertions(+), 3 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index 2032af8d8..f9fcde30d 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2442,9 +2442,8 @@ static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,
 
 #if defined(CONFIG_UNEVICTABLE_FILE)
 		if (file && scan) {
-			unsigned long low_scan_granularity = SWAP_CLUSTER_MAX >> sc->priority;
-			if (sc->file_is_low && scan > low_scan_granularity)
-				scan = low_scan_granularity;
+			if (sc->file_is_low)
+				scan = min(scan, SWAP_CLUSTER_MAX >> sc->priority);
 			else if (sc->file_is_min)
 				scan = 0;
 		}
-- 
2.31.0.97.g1424303384


From 357a41d2744cd1277f2e1d080227fe9066e60e81 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Thu, 8 Apr 2021 00:30:06 +0200
Subject: [PATCH 05/11] mm-5.11: do not protect dirty pages

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 mm/vmscan.c | 23 ++++++++++++++++++++---
 1 file changed, 20 insertions(+), 3 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index f9fcde30d..6623af0d1 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2776,6 +2776,9 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 	if (!cgroup_reclaim(sc)) {
 		unsigned long total_high_wmark = 0;
 		unsigned long free, anon;
+#if defined(CONFIG_UNEVICTABLE_FILE)
+		unsigned long reclaimable_file, clean_file, dirty_file;
+#endif
 		int z;
 
 		free = sum_zone_node_page_state(pgdat->node_id, NR_FREE_PAGES);
@@ -2803,10 +2806,24 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 			anon >> sc->priority;
 
 #if defined(CONFIG_UNEVICTABLE_FILE)
-		sc->file_is_low = K(file) < sysctl_unevictable_file_kbytes_low &&
-				  K(file) > sysctl_unevictable_file_kbytes_min;
+		reclaimable_file = file + node_page_state(pgdat, NR_ISOLATED_FILE);
+		dirty_file = node_page_state(pgdat, NR_FILE_DIRTY);
+
+		if (reclaimable_file < dirty_file) {
+			/*
+			 * post-factum: so, nr_file_dirty can never exceed
+			 *              (nr_inactive_file+nr_active_file+nr_isolated_file)?
+			 * vbabka: ugh, never say never
+			 */
+			VM_WARN_ON_ONCE(1);
+		} else {
+			clean_file = reclaimable_file - dirty_file;
 
-		sc->file_is_min = K(file) <= sysctl_unevictable_file_kbytes_min;
+			sc->file_is_low = K(clean_file) < sysctl_unevictable_file_kbytes_low &&
+				          K(clean_file) > sysctl_unevictable_file_kbytes_min;
+
+			sc->file_is_min = K(clean_file) <= sysctl_unevictable_file_kbytes_min;
+		}
 #endif
 	}
 
-- 
2.31.0.97.g1424303384


From 838b3cede8df6d9c9c5c5dc71067cfa8c9e320a8 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Thu, 8 Apr 2021 00:39:08 +0200
Subject: [PATCH 06/11] mm-5.11: be more concise about clean pages

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 mm/vmscan.c | 10 +++++-----
 1 file changed, 5 insertions(+), 5 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index 6623af0d1..5e50353ff 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2816,14 +2816,14 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 			 * vbabka: ugh, never say never
 			 */
 			VM_WARN_ON_ONCE(1);
-		} else {
+			clean_file = ULONG_MAX;
+		} else
 			clean_file = reclaimable_file - dirty_file;
 
-			sc->file_is_low = K(clean_file) < sysctl_unevictable_file_kbytes_low &&
-				          K(clean_file) > sysctl_unevictable_file_kbytes_min;
+		sc->file_is_low = K(clean_file) < sysctl_unevictable_file_kbytes_low &&
+			          K(clean_file) > sysctl_unevictable_file_kbytes_min;
 
-			sc->file_is_min = K(clean_file) <= sysctl_unevictable_file_kbytes_min;
-		}
+		sc->file_is_min = K(clean_file) <= sysctl_unevictable_file_kbytes_min;
 #endif
 	}
 
-- 
2.31.0.97.g1424303384


From 90dce1bdfdc91f11253febce25c7afde8e95fc6d Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Thu, 8 Apr 2021 08:08:50 +0200
Subject: [PATCH 07/11] mm-5.11: nits on branching

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 mm/vmscan.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index 5e50353ff..267763fe2 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2809,13 +2809,13 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 		reclaimable_file = file + node_page_state(pgdat, NR_ISOLATED_FILE);
 		dirty_file = node_page_state(pgdat, NR_FILE_DIRTY);
 
-		if (reclaimable_file < dirty_file) {
+		if (unlikely(reclaimable_file < dirty_file)) {
 			/*
 			 * post-factum: so, nr_file_dirty can never exceed
 			 *              (nr_inactive_file+nr_active_file+nr_isolated_file)?
 			 * vbabka: ugh, never say never
 			 */
-			VM_WARN_ON_ONCE(1);
+			WARN_ON_ONCE(1);
 			clean_file = ULONG_MAX;
 		} else
 			clean_file = reclaimable_file - dirty_file;
-- 
2.31.0.97.g1424303384


From d2447ad19e7a9c0f391d9eb43a04b26ca9e26e7e Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Thu, 8 Apr 2021 08:11:45 +0200
Subject: [PATCH 08/11] mm-5.11: remove WARN_ON since it really can happen due
 to non-atomicity

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 mm/vmscan.c | 10 ++--------
 1 file changed, 2 insertions(+), 8 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index 267763fe2..6cd69d856 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2809,15 +2809,9 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 		reclaimable_file = file + node_page_state(pgdat, NR_ISOLATED_FILE);
 		dirty_file = node_page_state(pgdat, NR_FILE_DIRTY);
 
-		if (unlikely(reclaimable_file < dirty_file)) {
-			/*
-			 * post-factum: so, nr_file_dirty can never exceed
-			 *              (nr_inactive_file+nr_active_file+nr_isolated_file)?
-			 * vbabka: ugh, never say never
-			 */
-			WARN_ON_ONCE(1);
+		if (unlikely(reclaimable_file < dirty_file))
 			clean_file = ULONG_MAX;
-		} else
+		else
 			clean_file = reclaimable_file - dirty_file;
 
 		sc->file_is_low = K(clean_file) < sysctl_unevictable_file_kbytes_low &&
-- 
2.31.0.97.g1424303384


From 04107c4bb42a16fa0715b6213814fd586b26ff2a Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Thu, 8 Apr 2021 08:12:41 +0200
Subject: [PATCH 09/11] mm-5.11: call node_page_state() closer to each other

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 mm/vmscan.c | 7 ++++---
 1 file changed, 4 insertions(+), 3 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index 6cd69d856..d7a4d80ff 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2784,6 +2784,10 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 		free = sum_zone_node_page_state(pgdat->node_id, NR_FREE_PAGES);
 		file = node_page_state(pgdat, NR_ACTIVE_FILE) +
 			   node_page_state(pgdat, NR_INACTIVE_FILE);
+#if defined(CONFIG_UNEVICTABLE_FILE)
+		reclaimable_file = file + node_page_state(pgdat, NR_ISOLATED_FILE);
+		dirty_file = node_page_state(pgdat, NR_FILE_DIRTY);
+#endif
 
 		for (z = 0; z < MAX_NR_ZONES; z++) {
 			struct zone *zone = &pgdat->node_zones[z];
@@ -2806,9 +2810,6 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 			anon >> sc->priority;
 
 #if defined(CONFIG_UNEVICTABLE_FILE)
-		reclaimable_file = file + node_page_state(pgdat, NR_ISOLATED_FILE);
-		dirty_file = node_page_state(pgdat, NR_FILE_DIRTY);
-
 		if (unlikely(reclaimable_file < dirty_file))
 			clean_file = ULONG_MAX;
 		else
-- 
2.31.0.97.g1424303384


From 9aaa7d4fa23db7a886362f245336cdc7529e7a0c Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Thu, 8 Apr 2021 08:14:26 +0200
Subject: [PATCH 10/11] mm-5.11: explain non-atomicity

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 mm/vmscan.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index d7a4d80ff..3ac537721 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2810,6 +2810,10 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 			anon >> sc->priority;
 
 #if defined(CONFIG_UNEVICTABLE_FILE)
+		/*
+		 * node_page_state() sum can go out of sync since
+		 * all the values are not read at once
+		 */
 		if (unlikely(reclaimable_file < dirty_file))
 			clean_file = ULONG_MAX;
 		else
-- 
2.31.0.97.g1424303384


From 26f289a07902d8805f4ee0a7226c2d08691a18c2 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Thu, 8 Apr 2021 16:40:18 +0200
Subject: [PATCH 11/11] mm-5.11: do not disable file pages protection under
 dirty pressure

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 mm/vmscan.c | 6 +++++-
 1 file changed, 5 insertions(+), 1 deletion(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index 3ac537721..c8ebf5266 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2815,7 +2815,11 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 		 * all the values are not read at once
 		 */
 		if (unlikely(reclaimable_file < dirty_file))
-			clean_file = ULONG_MAX;
+			/*
+			 * in this case assume the system does not have
+			 * clean file pages anymore
+			 */
+			clean_file = 0;
 		else
 			clean_file = reclaimable_file - dirty_file;
 
-- 
2.31.0.97.g1424303384

