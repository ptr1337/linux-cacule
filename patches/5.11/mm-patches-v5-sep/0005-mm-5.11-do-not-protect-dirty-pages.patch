From 1f23a60df36e6ed3fcedf7456be8e8d07b3acb06 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Thu, 8 Apr 2021 00:30:06 +0200
Subject: [PATCH 05/10] mm-5.11: do not protect dirty pages

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 mm/vmscan.c | 23 ++++++++++++++++++++---
 1 file changed, 20 insertions(+), 3 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index f9fcde30d..6623af0d1 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2776,6 +2776,9 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 	if (!cgroup_reclaim(sc)) {
 		unsigned long total_high_wmark = 0;
 		unsigned long free, anon;
+#if defined(CONFIG_UNEVICTABLE_FILE)
+		unsigned long reclaimable_file, clean_file, dirty_file;
+#endif
 		int z;
 
 		free = sum_zone_node_page_state(pgdat->node_id, NR_FREE_PAGES);
@@ -2803,10 +2806,24 @@ static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 			anon >> sc->priority;
 
 #if defined(CONFIG_UNEVICTABLE_FILE)
-		sc->file_is_low = K(file) < sysctl_unevictable_file_kbytes_low &&
-				  K(file) > sysctl_unevictable_file_kbytes_min;
+		reclaimable_file = file + node_page_state(pgdat, NR_ISOLATED_FILE);
+		dirty_file = node_page_state(pgdat, NR_FILE_DIRTY);
+
+		if (reclaimable_file < dirty_file) {
+			/*
+			 * post-factum: so, nr_file_dirty can never exceed
+			 *              (nr_inactive_file+nr_active_file+nr_isolated_file)?
+			 * vbabka: ugh, never say never
+			 */
+			VM_WARN_ON_ONCE(1);
+		} else {
+			clean_file = reclaimable_file - dirty_file;
 
-		sc->file_is_min = K(file) <= sysctl_unevictable_file_kbytes_min;
+			sc->file_is_low = K(clean_file) < sysctl_unevictable_file_kbytes_low &&
+				          K(clean_file) > sysctl_unevictable_file_kbytes_min;
+
+			sc->file_is_min = K(clean_file) <= sysctl_unevictable_file_kbytes_min;
+		}
 #endif
 	}
 
-- 
2.31.0.97.g1424303384

