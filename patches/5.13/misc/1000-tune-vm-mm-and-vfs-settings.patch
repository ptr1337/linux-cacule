From ed51cc440936145e38cd24927045fcd6d306ddad Mon Sep 17 00:00:00 2001
From: Tor Vic <torvic9@mailbox.org>
Date: Fri, 28 May 2021 10:29:32 +0200
Subject: [PATCH] tune vm, mm and vfs settings

Signed-off-by: Tor Vic <torvic9@mailbox.org>
---
 fs/dcache.c             | 2 +-
 include/linux/pagemap.h | 2 +-
 mm/page-writeback.c     | 6 +++---
 mm/vmpressure.c         | 5 ++---
 mm/vmscan.c             | 2 +-
 5 files changed, 8 insertions(+), 9 deletions(-)

diff --git a/fs/dcache.c b/fs/dcache.c
index cf871a81f4fd..21a59833d270 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -70,9 +70,9 @@
  *
  * If no ancestor relationship:
  * arbitrary, since it's serialized on rename_lock
  */
-int sysctl_vfs_cache_pressure __read_mostly = 100;
+int sysctl_vfs_cache_pressure __read_mostly = 80;
 EXPORT_SYMBOL_GPL(sysctl_vfs_cache_pressure);
 
 __cacheline_aligned_in_smp DEFINE_SEQLOCK(rename_lock);
 
diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h
index e89df447fae3..297f34636fe7 100644
--- a/include/linux/pagemap.h
+++ b/include/linux/pagemap.h
@@ -846,9 +846,9 @@ struct readahead_control {
 		.ra = r,						\
 		._index = i,						\
 	}
 
-#define VM_READAHEAD_PAGES	(SZ_128K / PAGE_SIZE)
+#define VM_READAHEAD_PAGES	(SZ_512K / PAGE_SIZE)
 
 void page_cache_ra_unbounded(struct readahead_control *,
 		unsigned long nr_to_read, unsigned long lookahead_count);
 void page_cache_sync_ra(struct readahead_control *, unsigned long req_count);
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index 0062d5c57d41..fdeed7748c8d 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -70,9 +70,9 @@ static long ratelimit_pages = 32;
 
 /*
  * Start background writeback (via writeback threads) at this percentage
  */
-int dirty_background_ratio = 10;
+int dirty_background_ratio = 5;
 
 /*
  * dirty_background_bytes starts at 0 (disabled) so that it is a function of
  * dirty_background_ratio * the amount of dirtyable memory
@@ -87,9 +87,9 @@ int vm_highmem_is_dirtyable;
 
 /*
  * The generator of dirty data starts writeback at this percentage
  */
-int vm_dirty_ratio = 20;
+int vm_dirty_ratio = 15;
 
 /*
  * vm_dirty_bytes starts at 0 (disabled) so that it is a function of
  * vm_dirty_ratio * the amount of dirtyable memory
@@ -105,9 +105,9 @@ EXPORT_SYMBOL_GPL(dirty_writeback_interval);
 
 /*
  * The longest time for which data is allowed to remain dirty
  */
-unsigned int dirty_expire_interval = 30 * 100; /* centiseconds */
+unsigned int dirty_expire_interval = 20 * 100; /* centiseconds */
 
 /*
  * Flag that makes the machine dump writes/reads and block dirtyings.
  */
diff --git a/mm/vmpressure.c b/mm/vmpressure.c
index d69019fc3789..97cc4b6fc9a0 100644
--- a/mm/vmpressure.c
+++ b/mm/vmpressure.c
@@ -10,9 +10,8 @@
  */
 
 #include <linux/cgroup.h>
 #include <linux/fs.h>
-#include <linux/log2.h>
 #include <linux/sched.h>
 #include <linux/mm.h>
 #include <linux/vmstat.h>
 #include <linux/eventfd.h>
@@ -42,9 +41,9 @@ static const unsigned long vmpressure_win = SWAP_CLUSTER_MAX * 16;
  * scanned/reclaimed ratio. The current values were chosen empirically. In
  * essence, they are percents: the higher the value, the more number
  * unsuccessful reclaims there were.
  */
-static const unsigned int vmpressure_level_med = 60;
+static const unsigned int vmpressure_level_med = 65;
 static const unsigned int vmpressure_level_critical = 95;
 
 /*
  * When there are too little pages left to scan, vmpressure() may miss the
@@ -64,9 +63,9 @@ static const unsigned int vmpressure_level_critical = 95;
  * critical level when scanning depth is ~10% of the lru size (vmscan
  * scans 'lru_size >> prio' pages, so it is actually 12.5%, or one
  * eights).
  */
-static const unsigned int vmpressure_level_critical_prio = ilog2(100 / 10);
+static const unsigned int vmpressure_level_critical_prio = 3;
 
 static struct vmpressure *work_to_vmpressure(struct work_struct *work)
 {
 	return container_of(work, struct vmpressure, work);
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 5199b9696bab..5bc76e67d4de 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -166,9 +166,9 @@ struct scan_control {
 
 /*
  * From 0 .. 200.  Higher means more swappy.
  */
-int vm_swappiness = 60;
+int vm_swappiness = 30;
 
 static void set_task_reclaim_state(struct task_struct *task,
 				   struct reclaim_state *rs)
 {
-- 
2.31.1

