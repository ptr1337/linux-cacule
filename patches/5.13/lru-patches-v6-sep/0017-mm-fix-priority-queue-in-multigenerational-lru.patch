From b60a5625a40ad0aebedc6ca377db75feaa86ef81 Mon Sep 17 00:00:00 2001
From: Yu Zhao <yuzhao@google.com>
Date: Wed, 21 Jul 2021 17:24:25 -0600
Subject: [PATCH 17/22] mm: fix priority queue in multigenerational lru

The priority queue helps select the best memcg to reclaim memory from
when there are multiple candidates. If reclaim didn't make progress
after it has tried those at the front, it increments the priority of
the rest in the queue by one. If the queue is not empty, more memcgs
will appear at the front and then reclaim will try them again.

However, this wasn't properly implemented, which causes the memory
pressure to be distributed to all memcgs in the queue. If one of them
uses a large amount of page cache, the distributed memory pressure
will slow down the rest.

Signed-off-by: Yu Zhao <yuzhao@google.com>
---
 mm/vmscan.c | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index ac8c71d..0cc6e59 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -4223,6 +4223,8 @@ static int scan_pages(struct lruvec *lruvec, struct scan_control *sc, long *nr_t
 	}
 
 	success = try_inc_min_seq(lruvec, type);
+	if (memcg && !mem_cgroup_is_root(memcg) && !cgroup_reclaim(sc) && success && file)
+		atomic_add_unless(&lrugen->priority, -1, 0);
 
 	item = current_is_kswapd() ? PGSCAN_KSWAPD : PGSCAN_DIRECT;
 	if (!cgroup_reclaim(sc)) {
@@ -4432,6 +4434,10 @@ static unsigned long get_nr_to_scan(struct lruvec *lruvec, struct scan_control *
 	DEFINE_MAX_SEQ();
 	DEFINE_MIN_SEQ();
 
+	/* only proceed with memcgs at the front of the priority queue */
+	if (!cgroup_reclaim(sc) && atomic_read(&lrugen->priority) != DEF_PRIORITY)
+		return 0;
+
 	lru_add_drain();
 
 	for (type = !swappiness; type < ANON_AND_FILE; type++) {
-- 
2.32.0.452.g940fe202ad

